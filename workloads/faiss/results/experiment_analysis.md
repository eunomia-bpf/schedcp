# FAISS 向量搜索基准测试：GPU UVM 内存管理优化分析

## 摘要

本文档对基于 FAISS（Facebook AI Similarity Search）的十亿级向量相似性搜索中 GPU 统一虚拟内存（UVM）性能优化进行了全面分析。实验在 SIFT 数据集上使用 IVF4096,Flat 索引配置，评估了基于预取的自适应树策略与基线 UVM 的性能差异，数据集规模从 2000 万到 1 亿向量不等。

**核心发现：**
1. **GPU UVM+BPF 预取优化将索引构建时间减少 21-29%**，数据集越大改进越显著（SIFT100M：67.8s → 48.1s，减少 29%）
2. **大规模数据集搜索延迟降低 10-16%**（SIFT100M：nprobe=1 时 -15.9%，nprobe=16 时 -10.5%）
3. **GPU 设备内存搜索吞吐量比 CPU 高 29 倍**，但受限于 VRAM 容量
4. **UVM 使处理超出 GPU VRAM 的数据集成为可能**，性能优雅降级

---

## 1. 引言

### 1.1 研究背景

现代 AI 应用越来越依赖向量相似性搜索来完成检索增强生成（RAG）、推荐系统和语义搜索等任务。FAISS 是一个广泛采用的高效相似性搜索库，支持 CPU 和 GPU 执行。然而，GPU 内存容量通常限制了可高效处理的数据集大小。

CUDA 统一虚拟内存（UVM）提供了 CPU 和 GPU 内存之间的统一地址空间，实现自动数据迁移，允许处理超出 GPU VRAM 的数据集。但是，UVM 的默认页面迁移策略可能导致次优性能，原因包括：
- 被动响应页面错误而非主动预取
- 缺乏应用特定访问模式感知
- 对 ANN 搜索中不规则内存访问模式的低效处理

### 1.2 研究问题

自适应预取策略能否改善大规模向量相似性搜索工作负载的 UVM 性能？改进如何随数据集规模扩展？

### 1.3 主要贡献

1. 对 FAISS 向量搜索的 GPU 内存管理策略进行系统评估
2. 在多个数据集规模上对 UVM 基线与预取优化 UVM 进行定量比较
3. 分析不同部署场景下 CPU、GPU 和 UVM 的性能权衡

---

## 2. 实验设置

### 2.1 硬件配置

- **GPU**：NVIDIA GeForce RTX 5090（根据基准测试模式推测为 32GB VRAM）
- **平台**：Linux 6.15.11
- **CUDA**：12.9+

### 2.2 软件配置

- **FAISS**：自定义构建，支持 CUDA
- **索引类型**：IVF4096,Flat（4096 个聚类中心的倒排文件索引，精确距离计算）
- **预取策略**：`prefetch_adaptive_tree_iter`，参数 `-M 50 -b 4096`

### 2.3 数据集

**SIFT（尺度不变特征变换）向量：**
| 参数 | 值 |
|------|-----|
| 维度 | 128 |
| 数据类型 | uint8（转换为 float32）|
| 训练集 | 1 亿向量 |
| 查询集 | 10,000 向量 |
| Ground Truth | 每个查询的 Top-1000 邻居 |

**评估的数据集规模：**
| 名称 | 向量数 | 近似大小（float32）| 内存占用 |
|------|--------|-------------------|----------|
| SIFT20M | 20,000,000 | ~9.5 GB | 可完全放入 GPU VRAM |
| SIFT50M | 50,000,000 | ~24 GB | 32GB VRAM 边界情况 |
| SIFT100M | 100,000,000 | ~48 GB | 超出 GPU VRAM |

### 2.4 基准测试参数

- **nprobe 值**：1、4、16（搜索时探测的倒排列表数量）
- **添加批次大小**：32,768 向量
- **查询批次大小**：16,384 向量
- **检索邻居数**：10

### 2.5 评估指标

1. **索引构建时间**：将所有向量添加到索引的时间
2. **搜索时间**：处理 10,000 个查询的时间
3. **每秒查询数（QPS）**：吞吐量指标（10,000 / 搜索时间）
4. **Recall@k（1-R@k）**：真实最近邻在 top-k 结果中的查询比例

---

## 3. 实验结果

### 3.1 CPU 与 GPU 设备内存性能对比（SIFT20M）

此对比建立了当数据完全放入 GPU VRAM 时 CPU 和 GPU 执行之间的基线性能差距。

| 指标 | CPU | GPU（设备内存）| 加速比 |
|------|-----|---------------|--------|
| **索引构建时间** | 78.0s | 3.71s | **21.0x** |
| **训练时间** | 0.19s | 0.05s | 3.8x |
| **搜索时间（nprobe=1）** | 0.87s | 0.029s | **30.0x** |
| **搜索时间（nprobe=4）** | 2.00s | 0.070s | **28.5x** |
| **搜索时间（nprobe=16）** | 7.61s | 0.258s | **29.5x** |
| **QPS（nprobe=16）** | 1,314 | 38,721 | **29.5x** |
| **Recall@1（nprobe=16）** | 93.3% | 93.3% | - |

**关键观察：**
- GPU 比 CPU 索引构建快约 21 倍，搜索快约 29 倍
- CPU 和 GPU 的 Recall 相同（验证正确性）
- 当数据集可放入 VRAM 时，GPU 设备内存是最优选择

### 3.2 GPU UVM：基线 vs 预取优化（SIFT50M）

| 指标 | UVM 基线 | UVM + 预取 | 改进 |
|------|---------|-----------|------|
| **索引构建时间** | 16.77s | 13.19s | **21.4%** |
| **搜索时间（nprobe=1）** | 0.048s | 0.049s | -1.5% |
| **搜索时间（nprobe=4）** | 0.168s | 0.168s | -0.1% |
| **搜索时间（nprobe=16）** | 0.642s | 0.640s | **0.3%** |
| **QPS（nprobe=16）** | 15,585 | 15,631 | 0.3% |
| **Recall@1（nprobe=16）** | 94.36% | 94.36% | - |

**关键观察：**
- 预取优化在**索引构建时间上提供 21.4% 的改进**
- 搜索性能保持相当（nprobe=16 时略有改进）
- SIFT50M（~24GB）接近 GPU VRAM 边界，解释了适度的 UVM 开销

### 3.3 GPU UVM：基线 vs BPF 预取优化（SIFT100M）

| 指标 | UVM 基线 | UVM+BPF Prefetch | 改进 |
|------|---------|------------------|------|
| **索引构建时间** | 67.77s | 48.15s | **28.9%** |
| **搜索时间（nprobe=1）** | 14.04s | 11.80s | **-15.9%** |
| **搜索时间（nprobe=4）** | 15.96s | 14.18s | **-11.1%** |
| **搜索时间（nprobe=16）** | 57.44s | 51.42s | **-10.5%** |
| **QPS（nprobe=16）** | 174 | 194 | **+11.7%** |
| **Recall@1（nprobe=16）** | 94.76% | 94.76% | - |

**关键观察：**
- BPF 预取优化在**索引构建时间上提供 28.9% 的改进**
- 所有 nprobe 值的搜索延迟**降低 10-16%**
- **随 nprobe 增加，改进幅度略有下降**（nprobe=1: -15.9% → nprobe=16: -10.5%），这是因为更高的 nprobe 值本身就需要更多的内存访问，预取的边际收益递减
- 由于内存压力增加，相比 SIFT50M 改进更大

### 3.4 可扩展性分析

#### 索引构建时间对比

| 数据集 | GPU 设备内存 | UVM 基线 | UVM+BPF Prefetch |
|--------|-------------|---------|------------------|
| SIFT20M | 3.71s | N/A | N/A |
| SIFT50M | N/A | 16.77s | 13.19s |
| SIFT100M | N/A | 67.77s | 48.15s |

#### 标准化构建吞吐量（向量/秒）

| 数据集 | UVM 基线 | UVM+BPF Prefetch | 改进 |
|--------|---------|------------------|------|
| SIFT50M | 2.98M 向量/秒 | 3.79M 向量/秒 | **27.2%** |
| SIFT100M | 1.48M 向量/秒 | 2.08M 向量/秒 | **40.5%** |

**关键发现：**
- BPF 预取优化在**更大规模时显示出更大的收益**，吞吐量改进从 5000 万向量时的 27% 增长到 1 亿向量时的 40%
- **构建时间存在明显拐点**：从图 (a) 可观察到，SIFT100M 在约 55-60M 向量处构建速度明显变慢（斜率增大），这标志着数据集开始超出 GPU VRAM，触发频繁的页面迁移

### 3.5 搜索性能与数据集规模关系

| 数据集 | nprobe=1 QPS | nprobe=16 QPS | Recall@1（nprobe=16）|
|--------|--------------|---------------|---------------------|
| SIFT20M（GPU 设备内存）| 348,610 | 38,721 | 93.3% |
| SIFT50M（UVM 基线）| 207,802 | 15,585 | 94.4% |
| SIFT50M（UVM + 预取）| 204,700 | 15,631 | 94.4% |
| SIFT100M（UVM 基线）| 712 | 174 | 94.8% |
| SIFT100M（UVM + 预取）| 847 | 194 | 94.8% |

**关键观察：** 当数据集超出 GPU VRAM（SIFT100M）时，搜索性能显著下降，但预取优化提供了一致的改进。

---

## 4. 分析与讨论

### 4.1 为什么预取能提高性能？

自适应预取策略通过以下方式提高 UVM 性能：

1. **主动数据迁移**：预测内存访问模式，在页面错误发生前预取数据
2. **批量预取**：将多个小传输聚合为更大、更高效的 GPU 内存操作
3. **减少页面错误开销**：最小化导致 GPU 内核停顿的同步页面错误
4. **自适应树结构**：基于访问历史和工作集模式组织预取决策

### 4.2 大规模性能降级原因

SIFT100M 的显著性能下降（QPS：38,721 → 174，减少 222 倍）归因于：

1. **内存抖动**：数据集（~48GB）显著超出 GPU VRAM（~32GB）
2. **PCIe 带宽限制**：数据迁移受 PCIe Gen4 带宽（~25 GB/s）瓶颈
3. **随机访问模式**：IVF 搜索访问非连续内存区域，硬件预取器失效

### 4.3 召回率与性能权衡

| nprobe | Recall@1 | 搜索时间增加 |
|--------|----------|-------------|
| 1 | ~44% | 基线 |
| 4 | ~76% | ~3-4x |
| 16 | ~94% | ~10-15x |

nprobe=16 配置为大多数应用提供了最佳的精度-性能权衡。

---

## 5. 实验设计评估

### 5.1 优点

1. **标准基准数据集**：SIFT 是 ANN 搜索社区公认的基准，支持可重复性和与已发表结果的比较
2. **多个规模点**：测试 2000 万、5000 万和 1 亿向量，捕获跨越 GPU VRAM 边界的行为
3. **综合指标**：同时测量吞吐量（QPS）和准确率（Recall）
4. **详细进度跟踪**：JSON 输出包含每 5% 进度，支持细粒度分析
5. **实际索引类型**：IVF4096,Flat 在生产部署中常用

### 5.2 局限性与改进建议

| 问题 | 影响 | 改进建议 |
|------|------|---------|
| **无统计显著性** | 结果可能不可重复 | 运行 3-5 次重复实验，报告均值/标准差 |
| **单一 GPU 配置** | 泛化性有限 | 在多种 GPU 架构上测试（A100、H100）|
| **大规模缺少 GPU 设备内存对比** | 对比基线不完整 | 添加 OOM 处理或多 GPU 对比 |
| **索引类型有限** | 结果仅针对 IVF | 测试 IVF-PQ、HNSW 以获得更广覆盖 |
| **无内存分析** | 根因分析受限 | 添加 CUDA 性能分析（nsight、nvprof）|
| **固定预取参数** | 可能非最优 | 对 `-M` 和 `-b` 进行参数扫描 |
| **无冷启动测量** | 缓存效应未知 | 运行间清除缓存 |
| **单一查询批次大小** | 可能不反映生产环境 | 测试从 1 到 100K 的批次大小 |

### 5.3 达到 OSDI 质量评估的建议补充

1. **微基准测试**：使用合成访问模式隔离预取策略有效性
2. **端到端延迟分布**：报告 P50、P99、P99.9 延迟用于尾延迟分析
3. **内存带宽利用率**：测量基准测试期间的 PCIe 和 HBM 利用率
4. **能效**：报告 查询/焦耳 或 索引向量/焦耳
5. **与最先进方法对比**：与其他 UVM 优化方法比较（如 NVIDIA UMM 提示、应用导向预取）
6. **敏感性分析**：变化聚类数量（IVF 聚类）、向量维度和数据分布

---

## 6. 结论

### 6.1 主要结论

1. **预取优化有效**：对超出 GPU VRAM 的数据集，索引构建时间减少 21-29%，搜索吞吐量提升 10-16%
2. **改进的可扩展性**：收益随数据集规模增加，表明该方法对十亿级部署特别有价值
3. **UVM 实现大规模 GPU 处理**：通过优化，单个 GPU 可索引和搜索 1 亿向量，尽管有显著性能开销
4. **设备内存仍是最优选择**：当数据可放入 GPU VRAM 时，设备内存比 CPU 提供 29 倍性能优势

### 6.2 部署建议

- **数据集 < 50% GPU VRAM**：使用 GPU 设备内存获得最大性能
- **数据集 50-150% GPU VRAM**：使用带预取优化的 UVM
- **数据集 > 150% GPU VRAM**：考虑多 GPU 分片或 CPU + GPU 混合方案

### 6.3 未来工作

1. 在 SIFT1B（10 亿向量）上评估以验证十亿级声明
2. 与生产向量数据库集成（Milvus、Pinecone）
3. 探索硬件特定优化（NVIDIA Grace Hopper 统一内存）
4. 开发自动内存管理策略选择的成本模型

---

## 附录 A：原始实验数据

### A.1 SIFT20M 结果

**GPU 设备内存：**
```json
{
  "index_add": {"total_time": 3.71},
  "search": [
    {"nprobe": 1, "search_time": 0.029, "qps": 348610, "recall": {"1-R@1": 0.4158}},
    {"nprobe": 4, "search_time": 0.070, "qps": 142035, "recall": {"1-R@1": 0.7343}},
    {"nprobe": 16, "search_time": 0.258, "qps": 38721, "recall": {"1-R@1": 0.933}}
  ]
}
```

**CPU：**
```json
{
  "index_add": {"total_time": 77.97},
  "search": [
    {"nprobe": 1, "search_time": 0.875, "qps": 11434, "recall": {"1-R@1": 0.4158}},
    {"nprobe": 4, "search_time": 2.004, "qps": 4990, "recall": {"1-R@1": 0.7343}},
    {"nprobe": 16, "search_time": 7.612, "qps": 1314, "recall": {"1-R@1": 0.933}}
  ]
}
```

### A.2 SIFT50M 结果

**UVM 基线：**
```json
{
  "index_add": {"total_time": 16.77},
  "search": [
    {"nprobe": 1, "search_time": 0.048, "qps": 207802, "recall": {"1-R@1": 0.4387}},
    {"nprobe": 4, "search_time": 0.168, "qps": 59594, "recall": {"1-R@1": 0.7574}},
    {"nprobe": 16, "search_time": 0.642, "qps": 15585, "recall": {"1-R@1": 0.9436}}
  ]
}
```

**UVM + 预取：**
```json
{
  "index_add": {"total_time": 13.19},
  "search": [
    {"nprobe": 1, "search_time": 0.049, "qps": 204700, "recall": {"1-R@1": 0.4387}},
    {"nprobe": 4, "search_time": 0.168, "qps": 59530, "recall": {"1-R@1": 0.7574}},
    {"nprobe": 16, "search_time": 0.640, "qps": 15631, "recall": {"1-R@1": 0.9436}}
  ]
}
```

### A.3 SIFT100M 结果

**UVM 基线：**
```json
{
  "index_add": {"total_time": 67.77},
  "search": [
    {"nprobe": 1, "search_time": 14.04, "qps": 712, "recall": {"1-R@1": 0.4486}},
    {"nprobe": 4, "search_time": 15.96, "qps": 627, "recall": {"1-R@1": 0.7655}},
    {"nprobe": 16, "search_time": 57.44, "qps": 174, "recall": {"1-R@1": 0.9476}}
  ]
}
```

**UVM + 预取：**
```json
{
  "index_add": {"total_time": 48.15},
  "search": [
    {"nprobe": 1, "search_time": 11.80, "qps": 847, "recall": {"1-R@1": 0.4486}},
    {"nprobe": 4, "search_time": 14.18, "qps": 705, "recall": {"1-R@1": 0.7655}},
    {"nprobe": 16, "search_time": 51.42, "qps": 194, "recall": {"1-R@1": 0.9476}}
  ]
}
```

---

## 附录 B：基准测试脚本说明

### B.1 GPU 基准测试（`bench_gpu_1bn.py`）

**主要功能：**
- 支持 GPU 设备内存和 UVM 模式（`-uvm` 标志）
- 可配置 nprobe 值、批次大小和 float16 模式
- 内存映射数据集加载以提高效率
- JSON 输出包含详细进度跟踪

**使用方法：**
```bash
# GPU 设备内存
uv run python bench_gpu_1bn.py SIFT100M IVF4096,Flat -nprobe 1,4,16

# GPU UVM
uv run python bench_gpu_1bn.py SIFT100M IVF4096,Flat -nprobe 1,4,16 -uvm
```

### B.2 CPU 基准测试（`bench_cpu_1bn.py`）

**主要功能：**
- 使用系统 RAM 的纯 CPU 执行
- 与 GPU 基准测试相同的索引类型和评估指标
- 作为 GPU 加速对比的基线

**使用方法：**
```bash
uv run python bench_cpu_1bn.py SIFT20M IVF4096,Flat -nprobe 1,4,16
```

---

## 参考文献

1. Johnson, J., Douze, M., & Jégou, H. (2019). Billion-scale similarity search with GPUs. *IEEE Transactions on Big Data*.
2. Jégou, H., Douze, M., & Schmid, C. (2011). Product quantization for nearest neighbor search. *IEEE TPAMI*.
3. NVIDIA CUDA Documentation: Unified Memory Programming. https://docs.nvidia.com/cuda/cuda-c-programming-guide/
4. FAISS Wiki: https://github.com/facebookresearch/faiss/wiki
