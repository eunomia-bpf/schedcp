[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:36 [api_server.py:1772] vLLM API server version 0.12.0
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:36 [utils.py:253] non-default args: {'model_tag': 'Qwen/Qwen3-30B-A3B-FP8', 'model': 'Qwen/Qwen3-30B-A3B-FP8', 'max_model_len': 2048, 'enforce_eager': True, 'cpu_offload_gb': 4.0, 'kv_transfer_config': KVTransferConfig(kv_connector='LMCacheConnectorV1', engine_id='86fc5379-fe00-4b9b-893a-0f6c0706a970', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None, enable_permute_local_kv=False)}
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:40 [model.py:637] Resolved architecture: Qwen3MoeForCausalLM
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:40 [model.py:1750] Using max model len 2048
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:40 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=476072)[0;0m WARNING 12-08 02:07:40 [vllm.py:601] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:40 [vllm.py:707] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=476072)[0;0m WARNING 12-08 02:07:40 [vllm.py:896] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:43 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='Qwen/Qwen3-30B-A3B-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'all', '+quant_fp8'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:44 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://128.114.59.195:41247 backend=nccl
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:44 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:44 [gpu_model_runner.py:3467] Starting to load model Qwen/Qwen3-30B-A3B-FP8...
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:45 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:45 [layer.py:379] Enabled separate cuda stream for MoE shared_experts
[0;36m(EngineCore_DP0 pid=476226)[0;0m WARNING 12-08 02:07:45 [fp8.py:183] DeepGEMM backend requested but not available.
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:45 [fp8.py:202] Using Triton backend for FP8 MoE
[0;36m(EngineCore_DP0 pid=476226)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=476226)[0;0m Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:02,  2.01it/s]
[0;36m(EngineCore_DP0 pid=476226)[0;0m Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:01<00:02,  1.70it/s]
[0;36m(EngineCore_DP0 pid=476226)[0;0m Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:01<00:02,  1.63it/s]
[0;36m(EngineCore_DP0 pid=476226)[0;0m Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:02<00:01,  2.05it/s]
[0;36m(EngineCore_DP0 pid=476226)[0;0m Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:02<00:01,  1.85it/s]
[0;36m(EngineCore_DP0 pid=476226)[0;0m Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:03<00:00,  1.73it/s]
[0;36m(EngineCore_DP0 pid=476226)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:04<00:00,  1.66it/s]
[0;36m(EngineCore_DP0 pid=476226)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:04<00:00,  1.74it/s]
[0;36m(EngineCore_DP0 pid=476226)[0;0m 
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:50 [default_loader.py:308] Loading weights took 4.07 seconds
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:51 [gpu_model_runner.py:3549] Model loading took 24.9801 GiB memory and 5.927862 seconds
[0;36m(EngineCore_DP0 pid=476226)[0;0m WARNING 12-08 02:07:51 [fused_moe.py:888] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_GeForce_RTX_5090,dtype=fp8_w8a8,block_shape=[128,128].json']
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:52 [gpu_worker.py:359] Available KV cache memory: 1.76 GiB
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:52 [kv_cache_utils.py:1286] GPU KV cache size: 19,168 tokens
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:52 [kv_cache_utils.py:1291] Maximum concurrency for 2,048 tokens per request: 9.36x
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:53 [factory.py:64] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 86fc5379-fe00-4b9b-893a-0f6c0706a970
[0;36m(EngineCore_DP0 pid=476226)[0;0m WARNING 12-08 02:07:53 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:53 [lmcache_connector.py:52] Initializing latest dev LMCache connector
[0;36m(EngineCore_DP0 pid=476226)[0;0m [33;20m[2025-12-08 02:07:53,012] LMCache WARNING:[0m No LMCache configuration file is set. Trying to read configurations from the environment variables. [3m(utils.py:61:lmcache.integration.vllm.utils)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [33;20m[2025-12-08 02:07:53,012] LMCache WARNING:[0m You can set the configuration file through the environment variable: LMCACHE_CONFIG_FILE [3m(utils.py:65:lmcache.integration.vllm.utils)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,012] LMCache INFO:[0m use mla: False, kv shape: (48, 2, 256, 4, 128), num_draft_layers:0 [3m(vllm_v1_adapter.py:503:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,012] LMCache INFO:[0m CUDA device is available. Using CUDA for LMCache engine. [3m(vllm_v1_adapter.py:509:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,013] LMCache INFO:[0m Creating LMCacheEngine instance vllm-instance [3m(cache_engine.py:1531:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,013] LMCache INFO:[0m NUMA mapping for instance vllm-instance: None [3m(cache_engine.py:1534:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [33;20m[2025-12-08 02:07:53,013] LMCache WARNING:[0m Could not load 'builtin' from vLLM. Using builtin hash. This may cause inconsistencies in distributed caching. [3m(token_database.py:136:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [33;20m[2025-12-08 02:07:53,013] LMCache WARNING:[0m Using builtin hash without PYTHONHASHSEED set. For production environments (non-testing scenarios), you MUST set PYTHONHASHSEED to ensure consistent hashing across processes. Example: export PYTHONHASHSEED=0 [3m(token_database.py:143:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,013] LMCache INFO:[0m Initialized NONE_HASH=b"\xe0\x8cN\xe0\xf7\xee\xfe\x14\x08r\xcf\x9a\xd4\xdd'\xd0(\xb8_\x91Ka\x1a\xff\xa0\xdc\xc7eN\xb2\x92\xb1" from vLLM (>= PR#20511) [3m(token_database.py:74:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,013] LMCache INFO:[0m Using hash algorithm: builtin [3m(token_database.py:84:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,014] LMCache INFO:[0m sending cache usage stats to http://stats.lmcache.ai:8080/cache-usage [3m(usage_context.py:290:lmcache.usage_context)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,016] LMCache INFO:[0m Creating LMCacheEngine with config: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': 8.0, 'reserve_local_cpu_size': 0.0, 'local_disk': None, 'max_local_disk_size': 0.0, 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'p2p_host': None, 'p2p_init_ports': None, 'p2p_lookup_ports': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_instance_38ef555de0a24963b9ffd56a4586b44c', 'controller_pull_url': None, 'controller_reply_url': None, 'lmcache_worker_ports': None, 'lmcache_worker_ids': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_pd': False, 'pd_role': None, 'pd_buffer_size': None, 'pd_buffer_device': None, 'pd_peer_host': None, 'pd_peer_init_port': None, 'pd_peer_alloc_port': None, 'pd_proxy_host': None, 'pd_proxy_port': None, 'transfer_channel': None, 'nixl_backends': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None, 'lookup_timeout_ms': 3000, 'hit_miss_ratio': None, 'lookup_server_worker_ids': None, 'enable_scheduler_bypass_lookup': False, 'script_allowed_imports': None, 'enable_lazy_memory_allocator': False, 'lazy_memory_initial_ratio': 0.2, 'lazy_memory_expand_trigger_ratio': 0.5, 'lazy_memory_step_ratio': 0.1, 'lazy_memory_safe_size': 0.0, 'enable_chunk_statistics': False, 'chunk_statistics_auto_start_statistics': False, 'chunk_statistics_auto_exit_timeout_hours': 0.0, 'chunk_statistics_auto_exit_target_unique_chunks': 0, 'chunk_statistics_strategy': 'memory_bloom_filter'} [3m(cache_engine.py:93:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,016] LMCache INFO:[0m LMCacheWorker is not initialized (related configs: enable_controller: False, role: worker, worker_id: 0, worker_ids: [0]). [3m(cache_engine.py:135:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,016] LMCache INFO:[0m Initialize storage manager on rank 0, use layerwise: False,save only first rank: False [3m(cache_engine.py:167:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,017] LMCache INFO:[0m Initializing LRUCachePolicy [3m(lru.py:22:lmcache.v1.storage_backend.cache_policy.lru)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:53,017] LMCache INFO:[0m NUMA mapping None [3m(local_cpu_backend.py:351:lmcache.v1.storage_backend.local_cpu_backend)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [33;20m[2025-12-08 02:07:54,456] LMCache WARNING:[0m Controller message sender is not initialized [3m(local_cpu_backend.py:103:lmcache.v1.storage_backend.local_cpu_backend)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:54,456] LMCache INFO:[0m Initializing usage context. [3m(usage_context.py:362:lmcache.usage_context)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:55,914] LMCache INFO:[0m lmcache lookup server start on /tmp/engine_86fc5379-fe00-4b9b-893a-0f6c0706a970_service_lookup_lmcache_rpc_port_0 [3m(lmcache_lookup_client.py:329:lmcache.v1.lookup_client.lmcache_lookup_client)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:55,916] LMCache INFO:[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=1, port=7000, socket_path=None, include_index_list=None [3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:55,917] LMCache INFO:[0m LMCache initialized for role KVConnectorRole.WORKER with version 0.3.11.dev18-g58cae13ba, vllm version 0.12.0, lmcache cache_engine metadata: LMCacheEngineMetadata(model_name='Qwen/Qwen3-30B-A3B-FP8', world_size=1, worker_id=0, fmt='vllm', kv_dtype=torch.bfloat16, kv_shape=(48, 2, 256, 4, 128), use_mla=False, role='worker') [3m(vllm_v1_adapter.py:793:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:55 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=476226)[0;0m 2025-12-08 02:07:55,920 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=476226)[0;0m 2025-12-08 02:07:55,941 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:56 [core.py:254] init engine (profile, create kv cache, warmup model) took 5.75 seconds
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:57 [factory.py:64] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 86fc5379-fe00-4b9b-893a-0f6c0706a970
[0;36m(EngineCore_DP0 pid=476226)[0;0m WARNING 12-08 02:07:57 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:57 [lmcache_connector.py:52] Initializing latest dev LMCache connector
[0;36m(EngineCore_DP0 pid=476226)[0;0m [31;20m[2025-12-08 02:07:57,437] LMCache ERROR:[0m PrometheusLogger instance already created withdifferent metadata. This should not happen except in test [3m(observability.py:1325:lmcache.observability)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:57,438] LMCache INFO:[0m lmcache lookup client connect to rank 0 with socket path /tmp/engine_86fc5379-fe00-4b9b-893a-0f6c0706a970_service_lookup_lmcache_rpc_port_0 [3m(lmcache_lookup_client.py:93:lmcache.v1.lookup_client.lmcache_lookup_client)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [33;20m[2025-12-08 02:07:57,438] LMCache WARNING:[0m Could not load 'builtin' from vLLM. Using builtin hash. This may cause inconsistencies in distributed caching. [3m(token_database.py:136:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [33;20m[2025-12-08 02:07:57,438] LMCache WARNING:[0m Using builtin hash without PYTHONHASHSEED set. For production environments (non-testing scenarios), you MUST set PYTHONHASHSEED to ensure consistent hashing across processes. Example: export PYTHONHASHSEED=0 [3m(token_database.py:143:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:57,438] LMCache INFO:[0m Initialized NONE_HASH=b'\xd7\xd6<\xbd\x85\x1cS\xa8_\x88<*z 7lj\x833\x059\xf3\xfe\xf9\x95\xf7\x94\xc0\x06\xa8v\xad' from vLLM (>= PR#20511) [3m(token_database.py:74:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:57,438] LMCache INFO:[0m Using hash algorithm: builtin [3m(token_database.py:84:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:57,438] LMCache INFO:[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=0, port=6999, socket_path=None, include_index_list=None [3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:07:57,438] LMCache INFO:[0m LMCache initialized for role KVConnectorRole.SCHEDULER with version 0.3.11.dev18-g58cae13ba, vllm version 0.12.0, lmcache cache_engine metadata: None [3m(vllm_v1_adapter.py:793:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m WARNING 12-08 02:07:57 [vllm.py:608] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=476226)[0;0m INFO 12-08 02:07:57 [vllm.py:707] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:57 [api_server.py:1520] Supported tasks: ['generate']
[0;36m(APIServer pid=476072)[0;0m WARNING 12-08 02:07:57 [model.py:1576] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:57 [serving_responses.py:194] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [serving_completion.py:73] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [api_server.py:1847] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:07:58 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=476072)[0;0m INFO:     Started server process [476072]
[0;36m(APIServer pid=476072)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=476072)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:44784 - "GET /health HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:44792 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:07,820] LMCache INFO:[0m Reqid: cmpl-a3427742d244e480-0, Total tokens 729, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:07,824] LMCache INFO:[0m Post-initializing LMCacheEngine [3m(cache_engine.py:226:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:08,136] LMCache INFO:[0m Storing KV cache for 729 out of 729 tokens (skip_leading_tokens=0) for request cmpl-a3427742d244e480-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:08,146] LMCache INFO:[0m Stored 729 out of total 729 tokens. size: 0.0667 gb, cost 3.2050 ms, throughput: 20.8241 GB/s; offload_time: 3.1817 ms, put_time: 0.0234 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:08:08 [loggers.py:236] Engine 000: Avg prompt throughput: 68.7 tokens/s, Avg generation throughput: 0.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.8%, Prefix cache hit rate: 0.0%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:44792 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:10,545] LMCache INFO:[0m Reqid: cmpl-benchmark-serving0-0, Total tokens 729, LMCache hit tokens: 729, need to load: 8 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:10,548] LMCache INFO:[0m Retrieved 217 out of 217 required tokens (from 729 total tokens). size: 0.0199 gb, cost 0.6149 ms, throughput: 32.3115 GB/s; [3m(cache_engine.py:582:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43086 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,207] LMCache INFO:[0m Reqid: cmpl-benchmark-serving1-0, Total tokens 9, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,252] LMCache INFO:[0m Storing KV cache for 9 out of 9 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving1-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,252] LMCache INFO:[0m Stored 9 out of total 9 tokens. size: 0.0008 gb, cost 0.1735 ms, throughput: 4.7485 GB/s; offload_time: 0.1617 ms, put_time: 0.0118 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43096 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,501] LMCache INFO:[0m Reqid: cmpl-benchmark-serving2-0, Total tokens 110, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,589] LMCache INFO:[0m Storing KV cache for 110 out of 110 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving2-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,590] LMCache INFO:[0m Stored 110 out of total 110 tokens. size: 0.0101 gb, cost 0.4408 ms, throughput: 22.8466 GB/s; offload_time: 0.4309 ms, put_time: 0.0099 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43102 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,713] LMCache INFO:[0m Reqid: cmpl-benchmark-serving3-0, Total tokens 59, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43106 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43112 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43114 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,798] LMCache INFO:[0m Storing KV cache for 59 out of 59 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving3-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,798] LMCache INFO:[0m Stored 59 out of total 59 tokens. size: 0.0054 gb, cost 0.3258 ms, throughput: 16.5802 GB/s; offload_time: 0.3141 ms, put_time: 0.0117 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,800] LMCache INFO:[0m Reqid: cmpl-benchmark-serving4-0, Total tokens 21, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,801] LMCache INFO:[0m Reqid: cmpl-benchmark-serving5-0, Total tokens 57, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,802] LMCache INFO:[0m Reqid: cmpl-benchmark-serving6-0, Total tokens 36, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,900] LMCache INFO:[0m Storing KV cache for 21 out of 21 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving4-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,900] LMCache INFO:[0m Stored 21 out of total 21 tokens. size: 0.0019 gb, cost 0.1495 ms, throughput: 12.8563 GB/s; offload_time: 0.1424 ms, put_time: 0.0072 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,900] LMCache INFO:[0m Storing KV cache for 57 out of 57 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving5-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,900] LMCache INFO:[0m Stored 57 out of total 57 tokens. size: 0.0052 gb, cost 0.2190 ms, throughput: 23.8302 GB/s; offload_time: 0.2139 ms, put_time: 0.0050 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,900] LMCache INFO:[0m Storing KV cache for 36 out of 36 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving6-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:11,901] LMCache INFO:[0m Stored 36 out of total 36 tokens. size: 0.0033 gb, cost 0.1539 ms, throughput: 21.4188 GB/s; offload_time: 0.1493 ms, put_time: 0.0046 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43120 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,233] LMCache INFO:[0m Reqid: cmpl-benchmark-serving7-0, Total tokens 78, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,325] LMCache INFO:[0m Storing KV cache for 78 out of 78 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving7-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,325] LMCache INFO:[0m Stored 78 out of total 78 tokens. size: 0.0071 gb, cost 0.3455 ms, throughput: 20.6665 GB/s; offload_time: 0.3366 ms, put_time: 0.0089 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43136 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,427] LMCache INFO:[0m Reqid: cmpl-benchmark-serving8-0, Total tokens 667, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,666] LMCache INFO:[0m Storing KV cache for 667 out of 667 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving8-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,668] LMCache INFO:[0m Stored 667 out of total 667 tokens. size: 0.0611 gb, cost 1.9883 ms, throughput: 30.7127 GB/s; offload_time: 1.9721 ms, put_time: 0.0161 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43148 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43156 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,719] LMCache INFO:[0m Reqid: cmpl-benchmark-serving9-0, Total tokens 414, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,720] LMCache INFO:[0m Reqid: cmpl-benchmark-serving10-0, Total tokens 227, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,954] LMCache INFO:[0m Storing KV cache for 414 out of 414 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving9-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,955] LMCache INFO:[0m Stored 414 out of total 414 tokens. size: 0.0379 gb, cost 1.3006 ms, throughput: 29.1435 GB/s; offload_time: 1.2875 ms, put_time: 0.0131 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,955] LMCache INFO:[0m Storing KV cache for 227 out of 227 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving10-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:12,956] LMCache INFO:[0m Stored 227 out of total 227 tokens. size: 0.0208 gb, cost 0.6742 ms, throughput: 30.8271 GB/s; offload_time: 0.6687 ms, put_time: 0.0055 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43148 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:13,461] LMCache INFO:[0m Reqid: cmpl-benchmark-serving11-0, Total tokens 7, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:13,531] LMCache INFO:[0m Storing KV cache for 7 out of 7 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving11-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:13,531] LMCache INFO:[0m Stored 7 out of total 7 tokens. size: 0.0006 gb, cost 0.1684 ms, throughput: 3.8053 GB/s; offload_time: 0.1582 ms, put_time: 0.0103 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43160 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:13,880] LMCache INFO:[0m Reqid: cmpl-benchmark-serving12-0, Total tokens 379, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43174 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43186 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43192 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,050] LMCache INFO:[0m Storing KV cache for 379 out of 379 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving12-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,051] LMCache INFO:[0m Stored 379 out of total 379 tokens. size: 0.0347 gb, cost 1.1857 ms, throughput: 29.2643 GB/s; offload_time: 1.1732 ms, put_time: 0.0125 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,052] LMCache INFO:[0m Reqid: cmpl-benchmark-serving13-0, Total tokens 401, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,053] LMCache INFO:[0m Reqid: cmpl-benchmark-serving14-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,054] LMCache INFO:[0m Reqid: cmpl-benchmark-serving15-0, Total tokens 743, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43194 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43204 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43220 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43224 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,432] LMCache INFO:[0m Storing KV cache for 401 out of 401 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving13-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,433] LMCache INFO:[0m Stored 401 out of total 401 tokens. size: 0.0367 gb, cost 1.2647 ms, throughput: 29.0298 GB/s; offload_time: 1.2513 ms, put_time: 0.0134 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,434] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving14-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,434] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1222 ms, throughput: 12.7408 GB/s; offload_time: 0.1148 ms, put_time: 0.0074 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,434] LMCache INFO:[0m Storing KV cache for 743 out of 743 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving15-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,436] LMCache INFO:[0m Stored 743 out of total 743 tokens. size: 0.0680 gb, cost 2.0754 ms, throughput: 32.7760 GB/s; offload_time: 2.0685 ms, put_time: 0.0069 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,437] LMCache INFO:[0m Reqid: cmpl-benchmark-serving16-0, Total tokens 245, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,438] LMCache INFO:[0m Reqid: cmpl-benchmark-serving17-0, Total tokens 20, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,439] LMCache INFO:[0m Reqid: cmpl-benchmark-serving18-0, Total tokens 403, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,440] LMCache INFO:[0m Reqid: cmpl-benchmark-serving19-0, Total tokens 325, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43228 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43232 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43236 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,774] LMCache INFO:[0m Storing KV cache for 245 out of 245 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving16-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,774] LMCache INFO:[0m Stored 245 out of total 245 tokens. size: 0.0224 gb, cost 0.8149 ms, throughput: 27.5256 GB/s; offload_time: 0.8042 ms, put_time: 0.0107 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,775] LMCache INFO:[0m Storing KV cache for 20 out of 20 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving17-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,775] LMCache INFO:[0m Stored 20 out of total 20 tokens. size: 0.0018 gb, cost 0.1271 ms, throughput: 14.4039 GB/s; offload_time: 0.1217 ms, put_time: 0.0055 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,775] LMCache INFO:[0m Storing KV cache for 403 out of 403 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving18-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,776] LMCache INFO:[0m Stored 403 out of total 403 tokens. size: 0.0369 gb, cost 1.1621 ms, throughput: 31.7480 GB/s; offload_time: 1.1558 ms, put_time: 0.0063 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,776] LMCache INFO:[0m Storing KV cache for 325 out of 325 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving19-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,777] LMCache INFO:[0m Stored 325 out of total 325 tokens. size: 0.0298 gb, cost 0.9524 ms, throughput: 31.2417 GB/s; offload_time: 0.9462 ms, put_time: 0.0062 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,780] LMCache INFO:[0m Reqid: cmpl-benchmark-serving20-0, Total tokens 61, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,781] LMCache INFO:[0m Reqid: cmpl-benchmark-serving21-0, Total tokens 412, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:14,782] LMCache INFO:[0m Reqid: cmpl-benchmark-serving22-0, Total tokens 88, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43246 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43248 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,003] LMCache INFO:[0m Storing KV cache for 61 out of 61 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving20-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,004] LMCache INFO:[0m Stored 61 out of total 61 tokens. size: 0.0056 gb, cost 0.3265 ms, throughput: 17.1044 GB/s; offload_time: 0.3144 ms, put_time: 0.0121 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,004] LMCache INFO:[0m Storing KV cache for 412 out of 412 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving21-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,005] LMCache INFO:[0m Stored 412 out of total 412 tokens. size: 0.0377 gb, cost 1.2010 ms, throughput: 31.4071 GB/s; offload_time: 1.1938 ms, put_time: 0.0072 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,005] LMCache INFO:[0m Storing KV cache for 88 out of 88 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving22-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,005] LMCache INFO:[0m Stored 88 out of total 88 tokens. size: 0.0081 gb, cost 0.2990 ms, throughput: 26.9451 GB/s; offload_time: 0.2945 ms, put_time: 0.0045 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,008] LMCache INFO:[0m Reqid: cmpl-benchmark-serving23-0, Total tokens 181, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,009] LMCache INFO:[0m Reqid: cmpl-benchmark-serving24-0, Total tokens 26, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,125] LMCache INFO:[0m Storing KV cache for 181 out of 181 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving23-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,125] LMCache INFO:[0m Stored 181 out of total 181 tokens. size: 0.0166 gb, cost 0.6250 ms, throughput: 26.5151 GB/s; offload_time: 0.6148 ms, put_time: 0.0102 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,126] LMCache INFO:[0m Storing KV cache for 26 out of 26 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving24-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,126] LMCache INFO:[0m Stored 26 out of total 26 tokens. size: 0.0024 gb, cost 0.1371 ms, throughput: 17.3562 GB/s; offload_time: 0.1321 ms, put_time: 0.0050 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43254 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,351] LMCache INFO:[0m Reqid: cmpl-benchmark-serving25-0, Total tokens 28, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43268 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,443] LMCache INFO:[0m Storing KV cache for 28 out of 28 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving25-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,444] LMCache INFO:[0m Stored 28 out of total 28 tokens. size: 0.0026 gb, cost 0.2287 ms, throughput: 11.2081 GB/s; offload_time: 0.2175 ms, put_time: 0.0112 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,446] LMCache INFO:[0m Reqid: cmpl-benchmark-serving26-0, Total tokens 188, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43280 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,557] LMCache INFO:[0m Storing KV cache for 188 out of 188 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving26-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,558] LMCache INFO:[0m Stored 188 out of total 188 tokens. size: 0.0172 gb, cost 0.6462 ms, throughput: 26.6349 GB/s; offload_time: 0.6363 ms, put_time: 0.0099 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,560] LMCache INFO:[0m Reqid: cmpl-benchmark-serving27-0, Total tokens 507, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43292 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43308 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,769] LMCache INFO:[0m Storing KV cache for 507 out of 507 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving27-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,770] LMCache INFO:[0m Stored 507 out of total 507 tokens. size: 0.0464 gb, cost 1.5649 ms, throughput: 29.6607 GB/s; offload_time: 1.5535 ms, put_time: 0.0115 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,772] LMCache INFO:[0m Reqid: cmpl-benchmark-serving28-0, Total tokens 249, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:15,773] LMCache INFO:[0m Reqid: cmpl-benchmark-serving29-0, Total tokens 384, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43314 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43320 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43334 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,013] LMCache INFO:[0m Storing KV cache for 249 out of 249 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving28-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,014] LMCache INFO:[0m Stored 249 out of total 249 tokens. size: 0.0228 gb, cost 0.8352 ms, throughput: 27.2934 GB/s; offload_time: 0.8225 ms, put_time: 0.0127 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,014] LMCache INFO:[0m Storing KV cache for 384 out of 384 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving29-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,015] LMCache INFO:[0m Stored 384 out of total 384 tokens. size: 0.0352 gb, cost 1.1208 ms, throughput: 31.3658 GB/s; offload_time: 1.1089 ms, put_time: 0.0120 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,017] LMCache INFO:[0m Reqid: cmpl-benchmark-serving30-0, Total tokens 835, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,018] LMCache INFO:[0m Reqid: cmpl-benchmark-serving31-0, Total tokens 386, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,018] LMCache INFO:[0m Reqid: cmpl-benchmark-serving32-0, Total tokens 5, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,413] LMCache INFO:[0m Storing KV cache for 835 out of 835 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving30-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,416] LMCache INFO:[0m Stored 835 out of total 835 tokens. size: 0.0764 gb, cost 2.4316 ms, throughput: 31.4383 GB/s; offload_time: 2.4173 ms, put_time: 0.0143 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,416] LMCache INFO:[0m Storing KV cache for 386 out of 386 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving31-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,417] LMCache INFO:[0m Stored 386 out of total 386 tokens. size: 0.0353 gb, cost 1.1129 ms, throughput: 31.7554 GB/s; offload_time: 1.1067 ms, put_time: 0.0062 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,417] LMCache INFO:[0m Storing KV cache for 5 out of 5 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving32-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,417] LMCache INFO:[0m Stored 5 out of total 5 tokens. size: 0.0005 gb, cost 0.0698 ms, throughput: 6.5593 GB/s; offload_time: 0.0652 ms, put_time: 0.0045 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43246 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,667] LMCache INFO:[0m Reqid: cmpl-benchmark-serving33-0, Total tokens 679, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,942] LMCache INFO:[0m Storing KV cache for 679 out of 679 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving33-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:16,944] LMCache INFO:[0m Stored 679 out of total 679 tokens. size: 0.0622 gb, cost 2.0323 ms, throughput: 30.5882 GB/s; offload_time: 2.0171 ms, put_time: 0.0152 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43320 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:17,380] LMCache INFO:[0m Reqid: cmpl-benchmark-serving34-0, Total tokens 25, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:17,483] LMCache INFO:[0m Storing KV cache for 25 out of 25 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving34-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:17,483] LMCache INFO:[0m Stored 25 out of total 25 tokens. size: 0.0023 gb, cost 0.2200 ms, throughput: 10.4032 GB/s; offload_time: 0.2098 ms, put_time: 0.0103 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43346 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:17,753] LMCache INFO:[0m Reqid: cmpl-benchmark-serving35-0, Total tokens 742, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43220 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43348 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,019] LMCache INFO:[0m Storing KV cache for 742 out of 742 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving35-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,021] LMCache INFO:[0m Stored 742 out of total 742 tokens. size: 0.0679 gb, cost 2.2396 ms, throughput: 30.3329 GB/s; offload_time: 2.2250 ms, put_time: 0.0146 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,023] LMCache INFO:[0m Reqid: cmpl-benchmark-serving36-0, Total tokens 385, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,024] LMCache INFO:[0m Reqid: cmpl-benchmark-serving37-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34290 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34298 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,226] LMCache INFO:[0m Storing KV cache for 385 out of 385 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving36-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,227] LMCache INFO:[0m Stored 385 out of total 385 tokens. size: 0.0352 gb, cost 1.2186 ms, throughput: 28.9260 GB/s; offload_time: 1.2060 ms, put_time: 0.0126 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,227] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving37-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,227] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.2257 ms, throughput: 23.5319 GB/s; offload_time: 0.2200 ms, put_time: 0.0056 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,229] LMCache INFO:[0m Reqid: cmpl-benchmark-serving38-0, Total tokens 332, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,230] LMCache INFO:[0m Reqid: cmpl-benchmark-serving39-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34304 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:08:18 [loggers.py:236] Engine 000: Avg prompt throughput: 1018.6 tokens/s, Avg generation throughput: 125.4 tokens/s, Running: 34 reqs, Waiting: 0 reqs, GPU KV cache usage: 52.7%, Prefix cache hit rate: 6.6%, External prefix cache hit rate: 0.1%
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,386] LMCache INFO:[0m Storing KV cache for 332 out of 332 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving38-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,387] LMCache INFO:[0m Stored 332 out of total 332 tokens. size: 0.0304 gb, cost 1.0460 ms, throughput: 29.0587 GB/s; offload_time: 1.0349 ms, put_time: 0.0111 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,387] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving39-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,387] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.0956 ms, throughput: 10.5317 GB/s; offload_time: 0.0906 ms, put_time: 0.0051 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34310 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,389] LMCache INFO:[0m Reqid: cmpl-benchmark-serving40-0, Total tokens 37, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34320 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,493] LMCache INFO:[0m Storing KV cache for 37 out of 37 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving40-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,494] LMCache INFO:[0m Stored 37 out of total 37 tokens. size: 0.0034 gb, cost 0.2345 ms, throughput: 14.4462 GB/s; offload_time: 0.2256 ms, put_time: 0.0089 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,495] LMCache INFO:[0m Reqid: cmpl-benchmark-serving41-0, Total tokens 161, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,496] LMCache INFO:[0m Reqid: cmpl-benchmark-serving42-0, Total tokens 788, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,805] LMCache INFO:[0m Storing KV cache for 161 out of 161 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving41-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,806] LMCache INFO:[0m Stored 161 out of total 161 tokens. size: 0.0147 gb, cost 0.6082 ms, throughput: 24.2352 GB/s; offload_time: 0.5965 ms, put_time: 0.0117 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,806] LMCache INFO:[0m Storing KV cache for 788 out of 788 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving42-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,808] LMCache INFO:[0m Stored 788 out of total 788 tokens. size: 0.0721 gb, cost 2.2118 ms, throughput: 32.6175 GB/s; offload_time: 2.2030 ms, put_time: 0.0088 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43160 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:18,983] LMCache INFO:[0m Reqid: cmpl-benchmark-serving43-0, Total tokens 30, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34332 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,082] LMCache INFO:[0m Storing KV cache for 30 out of 30 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving43-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,082] LMCache INFO:[0m Stored 30 out of total 30 tokens. size: 0.0027 gb, cost 0.2313 ms, throughput: 11.8732 GB/s; offload_time: 0.2208 ms, put_time: 0.0105 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,084] LMCache INFO:[0m Reqid: cmpl-benchmark-serving44-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,191] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving44-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,191] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.3318 ms, throughput: 16.0047 GB/s; offload_time: 0.3192 ms, put_time: 0.0126 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43232 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,284] LMCache INFO:[0m Reqid: cmpl-benchmark-serving45-0, Total tokens 24, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43194 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,386] LMCache INFO:[0m Storing KV cache for 24 out of 24 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving45-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,386] LMCache INFO:[0m Stored 24 out of total 24 tokens. size: 0.0022 gb, cost 0.2209 ms, throughput: 9.9486 GB/s; offload_time: 0.2106 ms, put_time: 0.0103 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,388] LMCache INFO:[0m Reqid: cmpl-benchmark-serving46-0, Total tokens 335, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43268 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,543] LMCache INFO:[0m Storing KV cache for 335 out of 335 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving46-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,545] LMCache INFO:[0m Stored 335 out of total 335 tokens. size: 0.0307 gb, cost 1.0196 ms, throughput: 30.0810 GB/s; offload_time: 1.0101 ms, put_time: 0.0094 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,547] LMCache INFO:[0m Reqid: cmpl-benchmark-serving47-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:44792 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,652] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving47-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,652] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.2840 ms, throughput: 18.7000 GB/s; offload_time: 0.2753 ms, put_time: 0.0087 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,654] LMCache INFO:[0m Reqid: cmpl-benchmark-serving48-0, Total tokens 273, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43346 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,799] LMCache INFO:[0m Storing KV cache for 273 out of 273 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving48-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,800] LMCache INFO:[0m Stored 273 out of total 273 tokens. size: 0.0250 gb, cost 0.8946 ms, throughput: 27.9384 GB/s; offload_time: 0.8825 ms, put_time: 0.0121 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,802] LMCache INFO:[0m Reqid: cmpl-benchmark-serving49-0, Total tokens 5, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,897] LMCache INFO:[0m Storing KV cache for 5 out of 5 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving49-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:19,898] LMCache INFO:[0m Stored 5 out of total 5 tokens. size: 0.0005 gb, cost 0.1085 ms, throughput: 4.2204 GB/s; offload_time: 0.1011 ms, put_time: 0.0074 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34304 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:20,523] LMCache INFO:[0m Reqid: cmpl-benchmark-serving50-0, Total tokens 48, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:20,624] LMCache INFO:[0m Storing KV cache for 48 out of 48 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving50-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:20,625] LMCache INFO:[0m Stored 48 out of total 48 tokens. size: 0.0044 gb, cost 0.2614 ms, throughput: 16.8111 GB/s; offload_time: 0.2525 ms, put_time: 0.0089 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43308 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:20,808] LMCache INFO:[0m Reqid: cmpl-benchmark-serving51-0, Total tokens 158, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:20,926] LMCache INFO:[0m Storing KV cache for 158 out of 158 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving51-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:20,927] LMCache INFO:[0m Stored 158 out of total 158 tokens. size: 0.0145 gb, cost 0.5521 ms, throughput: 26.1999 GB/s; offload_time: 0.5437 ms, put_time: 0.0085 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34320 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:21,460] LMCache INFO:[0m Reqid: cmpl-benchmark-serving52-0, Total tokens 306, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:21,604] LMCache INFO:[0m Storing KV cache for 306 out of 306 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving52-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:21,605] LMCache INFO:[0m Stored 306 out of total 306 tokens. size: 0.0280 gb, cost 0.9737 ms, throughput: 28.7717 GB/s; offload_time: 0.9628 ms, put_time: 0.0109 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43192 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:21,961] LMCache INFO:[0m Reqid: cmpl-benchmark-serving53-0, Total tokens 772, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43220 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,238] LMCache INFO:[0m Storing KV cache for 772 out of 772 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving53-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,241] LMCache INFO:[0m Stored 772 out of total 772 tokens. size: 0.0707 gb, cost 2.2641 ms, throughput: 31.2171 GB/s; offload_time: 2.2392 ms, put_time: 0.0249 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,243] LMCache INFO:[0m Reqid: cmpl-benchmark-serving54-0, Total tokens 315, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,394] LMCache INFO:[0m Storing KV cache for 315 out of 315 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving54-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,395] LMCache INFO:[0m Stored 315 out of total 315 tokens. size: 0.0288 gb, cost 1.0040 ms, throughput: 28.7246 GB/s; offload_time: 0.9926 ms, put_time: 0.0114 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:44792 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43254 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,671] LMCache INFO:[0m Reqid: cmpl-benchmark-serving55-0, Total tokens 20, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,672] LMCache INFO:[0m Reqid: cmpl-benchmark-serving56-0, Total tokens 75, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43224 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34320 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,789] LMCache INFO:[0m Storing KV cache for 20 out of 20 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving55-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,789] LMCache INFO:[0m Stored 20 out of total 20 tokens. size: 0.0018 gb, cost 0.2102 ms, throughput: 8.7102 GB/s; offload_time: 0.1987 ms, put_time: 0.0115 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,789] LMCache INFO:[0m Storing KV cache for 75 out of 75 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving56-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,790] LMCache INFO:[0m Stored 75 out of total 75 tokens. size: 0.0069 gb, cost 0.2718 ms, throughput: 25.2598 GB/s; offload_time: 0.2669 ms, put_time: 0.0050 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,791] LMCache INFO:[0m Reqid: cmpl-benchmark-serving57-0, Total tokens 85, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,792] LMCache INFO:[0m Reqid: cmpl-benchmark-serving58-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34344 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,907] LMCache INFO:[0m Storing KV cache for 85 out of 85 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving57-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,907] LMCache INFO:[0m Stored 85 out of total 85 tokens. size: 0.0078 gb, cost 0.3538 ms, throughput: 21.9934 GB/s; offload_time: 0.3447 ms, put_time: 0.0092 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,907] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving58-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,908] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.0977 ms, throughput: 10.3089 GB/s; offload_time: 0.0925 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34348 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:22,910] LMCache INFO:[0m Reqid: cmpl-benchmark-serving59-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34364 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,007] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving59-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,007] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1800 ms, throughput: 8.6477 GB/s; offload_time: 0.1696 ms, put_time: 0.0104 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,009] LMCache INFO:[0m Reqid: cmpl-benchmark-serving60-0, Total tokens 335, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,011] LMCache INFO:[0m Reqid: cmpl-benchmark-serving61-0, Total tokens 187, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,231] LMCache INFO:[0m Storing KV cache for 335 out of 335 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving60-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,232] LMCache INFO:[0m Stored 335 out of total 335 tokens. size: 0.0307 gb, cost 1.0361 ms, throughput: 29.6019 GB/s; offload_time: 1.0268 ms, put_time: 0.0093 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,232] LMCache INFO:[0m Storing KV cache for 187 out of 187 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving61-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,233] LMCache INFO:[0m Stored 187 out of total 187 tokens. size: 0.0171 gb, cost 0.5660 ms, throughput: 30.2478 GB/s; offload_time: 0.5607 ms, put_time: 0.0053 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34370 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,424] LMCache INFO:[0m Reqid: cmpl-benchmark-serving62-0, Total tokens 164, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43114 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34384 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,546] LMCache INFO:[0m Storing KV cache for 164 out of 164 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving62-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,547] LMCache INFO:[0m Stored 164 out of total 164 tokens. size: 0.0150 gb, cost 0.5802 ms, throughput: 25.8776 GB/s; offload_time: 0.5698 ms, put_time: 0.0104 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,549] LMCache INFO:[0m Reqid: cmpl-benchmark-serving63-0, Total tokens 14, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,550] LMCache INFO:[0m Reqid: cmpl-benchmark-serving64-0, Total tokens 548, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34386 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34394 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,783] LMCache INFO:[0m Storing KV cache for 14 out of 14 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving63-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,783] LMCache INFO:[0m Stored 14 out of total 14 tokens. size: 0.0013 gb, cost 0.1698 ms, throughput: 7.5506 GB/s; offload_time: 0.1596 ms, put_time: 0.0101 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,783] LMCache INFO:[0m Storing KV cache for 548 out of 548 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving64-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,785] LMCache INFO:[0m Stored 548 out of total 548 tokens. size: 0.0502 gb, cost 1.5600 ms, throughput: 32.1609 GB/s; offload_time: 1.5515 ms, put_time: 0.0085 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,787] LMCache INFO:[0m Reqid: cmpl-benchmark-serving65-0, Total tokens 691, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:23,787] LMCache INFO:[0m Reqid: cmpl-benchmark-serving66-0, Total tokens 135, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:24,076] LMCache INFO:[0m Storing KV cache for 691 out of 691 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving65-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:24,078] LMCache INFO:[0m Stored 691 out of total 691 tokens. size: 0.0633 gb, cost 2.0665 ms, throughput: 30.6140 GB/s; offload_time: 2.0530 ms, put_time: 0.0135 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:24,078] LMCache INFO:[0m Storing KV cache for 135 out of 135 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving66-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:24,079] LMCache INFO:[0m Stored 135 out of total 135 tokens. size: 0.0124 gb, cost 0.4301 ms, throughput: 28.7345 GB/s; offload_time: 0.4249 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43346 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34408 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:24,180] LMCache INFO:[0m Reqid: cmpl-benchmark-serving67-0, Total tokens 256, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:24,181] LMCache INFO:[0m Reqid: cmpl-benchmark-serving68-0, Total tokens 22, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:24,331] LMCache INFO:[0m Storing KV cache for 256 out of 256 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving67-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:24,332] LMCache INFO:[0m Stored 256 out of total 256 tokens. size: 0.0234 gb, cost 0.7894 ms, throughput: 29.6905 GB/s; offload_time: 0.7815 ms, put_time: 0.0079 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:24,332] LMCache INFO:[0m Storing KV cache for 22 out of 22 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving68-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:24,332] LMCache INFO:[0m Stored 22 out of total 22 tokens. size: 0.0020 gb, cost 0.1284 ms, throughput: 15.6873 GB/s; offload_time: 0.1228 ms, put_time: 0.0055 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34290 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,094] LMCache INFO:[0m Reqid: cmpl-benchmark-serving69-0, Total tokens 27, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,199] LMCache INFO:[0m Storing KV cache for 27 out of 27 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving69-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,199] LMCache INFO:[0m Stored 27 out of total 27 tokens. size: 0.0025 gb, cost 0.2158 ms, throughput: 11.4532 GB/s; offload_time: 0.2066 ms, put_time: 0.0092 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34414 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,392] LMCache INFO:[0m Reqid: cmpl-benchmark-serving70-0, Total tokens 268, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34418 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34428 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,542] LMCache INFO:[0m Storing KV cache for 268 out of 268 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving70-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,543] LMCache INFO:[0m Stored 268 out of total 268 tokens. size: 0.0245 gb, cost 0.8619 ms, throughput: 28.4682 GB/s; offload_time: 0.8511 ms, put_time: 0.0108 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,545] LMCache INFO:[0m Reqid: cmpl-benchmark-serving71-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,546] LMCache INFO:[0m Reqid: cmpl-benchmark-serving72-0, Total tokens 4, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,654] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving71-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,655] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.1338 ms, throughput: 7.5271 GB/s; offload_time: 0.1264 ms, put_time: 0.0074 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,655] LMCache INFO:[0m Storing KV cache for 4 out of 4 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving72-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,655] LMCache INFO:[0m Stored 4 out of total 4 tokens. size: 0.0004 gb, cost 0.0734 ms, throughput: 4.9897 GB/s; offload_time: 0.0683 ms, put_time: 0.0051 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34432 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,846] LMCache INFO:[0m Reqid: cmpl-benchmark-serving73-0, Total tokens 20, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,949] LMCache INFO:[0m Storing KV cache for 20 out of 20 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving73-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:25,950] LMCache INFO:[0m Stored 20 out of total 20 tokens. size: 0.0018 gb, cost 0.2064 ms, throughput: 8.8717 GB/s; offload_time: 0.1951 ms, put_time: 0.0113 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34438 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,144] LMCache INFO:[0m Reqid: cmpl-benchmark-serving74-0, Total tokens 343, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,331] LMCache INFO:[0m Storing KV cache for 343 out of 343 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving74-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,332] LMCache INFO:[0m Stored 343 out of total 343 tokens. size: 0.0314 gb, cost 1.0781 ms, throughput: 29.1274 GB/s; offload_time: 1.0665 ms, put_time: 0.0117 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34446 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,433] LMCache INFO:[0m Reqid: cmpl-benchmark-serving75-0, Total tokens 21, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,539] LMCache INFO:[0m Storing KV cache for 21 out of 21 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving75-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,539] LMCache INFO:[0m Stored 21 out of total 21 tokens. size: 0.0019 gb, cost 0.1819 ms, throughput: 10.5685 GB/s; offload_time: 0.1736 ms, put_time: 0.0084 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34452 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34456 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,736] LMCache INFO:[0m Reqid: cmpl-benchmark-serving76-0, Total tokens 38, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,737] LMCache INFO:[0m Reqid: cmpl-benchmark-serving77-0, Total tokens 289, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34472 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34474 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,898] LMCache INFO:[0m Storing KV cache for 38 out of 38 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving76-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,898] LMCache INFO:[0m Stored 38 out of total 38 tokens. size: 0.0035 gb, cost 0.2623 ms, throughput: 13.2654 GB/s; offload_time: 0.2517 ms, put_time: 0.0106 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,898] LMCache INFO:[0m Storing KV cache for 289 out of 289 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving77-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,899] LMCache INFO:[0m Stored 289 out of total 289 tokens. size: 0.0265 gb, cost 0.8712 ms, throughput: 30.3696 GB/s; offload_time: 0.8643 ms, put_time: 0.0069 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,901] LMCache INFO:[0m Reqid: cmpl-benchmark-serving78-0, Total tokens 289, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:26,902] LMCache INFO:[0m Reqid: cmpl-benchmark-serving79-0, Total tokens 63, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,095] LMCache INFO:[0m Storing KV cache for 289 out of 289 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving78-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,096] LMCache INFO:[0m Stored 289 out of total 289 tokens. size: 0.0265 gb, cost 0.9401 ms, throughput: 28.1451 GB/s; offload_time: 0.9284 ms, put_time: 0.0117 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,096] LMCache INFO:[0m Storing KV cache for 63 out of 63 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving79-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,097] LMCache INFO:[0m Stored 63 out of total 63 tokens. size: 0.0058 gb, cost 0.2372 ms, throughput: 24.3128 GB/s; offload_time: 0.2320 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34478 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,303] LMCache INFO:[0m Reqid: cmpl-benchmark-serving80-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,409] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving80-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,409] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1787 ms, throughput: 8.7076 GB/s; offload_time: 0.1694 ms, put_time: 0.0094 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43246 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,509] LMCache INFO:[0m Reqid: cmpl-benchmark-serving81-0, Total tokens 14, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43192 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34386 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,615] LMCache INFO:[0m Storing KV cache for 14 out of 14 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving81-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,615] LMCache INFO:[0m Stored 14 out of total 14 tokens. size: 0.0013 gb, cost 0.1839 ms, throughput: 6.9681 GB/s; offload_time: 0.1739 ms, put_time: 0.0100 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,617] LMCache INFO:[0m Reqid: cmpl-benchmark-serving82-0, Total tokens 24, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,618] LMCache INFO:[0m Reqid: cmpl-benchmark-serving83-0, Total tokens 13, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43220 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,731] LMCache INFO:[0m Storing KV cache for 24 out of 24 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving82-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,732] LMCache INFO:[0m Stored 24 out of total 24 tokens. size: 0.0022 gb, cost 0.1951 ms, throughput: 11.2612 GB/s; offload_time: 0.1854 ms, put_time: 0.0097 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,732] LMCache INFO:[0m Storing KV cache for 13 out of 13 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving83-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,732] LMCache INFO:[0m Stored 13 out of total 13 tokens. size: 0.0012 gb, cost 0.0997 ms, throughput: 11.9426 GB/s; offload_time: 0.0944 ms, put_time: 0.0053 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34490 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:27,734] LMCache INFO:[0m Reqid: cmpl-benchmark-serving84-0, Total tokens 666, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,007] LMCache INFO:[0m Storing KV cache for 666 out of 666 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving84-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,009] LMCache INFO:[0m Stored 666 out of total 666 tokens. size: 0.0610 gb, cost 1.9742 ms, throughput: 30.8850 GB/s; offload_time: 1.9607 ms, put_time: 0.0135 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,011] LMCache INFO:[0m Reqid: cmpl-benchmark-serving85-0, Total tokens 501, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:38476 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,236] LMCache INFO:[0m Storing KV cache for 501 out of 501 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving85-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,238] LMCache INFO:[0m Stored 501 out of total 501 tokens. size: 0.0459 gb, cost 1.5026 ms, throughput: 30.5265 GB/s; offload_time: 1.4902 ms, put_time: 0.0123 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,240] LMCache INFO:[0m Reqid: cmpl-benchmark-serving86-0, Total tokens 70, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:38490 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,368] LMCache INFO:[0m Storing KV cache for 70 out of 70 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving86-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,368] LMCache INFO:[0m Stored 70 out of total 70 tokens. size: 0.0064 gb, cost 0.3177 ms, throughput: 20.1693 GB/s; offload_time: 0.3084 ms, put_time: 0.0093 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,370] LMCache INFO:[0m Reqid: cmpl-benchmark-serving87-0, Total tokens 262, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:08:28 [loggers.py:236] Engine 000: Avg prompt throughput: 894.8 tokens/s, Avg generation throughput: 372.4 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 69.3%, Prefix cache hit rate: 3.6%, External prefix cache hit rate: 0.0%
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,526] LMCache INFO:[0m Storing KV cache for 262 out of 262 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving87-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,527] LMCache INFO:[0m Stored 262 out of total 262 tokens. size: 0.0240 gb, cost 0.8344 ms, throughput: 28.7482 GB/s; offload_time: 0.8244 ms, put_time: 0.0100 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:38502 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,736] LMCache INFO:[0m Reqid: cmpl-benchmark-serving88-0, Total tokens 38, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,846] LMCache INFO:[0m Storing KV cache for 38 out of 38 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving88-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,846] LMCache INFO:[0m Stored 38 out of total 38 tokens. size: 0.0035 gb, cost 0.2290 ms, throughput: 15.1910 GB/s; offload_time: 0.2207 ms, put_time: 0.0083 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34414 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:38518 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,952] LMCache INFO:[0m Reqid: cmpl-benchmark-serving89-0, Total tokens 233, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:28,954] LMCache INFO:[0m Reqid: cmpl-benchmark-serving90-0, Total tokens 197, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,156] LMCache INFO:[0m Storing KV cache for 233 out of 233 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving89-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,157] LMCache INFO:[0m Stored 233 out of total 233 tokens. size: 0.0213 gb, cost 0.7604 ms, throughput: 28.0519 GB/s; offload_time: 0.7498 ms, put_time: 0.0106 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,157] LMCache INFO:[0m Storing KV cache for 197 out of 197 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving90-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:38530 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,158] LMCache INFO:[0m Stored 197 out of total 197 tokens. size: 0.0180 gb, cost 0.6016 ms, throughput: 29.9818 GB/s; offload_time: 0.5961 ms, put_time: 0.0055 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,160] LMCache INFO:[0m Reqid: cmpl-benchmark-serving91-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,269] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving91-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,269] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.1505 ms, throughput: 6.6905 GB/s; offload_time: 0.1423 ms, put_time: 0.0082 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34348 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,480] LMCache INFO:[0m Reqid: cmpl-benchmark-serving92-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,589] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving92-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,589] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.1577 ms, throughput: 6.3879 GB/s; offload_time: 0.1486 ms, put_time: 0.0090 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:34490 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,693] LMCache INFO:[0m Reqid: cmpl-benchmark-serving93-0, Total tokens 9, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,800] LMCache INFO:[0m Storing KV cache for 9 out of 9 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving93-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:29,817] LMCache INFO:[0m Stored 9 out of total 9 tokens. size: 0.0008 gb, cost 0.1782 ms, throughput: 4.6240 GB/s; offload_time: 0.1694 ms, put_time: 0.0088 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:44792 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,020] LMCache INFO:[0m Reqid: cmpl-benchmark-serving94-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:38536 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,130] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving94-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,130] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1686 ms, throughput: 9.2339 GB/s; offload_time: 0.1595 ms, put_time: 0.0091 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,132] LMCache INFO:[0m Reqid: cmpl-benchmark-serving95-0, Total tokens 299, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:38490 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,292] LMCache INFO:[0m Storing KV cache for 299 out of 299 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving95-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,294] LMCache INFO:[0m Stored 299 out of total 299 tokens. size: 0.0274 gb, cost 0.9674 ms, throughput: 28.2956 GB/s; offload_time: 0.9555 ms, put_time: 0.0120 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,296] LMCache INFO:[0m Reqid: cmpl-benchmark-serving96-0, Total tokens 78, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:38552 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:38562 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,415] LMCache INFO:[0m Storing KV cache for 78 out of 78 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving96-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,416] LMCache INFO:[0m Stored 78 out of total 78 tokens. size: 0.0071 gb, cost 0.3402 ms, throughput: 20.9917 GB/s; offload_time: 0.3314 ms, put_time: 0.0088 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,418] LMCache INFO:[0m Reqid: cmpl-benchmark-serving97-0, Total tokens 583, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,419] LMCache INFO:[0m Reqid: cmpl-benchmark-serving98-0, Total tokens 454, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=476072)[0;0m INFO:     127.0.0.1:43160 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,763] LMCache INFO:[0m Storing KV cache for 583 out of 583 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving97-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,765] LMCache INFO:[0m Stored 583 out of total 583 tokens. size: 0.0534 gb, cost 1.7245 ms, throughput: 30.9518 GB/s; offload_time: 1.7128 ms, put_time: 0.0117 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,765] LMCache INFO:[0m Storing KV cache for 454 out of 454 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving98-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,766] LMCache INFO:[0m Stored 454 out of total 454 tokens. size: 0.0416 gb, cost 1.2879 ms, throughput: 32.2743 GB/s; offload_time: 1.2814 ms, put_time: 0.0064 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,768] LMCache INFO:[0m Reqid: cmpl-benchmark-serving99-0, Total tokens 239, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,923] LMCache INFO:[0m Storing KV cache for 239 out of 239 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving99-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=476226)[0;0m [32;20m[2025-12-08 02:08:30,924] LMCache INFO:[0m Stored 239 out of total 239 tokens. size: 0.0219 gb, cost 0.7451 ms, throughput: 29.3661 GB/s; offload_time: 0.7374 ms, put_time: 0.0078 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:08:38 [loggers.py:236] Engine 000: Avg prompt throughput: 243.1 tokens/s, Avg generation throughput: 549.3 tokens/s, Running: 52 reqs, Waiting: 0 reqs, GPU KV cache usage: 74.3%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:08:48 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 468.0 tokens/s, Running: 43 reqs, Waiting: 0 reqs, GPU KV cache usage: 85.8%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:08:58 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 372.5 tokens/s, Running: 26 reqs, Waiting: 0 reqs, GPU KV cache usage: 67.8%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:09:08 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 279.1 tokens/s, Running: 19 reqs, Waiting: 0 reqs, GPU KV cache usage: 62.9%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:09:18 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 209.5 tokens/s, Running: 10 reqs, Waiting: 0 reqs, GPU KV cache usage: 43.1%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:09:28 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 120.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 6.9%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=476072)[0;0m INFO 12-08 02:09:32 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=476072)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=476072)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=476072)[0;0m INFO:     Application shutdown complete.
