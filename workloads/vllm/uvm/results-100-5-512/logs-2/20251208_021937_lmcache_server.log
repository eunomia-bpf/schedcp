[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:19:40 [api_server.py:1772] vLLM API server version 0.12.0
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:19:40 [utils.py:253] non-default args: {'model_tag': 'Qwen/Qwen3-30B-A3B-FP8', 'model': 'Qwen/Qwen3-30B-A3B-FP8', 'max_model_len': 2048, 'enforce_eager': True, 'cpu_offload_gb': 8.0, 'kv_transfer_config': KVTransferConfig(kv_connector='LMCacheConnectorV1', engine_id='8d25308f-1e5a-49ee-8115-d3a7d066d82d', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None, enable_permute_local_kv=False)}
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:19:41 [model.py:637] Resolved architecture: Qwen3MoeForCausalLM
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:19:41 [model.py:1750] Using max model len 2048
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:19:41 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=482896)[0;0m WARNING 12-08 02:19:41 [vllm.py:601] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:19:41 [vllm.py:707] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=482896)[0;0m WARNING 12-08 02:19:41 [vllm.py:896] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:44 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='Qwen/Qwen3-30B-A3B-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'all', '+quant_fp8'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:45 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://128.114.59.195:41033 backend=nccl
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:45 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:45 [gpu_model_runner.py:3467] Starting to load model Qwen/Qwen3-30B-A3B-FP8...
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:45 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:45 [layer.py:379] Enabled separate cuda stream for MoE shared_experts
[0;36m(EngineCore_DP0 pid=482991)[0;0m WARNING 12-08 02:19:45 [fp8.py:183] DeepGEMM backend requested but not available.
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:45 [fp8.py:202] Using Triton backend for FP8 MoE
[0;36m(EngineCore_DP0 pid=482991)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=482991)[0;0m Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:02,  2.03it/s]
[0;36m(EngineCore_DP0 pid=482991)[0;0m Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:01<00:02,  1.71it/s]
[0;36m(EngineCore_DP0 pid=482991)[0;0m Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:01<00:02,  1.75it/s]
[0;36m(EngineCore_DP0 pid=482991)[0;0m Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:01<00:01,  2.16it/s]
[0;36m(EngineCore_DP0 pid=482991)[0;0m Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:02<00:01,  1.91it/s]
[0;36m(EngineCore_DP0 pid=482991)[0;0m Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:03<00:00,  1.76it/s]
[0;36m(EngineCore_DP0 pid=482991)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:03<00:00,  1.68it/s]
[0;36m(EngineCore_DP0 pid=482991)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:03<00:00,  1.78it/s]
[0;36m(EngineCore_DP0 pid=482991)[0;0m 
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:52 [default_loader.py:308] Loading weights took 3.98 seconds
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:52 [gpu_model_runner.py:3549] Model loading took 20.9151 GiB memory and 6.831877 seconds
[0;36m(EngineCore_DP0 pid=482991)[0;0m WARNING 12-08 02:19:52 [fused_moe.py:888] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_GeForce_RTX_5090,dtype=fp8_w8a8,block_shape=[128,128].json']
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:54 [gpu_worker.py:359] Available KV cache memory: 5.81 GiB
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:54 [kv_cache_utils.py:1286] GPU KV cache size: 63,440 tokens
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:54 [kv_cache_utils.py:1291] Maximum concurrency for 2,048 tokens per request: 30.98x
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:54 [factory.py:64] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 8d25308f-1e5a-49ee-8115-d3a7d066d82d
[0;36m(EngineCore_DP0 pid=482991)[0;0m WARNING 12-08 02:19:54 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:54 [lmcache_connector.py:52] Initializing latest dev LMCache connector
[0;36m(EngineCore_DP0 pid=482991)[0;0m [33;20m[2025-12-08 02:19:54,974] LMCache WARNING:[0m No LMCache configuration file is set. Trying to read configurations from the environment variables. [3m(utils.py:61:lmcache.integration.vllm.utils)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [33;20m[2025-12-08 02:19:54,974] LMCache WARNING:[0m You can set the configuration file through the environment variable: LMCACHE_CONFIG_FILE [3m(utils.py:65:lmcache.integration.vllm.utils)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,974] LMCache INFO:[0m use mla: False, kv shape: (48, 2, 256, 4, 128), num_draft_layers:0 [3m(vllm_v1_adapter.py:503:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,974] LMCache INFO:[0m CUDA device is available. Using CUDA for LMCache engine. [3m(vllm_v1_adapter.py:509:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,974] LMCache INFO:[0m Creating LMCacheEngine instance vllm-instance [3m(cache_engine.py:1531:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,974] LMCache INFO:[0m NUMA mapping for instance vllm-instance: None [3m(cache_engine.py:1534:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [33;20m[2025-12-08 02:19:54,974] LMCache WARNING:[0m Could not load 'builtin' from vLLM. Using builtin hash. This may cause inconsistencies in distributed caching. [3m(token_database.py:136:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [33;20m[2025-12-08 02:19:54,974] LMCache WARNING:[0m Using builtin hash without PYTHONHASHSEED set. For production environments (non-testing scenarios), you MUST set PYTHONHASHSEED to ensure consistent hashing across processes. Example: export PYTHONHASHSEED=0 [3m(token_database.py:143:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,974] LMCache INFO:[0m Initialized NONE_HASH=b'\xc6p\x12TH\xcaP\xb6E\x89\x9e\x1cFY\x92\xcd/#\x04\xf1\xedmF\xb0$\xe3i\xacMI\xff\xed' from vLLM (>= PR#20511) [3m(token_database.py:74:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,974] LMCache INFO:[0m Using hash algorithm: builtin [3m(token_database.py:84:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,975] LMCache INFO:[0m sending cache usage stats to http://stats.lmcache.ai:8080/cache-usage [3m(usage_context.py:290:lmcache.usage_context)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,978] LMCache INFO:[0m Creating LMCacheEngine with config: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': 8.0, 'reserve_local_cpu_size': 0.0, 'local_disk': None, 'max_local_disk_size': 0.0, 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'p2p_host': None, 'p2p_init_ports': None, 'p2p_lookup_ports': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_instance_f3fb2c34064340e8b0258ced021d9476', 'controller_pull_url': None, 'controller_reply_url': None, 'lmcache_worker_ports': None, 'lmcache_worker_ids': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_pd': False, 'pd_role': None, 'pd_buffer_size': None, 'pd_buffer_device': None, 'pd_peer_host': None, 'pd_peer_init_port': None, 'pd_peer_alloc_port': None, 'pd_proxy_host': None, 'pd_proxy_port': None, 'transfer_channel': None, 'nixl_backends': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None, 'lookup_timeout_ms': 3000, 'hit_miss_ratio': None, 'lookup_server_worker_ids': None, 'enable_scheduler_bypass_lookup': False, 'script_allowed_imports': None, 'enable_lazy_memory_allocator': False, 'lazy_memory_initial_ratio': 0.2, 'lazy_memory_expand_trigger_ratio': 0.5, 'lazy_memory_step_ratio': 0.1, 'lazy_memory_safe_size': 0.0, 'enable_chunk_statistics': False, 'chunk_statistics_auto_start_statistics': False, 'chunk_statistics_auto_exit_timeout_hours': 0.0, 'chunk_statistics_auto_exit_target_unique_chunks': 0, 'chunk_statistics_strategy': 'memory_bloom_filter'} [3m(cache_engine.py:93:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,978] LMCache INFO:[0m LMCacheWorker is not initialized (related configs: enable_controller: False, role: worker, worker_id: 0, worker_ids: [0]). [3m(cache_engine.py:135:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,978] LMCache INFO:[0m Initialize storage manager on rank 0, use layerwise: False,save only first rank: False [3m(cache_engine.py:167:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,978] LMCache INFO:[0m Initializing LRUCachePolicy [3m(lru.py:22:lmcache.v1.storage_backend.cache_policy.lru)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:54,978] LMCache INFO:[0m NUMA mapping None [3m(local_cpu_backend.py:351:lmcache.v1.storage_backend.local_cpu_backend)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [33;20m[2025-12-08 02:19:56,412] LMCache WARNING:[0m Controller message sender is not initialized [3m(local_cpu_backend.py:103:lmcache.v1.storage_backend.local_cpu_backend)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:56,413] LMCache INFO:[0m Initializing usage context. [3m(usage_context.py:362:lmcache.usage_context)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:57,901] LMCache INFO:[0m lmcache lookup server start on /tmp/engine_8d25308f-1e5a-49ee-8115-d3a7d066d82d_service_lookup_lmcache_rpc_port_0 [3m(lmcache_lookup_client.py:329:lmcache.v1.lookup_client.lmcache_lookup_client)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:57,902] LMCache INFO:[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=1, port=7000, socket_path=None, include_index_list=None [3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:19:57,902] LMCache INFO:[0m LMCache initialized for role KVConnectorRole.WORKER with version 0.3.11.dev18-g58cae13ba, vllm version 0.12.0, lmcache cache_engine metadata: LMCacheEngineMetadata(model_name='Qwen/Qwen3-30B-A3B-FP8', world_size=1, worker_id=0, fmt='vllm', kv_dtype=torch.bfloat16, kv_shape=(48, 2, 256, 4, 128), use_mla=False, role='worker') [3m(vllm_v1_adapter.py:793:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:57 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=482991)[0;0m 2025-12-08 02:19:57,905 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=482991)[0;0m 2025-12-08 02:19:57,923 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:19:59 [core.py:254] init engine (profile, create kv cache, warmup model) took 7.08 seconds
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:20:00 [factory.py:64] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 8d25308f-1e5a-49ee-8115-d3a7d066d82d
[0;36m(EngineCore_DP0 pid=482991)[0;0m WARNING 12-08 02:20:00 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:20:00 [lmcache_connector.py:52] Initializing latest dev LMCache connector
[0;36m(EngineCore_DP0 pid=482991)[0;0m [31;20m[2025-12-08 02:20:00,318] LMCache ERROR:[0m PrometheusLogger instance already created withdifferent metadata. This should not happen except in test [3m(observability.py:1325:lmcache.observability)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:00,318] LMCache INFO:[0m lmcache lookup client connect to rank 0 with socket path /tmp/engine_8d25308f-1e5a-49ee-8115-d3a7d066d82d_service_lookup_lmcache_rpc_port_0 [3m(lmcache_lookup_client.py:93:lmcache.v1.lookup_client.lmcache_lookup_client)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [33;20m[2025-12-08 02:20:00,318] LMCache WARNING:[0m Could not load 'builtin' from vLLM. Using builtin hash. This may cause inconsistencies in distributed caching. [3m(token_database.py:136:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [33;20m[2025-12-08 02:20:00,318] LMCache WARNING:[0m Using builtin hash without PYTHONHASHSEED set. For production environments (non-testing scenarios), you MUST set PYTHONHASHSEED to ensure consistent hashing across processes. Example: export PYTHONHASHSEED=0 [3m(token_database.py:143:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:00,318] LMCache INFO:[0m Initialized NONE_HASH=b'\xfa]\xf7\xd0HQ\xa7\xe8\xa6\x8b\xa1\x06\x8e\xffq\xc3Z/\xdas\x9a\xe2\x02\xe1\x16$\x1dG\xf4*~\x15' from vLLM (>= PR#20511) [3m(token_database.py:74:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:00,318] LMCache INFO:[0m Using hash algorithm: builtin [3m(token_database.py:84:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:00,318] LMCache INFO:[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=0, port=6999, socket_path=None, include_index_list=None [3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:00,318] LMCache INFO:[0m LMCache initialized for role KVConnectorRole.SCHEDULER with version 0.3.11.dev18-g58cae13ba, vllm version 0.12.0, lmcache cache_engine metadata: None [3m(vllm_v1_adapter.py:793:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m WARNING 12-08 02:20:00 [vllm.py:608] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=482991)[0;0m INFO 12-08 02:20:00 [vllm.py:707] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:00 [api_server.py:1520] Supported tasks: ['generate']
[0;36m(APIServer pid=482896)[0;0m WARNING 12-08 02:20:00 [model.py:1576] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:00 [serving_responses.py:194] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:00 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [serving_completion.py:73] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [api_server.py:1847] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:01 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=482896)[0;0m INFO:     Started server process [482896]
[0;36m(APIServer pid=482896)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=482896)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:45300 - "GET /health HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:45312 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:07,438] LMCache INFO:[0m Reqid: cmpl-b20d8ebf127b3083-0, Total tokens 729, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:07,440] LMCache INFO:[0m Post-initializing LMCacheEngine [3m(cache_engine.py:226:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:07,971] LMCache INFO:[0m Storing KV cache for 729 out of 729 tokens (skip_leading_tokens=0) for request cmpl-b20d8ebf127b3083-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:07,974] LMCache INFO:[0m Stored 729 out of total 729 tokens. size: 0.0667 gb, cost 2.3986 ms, throughput: 27.8250 GB/s; offload_time: 2.3758 ms, put_time: 0.0228 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:11 [loggers.py:236] Engine 000: Avg prompt throughput: 68.8 tokens/s, Avg generation throughput: 11.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:45312 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:11,316] LMCache INFO:[0m Reqid: cmpl-benchmark-serving0-0, Total tokens 729, LMCache hit tokens: 729, need to load: 8 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:11,320] LMCache INFO:[0m Retrieved 217 out of 217 required tokens (from 729 total tokens). size: 0.0199 gb, cost 0.8869 ms, throughput: 22.4015 GB/s; [3m(cache_engine.py:582:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42460 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:11,970] LMCache INFO:[0m Reqid: cmpl-benchmark-serving1-0, Total tokens 9, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,042] LMCache INFO:[0m Storing KV cache for 9 out of 9 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving1-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,042] LMCache INFO:[0m Stored 9 out of total 9 tokens. size: 0.0008 gb, cost 0.1716 ms, throughput: 4.8027 GB/s; offload_time: 0.1590 ms, put_time: 0.0126 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42472 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,275] LMCache INFO:[0m Reqid: cmpl-benchmark-serving2-0, Total tokens 110, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,424] LMCache INFO:[0m Storing KV cache for 110 out of 110 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving2-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,424] LMCache INFO:[0m Stored 110 out of total 110 tokens. size: 0.0101 gb, cost 0.4407 ms, throughput: 22.8515 GB/s; offload_time: 0.4308 ms, put_time: 0.0099 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42482 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,474] LMCache INFO:[0m Reqid: cmpl-benchmark-serving3-0, Total tokens 59, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42492 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42494 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42502 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,611] LMCache INFO:[0m Storing KV cache for 59 out of 59 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving3-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,611] LMCache INFO:[0m Stored 59 out of total 59 tokens. size: 0.0054 gb, cost 0.2916 ms, throughput: 18.5232 GB/s; offload_time: 0.2820 ms, put_time: 0.0096 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,613] LMCache INFO:[0m Reqid: cmpl-benchmark-serving4-0, Total tokens 21, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,614] LMCache INFO:[0m Reqid: cmpl-benchmark-serving5-0, Total tokens 57, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,616] LMCache INFO:[0m Reqid: cmpl-benchmark-serving6-0, Total tokens 36, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,783] LMCache INFO:[0m Storing KV cache for 21 out of 21 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving4-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,783] LMCache INFO:[0m Stored 21 out of total 21 tokens. size: 0.0019 gb, cost 0.1697 ms, throughput: 11.3288 GB/s; offload_time: 0.1622 ms, put_time: 0.0075 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,783] LMCache INFO:[0m Storing KV cache for 57 out of 57 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving5-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,783] LMCache INFO:[0m Stored 57 out of total 57 tokens. size: 0.0052 gb, cost 0.2169 ms, throughput: 24.0634 GB/s; offload_time: 0.2117 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,783] LMCache INFO:[0m Storing KV cache for 36 out of 36 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving6-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:12,783] LMCache INFO:[0m Stored 36 out of total 36 tokens. size: 0.0033 gb, cost 0.1557 ms, throughput: 21.1725 GB/s; offload_time: 0.1509 ms, put_time: 0.0048 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42508 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:13,001] LMCache INFO:[0m Reqid: cmpl-benchmark-serving7-0, Total tokens 78, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:13,156] LMCache INFO:[0m Storing KV cache for 78 out of 78 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving7-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:13,156] LMCache INFO:[0m Stored 78 out of total 78 tokens. size: 0.0071 gb, cost 0.3279 ms, throughput: 21.7780 GB/s; offload_time: 0.3202 ms, put_time: 0.0077 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42516 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:13,245] LMCache INFO:[0m Reqid: cmpl-benchmark-serving8-0, Total tokens 667, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42524 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42534 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:13,703] LMCache INFO:[0m Storing KV cache for 667 out of 667 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving8-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:13,705] LMCache INFO:[0m Stored 667 out of total 667 tokens. size: 0.0611 gb, cost 1.9901 ms, throughput: 30.6850 GB/s; offload_time: 1.9753 ms, put_time: 0.0148 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:13,707] LMCache INFO:[0m Reqid: cmpl-benchmark-serving9-0, Total tokens 414, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:13,709] LMCache INFO:[0m Reqid: cmpl-benchmark-serving10-0, Total tokens 227, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:14,146] LMCache INFO:[0m Storing KV cache for 414 out of 414 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving9-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:14,148] LMCache INFO:[0m Stored 414 out of total 414 tokens. size: 0.0379 gb, cost 1.2361 ms, throughput: 30.6626 GB/s; offload_time: 1.2283 ms, put_time: 0.0078 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:14,148] LMCache INFO:[0m Storing KV cache for 227 out of 227 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving10-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:14,148] LMCache INFO:[0m Stored 227 out of total 227 tokens. size: 0.0208 gb, cost 0.6691 ms, throughput: 31.0617 GB/s; offload_time: 0.6639 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42536 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:14,245] LMCache INFO:[0m Reqid: cmpl-benchmark-serving11-0, Total tokens 7, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:14,373] LMCache INFO:[0m Storing KV cache for 7 out of 7 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving11-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:14,373] LMCache INFO:[0m Stored 7 out of total 7 tokens. size: 0.0006 gb, cost 0.1332 ms, throughput: 4.8096 GB/s; offload_time: 0.1228 ms, put_time: 0.0105 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42550 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42552 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:14,674] LMCache INFO:[0m Reqid: cmpl-benchmark-serving12-0, Total tokens 379, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:14,675] LMCache INFO:[0m Reqid: cmpl-benchmark-serving13-0, Total tokens 401, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42524 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42566 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42568 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42570 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42572 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:15,179] LMCache INFO:[0m Storing KV cache for 379 out of 379 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving12-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:15,181] LMCache INFO:[0m Stored 379 out of total 379 tokens. size: 0.0347 gb, cost 1.1722 ms, throughput: 29.6020 GB/s; offload_time: 1.1608 ms, put_time: 0.0113 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:15,181] LMCache INFO:[0m Storing KV cache for 401 out of 401 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving13-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:15,182] LMCache INFO:[0m Stored 401 out of total 401 tokens. size: 0.0367 gb, cost 1.1548 ms, throughput: 31.7915 GB/s; offload_time: 1.1485 ms, put_time: 0.0063 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:15,184] LMCache INFO:[0m Reqid: cmpl-benchmark-serving14-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:15,185] LMCache INFO:[0m Reqid: cmpl-benchmark-serving15-0, Total tokens 743, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:15,186] LMCache INFO:[0m Reqid: cmpl-benchmark-serving16-0, Total tokens 245, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:15,187] LMCache INFO:[0m Reqid: cmpl-benchmark-serving17-0, Total tokens 20, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:15,188] LMCache INFO:[0m Reqid: cmpl-benchmark-serving18-0, Total tokens 403, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42588 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42600 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42616 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42626 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42632 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42644 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,058] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving14-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,058] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.2172 ms, throughput: 7.1661 GB/s; offload_time: 0.2033 ms, put_time: 0.0138 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,058] LMCache INFO:[0m Storing KV cache for 743 out of 743 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving15-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,061] LMCache INFO:[0m Stored 743 out of total 743 tokens. size: 0.0680 gb, cost 2.0947 ms, throughput: 32.4740 GB/s; offload_time: 2.0869 ms, put_time: 0.0078 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,061] LMCache INFO:[0m Storing KV cache for 245 out of 245 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving16-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,061] LMCache INFO:[0m Stored 245 out of total 245 tokens. size: 0.0224 gb, cost 0.7159 ms, throughput: 31.3326 GB/s; offload_time: 0.7110 ms, put_time: 0.0048 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,061] LMCache INFO:[0m Storing KV cache for 20 out of 20 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving17-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,062] LMCache INFO:[0m Stored 20 out of total 20 tokens. size: 0.0018 gb, cost 0.1107 ms, throughput: 16.5376 GB/s; offload_time: 0.1064 ms, put_time: 0.0043 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,062] LMCache INFO:[0m Storing KV cache for 403 out of 403 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving18-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,063] LMCache INFO:[0m Stored 403 out of total 403 tokens. size: 0.0369 gb, cost 1.1391 ms, throughput: 32.3896 GB/s; offload_time: 1.1335 ms, put_time: 0.0056 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,066] LMCache INFO:[0m Reqid: cmpl-benchmark-serving19-0, Total tokens 325, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,066] LMCache INFO:[0m Reqid: cmpl-benchmark-serving20-0, Total tokens 61, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,067] LMCache INFO:[0m Reqid: cmpl-benchmark-serving21-0, Total tokens 412, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,069] LMCache INFO:[0m Reqid: cmpl-benchmark-serving22-0, Total tokens 88, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,070] LMCache INFO:[0m Reqid: cmpl-benchmark-serving23-0, Total tokens 181, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,071] LMCache INFO:[0m Reqid: cmpl-benchmark-serving24-0, Total tokens 26, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42658 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42664 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42668 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42684 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42686 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42702 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,736] LMCache INFO:[0m Storing KV cache for 325 out of 325 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving19-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,737] LMCache INFO:[0m Stored 325 out of total 325 tokens. size: 0.0298 gb, cost 1.0852 ms, throughput: 27.4197 GB/s; offload_time: 1.0732 ms, put_time: 0.0120 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42710 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,737] LMCache INFO:[0m Storing KV cache for 61 out of 61 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving20-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,737] LMCache INFO:[0m Stored 61 out of total 61 tokens. size: 0.0056 gb, cost 0.2423 ms, throughput: 23.0498 GB/s; offload_time: 0.2368 ms, put_time: 0.0055 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,737] LMCache INFO:[0m Storing KV cache for 412 out of 412 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving21-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,739] LMCache INFO:[0m Stored 412 out of total 412 tokens. size: 0.0377 gb, cost 1.1898 ms, throughput: 31.7037 GB/s; offload_time: 1.1831 ms, put_time: 0.0066 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,739] LMCache INFO:[0m Storing KV cache for 88 out of 88 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving22-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,739] LMCache INFO:[0m Stored 88 out of total 88 tokens. size: 0.0081 gb, cost 0.2934 ms, throughput: 27.4587 GB/s; offload_time: 0.2887 ms, put_time: 0.0047 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,739] LMCache INFO:[0m Storing KV cache for 181 out of 181 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving23-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,740] LMCache INFO:[0m Stored 181 out of total 181 tokens. size: 0.0166 gb, cost 0.5455 ms, throughput: 30.3795 GB/s; offload_time: 0.5406 ms, put_time: 0.0049 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,740] LMCache INFO:[0m Storing KV cache for 26 out of 26 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving24-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,740] LMCache INFO:[0m Stored 26 out of total 26 tokens. size: 0.0024 gb, cost 0.1248 ms, throughput: 19.0686 GB/s; offload_time: 0.1202 ms, put_time: 0.0046 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,742] LMCache INFO:[0m Reqid: cmpl-benchmark-serving25-0, Total tokens 28, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,743] LMCache INFO:[0m Reqid: cmpl-benchmark-serving26-0, Total tokens 188, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,744] LMCache INFO:[0m Reqid: cmpl-benchmark-serving27-0, Total tokens 507, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,745] LMCache INFO:[0m Reqid: cmpl-benchmark-serving28-0, Total tokens 249, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,746] LMCache INFO:[0m Reqid: cmpl-benchmark-serving29-0, Total tokens 384, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:16,747] LMCache INFO:[0m Reqid: cmpl-benchmark-serving30-0, Total tokens 835, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42716 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42724 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,008] LMCache INFO:[0m Storing KV cache for 28 out of 28 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving25-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,009] LMCache INFO:[0m Stored 28 out of total 28 tokens. size: 0.0026 gb, cost 0.2644 ms, throughput: 9.6964 GB/s; offload_time: 0.2528 ms, put_time: 0.0116 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,009] LMCache INFO:[0m Storing KV cache for 188 out of 188 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving26-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,010] LMCache INFO:[0m Stored 188 out of total 188 tokens. size: 0.0172 gb, cost 0.5751 ms, throughput: 29.9286 GB/s; offload_time: 0.5697 ms, put_time: 0.0054 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,010] LMCache INFO:[0m Storing KV cache for 507 out of 507 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving27-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,011] LMCache INFO:[0m Stored 507 out of total 507 tokens. size: 0.0464 gb, cost 1.4402 ms, throughput: 32.2291 GB/s; offload_time: 1.4337 ms, put_time: 0.0066 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,011] LMCache INFO:[0m Storing KV cache for 249 out of 249 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving28-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,012] LMCache INFO:[0m Stored 249 out of total 249 tokens. size: 0.0228 gb, cost 0.7243 ms, throughput: 31.4720 GB/s; offload_time: 0.7195 ms, put_time: 0.0049 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,012] LMCache INFO:[0m Storing KV cache for 384 out of 384 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving29-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,013] LMCache INFO:[0m Stored 384 out of total 384 tokens. size: 0.0352 gb, cost 1.1100 ms, throughput: 31.6711 GB/s; offload_time: 1.0997 ms, put_time: 0.0103 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,013] LMCache INFO:[0m Storing KV cache for 512 out of 512 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving30-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,015] LMCache INFO:[0m Stored 512 out of total 512 tokens. size: 0.0469 gb, cost 1.4089 ms, throughput: 33.2712 GB/s; offload_time: 1.4031 ms, put_time: 0.0058 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,017] LMCache INFO:[0m Reqid: cmpl-benchmark-serving31-0, Total tokens 386, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,018] LMCache INFO:[0m Reqid: cmpl-benchmark-serving32-0, Total tokens 5, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,020] LMCache INFO:[0m Reqid: cmpl-benchmark-serving33-0, Total tokens 679, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52912 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52924 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52940 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52954 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,797] LMCache INFO:[0m Storing KV cache for 386 out of 386 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving31-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,799] LMCache INFO:[0m Stored 386 out of total 386 tokens. size: 0.0353 gb, cost 1.2645 ms, throughput: 27.9466 GB/s; offload_time: 1.2513 ms, put_time: 0.0133 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,799] LMCache INFO:[0m Storing KV cache for 5 out of 5 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving32-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,799] LMCache INFO:[0m Stored 5 out of total 5 tokens. size: 0.0005 gb, cost 0.0820 ms, throughput: 5.5843 GB/s; offload_time: 0.0766 ms, put_time: 0.0054 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,799] LMCache INFO:[0m Storing KV cache for 679 out of 679 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving33-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,801] LMCache INFO:[0m Stored 679 out of total 679 tokens. size: 0.0622 gb, cost 1.9216 ms, throughput: 32.3495 GB/s; offload_time: 1.9146 ms, put_time: 0.0070 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,801] LMCache INFO:[0m Storing KV cache for 323 out of 835 tokens (skip_leading_tokens=512) for request cmpl-benchmark-serving30-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,802] LMCache INFO:[0m Stored 323 out of total 323 tokens. size: 0.0296 gb, cost 0.9485 ms, throughput: 31.1757 GB/s; offload_time: 0.9422 ms, put_time: 0.0063 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,803] LMCache INFO:[0m Reqid: cmpl-benchmark-serving34-0, Total tokens 25, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,804] LMCache INFO:[0m Reqid: cmpl-benchmark-serving35-0, Total tokens 742, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,804] LMCache INFO:[0m Reqid: cmpl-benchmark-serving36-0, Total tokens 385, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:18,805] LMCache INFO:[0m Reqid: cmpl-benchmark-serving37-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52956 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52966 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52968 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52974 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52990 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,593] LMCache INFO:[0m Storing KV cache for 25 out of 25 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving34-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,593] LMCache INFO:[0m Stored 25 out of total 25 tokens. size: 0.0023 gb, cost 0.2772 ms, throughput: 8.2570 GB/s; offload_time: 0.2651 ms, put_time: 0.0121 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,593] LMCache INFO:[0m Storing KV cache for 742 out of 742 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving35-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,595] LMCache INFO:[0m Stored 742 out of total 742 tokens. size: 0.0679 gb, cost 2.0842 ms, throughput: 32.5931 GB/s; offload_time: 2.0759 ms, put_time: 0.0084 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,595] LMCache INFO:[0m Storing KV cache for 385 out of 385 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving36-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,597] LMCache INFO:[0m Stored 385 out of total 385 tokens. size: 0.0352 gb, cost 1.1004 ms, throughput: 32.0304 GB/s; offload_time: 1.0942 ms, put_time: 0.0063 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,597] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving37-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,597] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.2120 ms, throughput: 25.0524 GB/s; offload_time: 0.2072 ms, put_time: 0.0048 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,599] LMCache INFO:[0m Reqid: cmpl-benchmark-serving38-0, Total tokens 332, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,601] LMCache INFO:[0m Reqid: cmpl-benchmark-serving39-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,602] LMCache INFO:[0m Reqid: cmpl-benchmark-serving40-0, Total tokens 37, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,603] LMCache INFO:[0m Reqid: cmpl-benchmark-serving41-0, Total tokens 161, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:19,603] LMCache INFO:[0m Reqid: cmpl-benchmark-serving42-0, Total tokens 788, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53006 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53012 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53016 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53018 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53030 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,400] LMCache INFO:[0m Storing KV cache for 332 out of 332 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving38-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,401] LMCache INFO:[0m Stored 332 out of total 332 tokens. size: 0.0304 gb, cost 1.0905 ms, throughput: 27.8735 GB/s; offload_time: 1.0781 ms, put_time: 0.0123 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,401] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving39-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,401] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.1066 ms, throughput: 9.4429 GB/s; offload_time: 0.1010 ms, put_time: 0.0057 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,401] LMCache INFO:[0m Storing KV cache for 37 out of 37 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving40-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,402] LMCache INFO:[0m Stored 37 out of total 37 tokens. size: 0.0034 gb, cost 0.1602 ms, throughput: 21.1421 GB/s; offload_time: 0.1552 ms, put_time: 0.0050 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,402] LMCache INFO:[0m Storing KV cache for 161 out of 161 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving41-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,402] LMCache INFO:[0m Stored 161 out of total 161 tokens. size: 0.0147 gb, cost 0.4839 ms, throughput: 30.4582 GB/s; offload_time: 0.4795 ms, put_time: 0.0045 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,402] LMCache INFO:[0m Storing KV cache for 788 out of 788 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving42-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,404] LMCache INFO:[0m Stored 788 out of total 788 tokens. size: 0.0721 gb, cost 2.2024 ms, throughput: 32.7566 GB/s; offload_time: 2.1936 ms, put_time: 0.0088 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,407] LMCache INFO:[0m Reqid: cmpl-benchmark-serving43-0, Total tokens 30, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,408] LMCache INFO:[0m Reqid: cmpl-benchmark-serving44-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,409] LMCache INFO:[0m Reqid: cmpl-benchmark-serving45-0, Total tokens 24, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,410] LMCache INFO:[0m Reqid: cmpl-benchmark-serving46-0, Total tokens 335, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,411] LMCache INFO:[0m Reqid: cmpl-benchmark-serving47-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53032 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53044 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,821] LMCache INFO:[0m Storing KV cache for 30 out of 30 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving43-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,822] LMCache INFO:[0m Stored 30 out of total 30 tokens. size: 0.0027 gb, cost 0.2440 ms, throughput: 11.2570 GB/s; offload_time: 0.2335 ms, put_time: 0.0105 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,822] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving44-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,822] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.2252 ms, throughput: 23.5805 GB/s; offload_time: 0.2201 ms, put_time: 0.0051 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,822] LMCache INFO:[0m Storing KV cache for 24 out of 24 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving45-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,822] LMCache INFO:[0m Stored 24 out of total 24 tokens. size: 0.0022 gb, cost 0.1226 ms, throughput: 17.9228 GB/s; offload_time: 0.1180 ms, put_time: 0.0046 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,822] LMCache INFO:[0m Storing KV cache for 335 out of 335 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving46-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,823] LMCache INFO:[0m Stored 335 out of total 335 tokens. size: 0.0307 gb, cost 0.9825 ms, throughput: 31.2151 GB/s; offload_time: 0.9756 ms, put_time: 0.0069 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,823] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving47-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,823] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.2125 ms, throughput: 24.9853 GB/s; offload_time: 0.2076 ms, put_time: 0.0049 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,826] LMCache INFO:[0m Reqid: cmpl-benchmark-serving48-0, Total tokens 273, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:20,827] LMCache INFO:[0m Reqid: cmpl-benchmark-serving49-0, Total tokens 5, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:21,106] LMCache INFO:[0m Storing KV cache for 273 out of 273 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving48-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:21,107] LMCache INFO:[0m Stored 273 out of total 273 tokens. size: 0.0250 gb, cost 0.8881 ms, throughput: 28.1445 GB/s; offload_time: 0.8779 ms, put_time: 0.0102 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:21,107] LMCache INFO:[0m Storing KV cache for 5 out of 5 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving49-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:21,107] LMCache INFO:[0m Stored 5 out of total 5 tokens. size: 0.0005 gb, cost 0.0773 ms, throughput: 5.9225 GB/s; offload_time: 0.0722 ms, put_time: 0.0051 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42710 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:21 [loggers.py:236] Engine 000: Avg prompt throughput: 1229.8 tokens/s, Avg generation throughput: 46.4 tokens/s, Running: 48 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.3%, Prefix cache hit rate: 5.5%, External prefix cache hit rate: 0.1%
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:21,269] LMCache INFO:[0m Reqid: cmpl-benchmark-serving50-0, Total tokens 48, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:21,449] LMCache INFO:[0m Storing KV cache for 48 out of 48 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving50-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:21,449] LMCache INFO:[0m Stored 48 out of total 48 tokens. size: 0.0044 gb, cost 0.2491 ms, throughput: 17.6381 GB/s; offload_time: 0.2417 ms, put_time: 0.0075 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53060 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:21,613] LMCache INFO:[0m Reqid: cmpl-benchmark-serving51-0, Total tokens 158, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:21,829] LMCache INFO:[0m Storing KV cache for 158 out of 158 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving51-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:21,830] LMCache INFO:[0m Stored 158 out of total 158 tokens. size: 0.0145 gb, cost 0.5868 ms, throughput: 24.6510 GB/s; offload_time: 0.5763 ms, put_time: 0.0106 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42632 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:22,319] LMCache INFO:[0m Reqid: cmpl-benchmark-serving52-0, Total tokens 306, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:22,600] LMCache INFO:[0m Storing KV cache for 306 out of 306 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving52-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:22,601] LMCache INFO:[0m Stored 306 out of total 306 tokens. size: 0.0280 gb, cost 0.9755 ms, throughput: 28.7201 GB/s; offload_time: 0.9651 ms, put_time: 0.0104 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52968 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:22,753] LMCache INFO:[0m Reqid: cmpl-benchmark-serving53-0, Total tokens 772, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52990 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,275] LMCache INFO:[0m Storing KV cache for 772 out of 772 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving53-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,277] LMCache INFO:[0m Stored 772 out of total 772 tokens. size: 0.0707 gb, cost 2.2751 ms, throughput: 31.0659 GB/s; offload_time: 2.2530 ms, put_time: 0.0221 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,279] LMCache INFO:[0m Reqid: cmpl-benchmark-serving54-0, Total tokens 315, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52924 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53068 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53078 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53090 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,557] LMCache INFO:[0m Storing KV cache for 315 out of 315 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving54-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,558] LMCache INFO:[0m Stored 315 out of total 315 tokens. size: 0.0288 gb, cost 0.9869 ms, throughput: 29.2227 GB/s; offload_time: 0.9769 ms, put_time: 0.0100 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,561] LMCache INFO:[0m Reqid: cmpl-benchmark-serving55-0, Total tokens 20, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,562] LMCache INFO:[0m Reqid: cmpl-benchmark-serving56-0, Total tokens 75, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,563] LMCache INFO:[0m Reqid: cmpl-benchmark-serving57-0, Total tokens 85, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,564] LMCache INFO:[0m Reqid: cmpl-benchmark-serving58-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53032 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53094 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53110 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,783] LMCache INFO:[0m Storing KV cache for 20 out of 20 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving55-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,783] LMCache INFO:[0m Stored 20 out of total 20 tokens. size: 0.0018 gb, cost 0.1664 ms, throughput: 11.0018 GB/s; offload_time: 0.1587 ms, put_time: 0.0078 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,783] LMCache INFO:[0m Storing KV cache for 75 out of 75 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving56-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,784] LMCache INFO:[0m Stored 75 out of total 75 tokens. size: 0.0069 gb, cost 0.2690 ms, throughput: 25.5226 GB/s; offload_time: 0.2639 ms, put_time: 0.0051 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,784] LMCache INFO:[0m Storing KV cache for 85 out of 85 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving57-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,784] LMCache INFO:[0m Stored 85 out of total 85 tokens. size: 0.0078 gb, cost 0.2861 ms, throughput: 27.2020 GB/s; offload_time: 0.2814 ms, put_time: 0.0047 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,784] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving58-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,784] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.0865 ms, throughput: 11.6479 GB/s; offload_time: 0.0817 ms, put_time: 0.0048 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,786] LMCache INFO:[0m Reqid: cmpl-benchmark-serving59-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,787] LMCache INFO:[0m Reqid: cmpl-benchmark-serving60-0, Total tokens 335, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:23,789] LMCache INFO:[0m Reqid: cmpl-benchmark-serving61-0, Total tokens 187, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53114 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,217] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving59-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,217] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1827 ms, throughput: 8.5170 GB/s; offload_time: 0.1728 ms, put_time: 0.0100 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,217] LMCache INFO:[0m Storing KV cache for 335 out of 335 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving60-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,218] LMCache INFO:[0m Stored 335 out of total 335 tokens. size: 0.0307 gb, cost 0.9762 ms, throughput: 31.4186 GB/s; offload_time: 0.9686 ms, put_time: 0.0076 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,218] LMCache INFO:[0m Storing KV cache for 187 out of 187 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving61-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,219] LMCache INFO:[0m Stored 187 out of total 187 tokens. size: 0.0171 gb, cost 0.5586 ms, throughput: 30.6497 GB/s; offload_time: 0.5538 ms, put_time: 0.0048 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,221] LMCache INFO:[0m Reqid: cmpl-benchmark-serving62-0, Total tokens 164, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:52940 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53130 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,439] LMCache INFO:[0m Storing KV cache for 164 out of 164 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving62-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,440] LMCache INFO:[0m Stored 164 out of total 164 tokens. size: 0.0150 gb, cost 0.5555 ms, throughput: 27.0305 GB/s; offload_time: 0.5483 ms, put_time: 0.0072 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,442] LMCache INFO:[0m Reqid: cmpl-benchmark-serving63-0, Total tokens 14, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,443] LMCache INFO:[0m Reqid: cmpl-benchmark-serving64-0, Total tokens 548, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42632 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53132 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53144 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53148 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,882] LMCache INFO:[0m Storing KV cache for 14 out of 14 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving63-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,882] LMCache INFO:[0m Stored 14 out of total 14 tokens. size: 0.0013 gb, cost 0.1644 ms, throughput: 7.7984 GB/s; offload_time: 0.1545 ms, put_time: 0.0099 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,882] LMCache INFO:[0m Storing KV cache for 548 out of 548 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving64-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,884] LMCache INFO:[0m Stored 548 out of total 548 tokens. size: 0.0502 gb, cost 1.5495 ms, throughput: 32.3793 GB/s; offload_time: 1.5418 ms, put_time: 0.0077 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,885] LMCache INFO:[0m Reqid: cmpl-benchmark-serving65-0, Total tokens 691, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,886] LMCache INFO:[0m Reqid: cmpl-benchmark-serving66-0, Total tokens 135, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,887] LMCache INFO:[0m Reqid: cmpl-benchmark-serving67-0, Total tokens 256, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:24,888] LMCache INFO:[0m Reqid: cmpl-benchmark-serving68-0, Total tokens 22, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:25,608] LMCache INFO:[0m Storing KV cache for 691 out of 691 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving65-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:25,610] LMCache INFO:[0m Stored 691 out of total 691 tokens. size: 0.0633 gb, cost 2.0130 ms, throughput: 31.4280 GB/s; offload_time: 2.0026 ms, put_time: 0.0103 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:25,610] LMCache INFO:[0m Storing KV cache for 135 out of 135 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving66-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:25,610] LMCache INFO:[0m Stored 135 out of total 135 tokens. size: 0.0124 gb, cost 0.4259 ms, throughput: 29.0203 GB/s; offload_time: 0.4203 ms, put_time: 0.0056 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:25,611] LMCache INFO:[0m Storing KV cache for 256 out of 256 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving67-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:25,611] LMCache INFO:[0m Stored 256 out of total 256 tokens. size: 0.0234 gb, cost 0.7157 ms, throughput: 32.7474 GB/s; offload_time: 0.7110 ms, put_time: 0.0047 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:25,611] LMCache INFO:[0m Storing KV cache for 22 out of 22 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving68-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:25,611] LMCache INFO:[0m Stored 22 out of total 22 tokens. size: 0.0020 gb, cost 0.1158 ms, throughput: 17.3895 GB/s; offload_time: 0.1112 ms, put_time: 0.0046 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42572 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:25,952] LMCache INFO:[0m Reqid: cmpl-benchmark-serving69-0, Total tokens 27, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,136] LMCache INFO:[0m Storing KV cache for 27 out of 27 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving69-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,136] LMCache INFO:[0m Stored 27 out of total 27 tokens. size: 0.0025 gb, cost 0.2254 ms, throughput: 10.9652 GB/s; offload_time: 0.2154 ms, put_time: 0.0101 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53150 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53162 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53176 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,309] LMCache INFO:[0m Reqid: cmpl-benchmark-serving70-0, Total tokens 268, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,310] LMCache INFO:[0m Reqid: cmpl-benchmark-serving71-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,311] LMCache INFO:[0m Reqid: cmpl-benchmark-serving72-0, Total tokens 4, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42550 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,597] LMCache INFO:[0m Storing KV cache for 268 out of 268 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving70-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,598] LMCache INFO:[0m Stored 268 out of total 268 tokens. size: 0.0245 gb, cost 0.8844 ms, throughput: 27.7435 GB/s; offload_time: 0.8735 ms, put_time: 0.0109 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,598] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving71-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,598] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.0953 ms, throughput: 10.5620 GB/s; offload_time: 0.0899 ms, put_time: 0.0054 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,599] LMCache INFO:[0m Storing KV cache for 4 out of 4 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving72-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,599] LMCache INFO:[0m Stored 4 out of total 4 tokens. size: 0.0004 gb, cost 0.0677 ms, throughput: 5.4092 GB/s; offload_time: 0.0630 ms, put_time: 0.0047 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,601] LMCache INFO:[0m Reqid: cmpl-benchmark-serving73-0, Total tokens 20, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,780] LMCache INFO:[0m Storing KV cache for 20 out of 20 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving73-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,780] LMCache INFO:[0m Stored 20 out of total 20 tokens. size: 0.0018 gb, cost 0.1895 ms, throughput: 9.6625 GB/s; offload_time: 0.1801 ms, put_time: 0.0094 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42664 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:26,953] LMCache INFO:[0m Reqid: cmpl-benchmark-serving74-0, Total tokens 343, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42686 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,299] LMCache INFO:[0m Storing KV cache for 343 out of 343 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving74-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,300] LMCache INFO:[0m Stored 343 out of total 343 tokens. size: 0.0314 gb, cost 1.0891 ms, throughput: 28.8346 GB/s; offload_time: 1.0777 ms, put_time: 0.0113 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,302] LMCache INFO:[0m Reqid: cmpl-benchmark-serving75-0, Total tokens 21, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42616 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53188 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,490] LMCache INFO:[0m Storing KV cache for 21 out of 21 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving75-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,490] LMCache INFO:[0m Stored 21 out of total 21 tokens. size: 0.0019 gb, cost 0.1714 ms, throughput: 11.2177 GB/s; offload_time: 0.1640 ms, put_time: 0.0074 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,492] LMCache INFO:[0m Reqid: cmpl-benchmark-serving76-0, Total tokens 38, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,494] LMCache INFO:[0m Reqid: cmpl-benchmark-serving77-0, Total tokens 289, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42568 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53196 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,838] LMCache INFO:[0m Storing KV cache for 38 out of 38 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving76-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,838] LMCache INFO:[0m Stored 38 out of total 38 tokens. size: 0.0035 gb, cost 0.2294 ms, throughput: 15.1682 GB/s; offload_time: 0.2202 ms, put_time: 0.0092 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,838] LMCache INFO:[0m Storing KV cache for 289 out of 289 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving77-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,839] LMCache INFO:[0m Stored 289 out of total 289 tokens. size: 0.0265 gb, cost 0.8609 ms, throughput: 30.7336 GB/s; offload_time: 0.8542 ms, put_time: 0.0067 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,841] LMCache INFO:[0m Reqid: cmpl-benchmark-serving78-0, Total tokens 289, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:27,842] LMCache INFO:[0m Reqid: cmpl-benchmark-serving79-0, Total tokens 63, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39032 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,191] LMCache INFO:[0m Storing KV cache for 289 out of 289 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving78-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,192] LMCache INFO:[0m Stored 289 out of total 289 tokens. size: 0.0265 gb, cost 0.9332 ms, throughput: 28.3526 GB/s; offload_time: 0.9233 ms, put_time: 0.0099 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,192] LMCache INFO:[0m Storing KV cache for 63 out of 63 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving79-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,193] LMCache INFO:[0m Stored 63 out of total 63 tokens. size: 0.0058 gb, cost 0.2371 ms, throughput: 24.3258 GB/s; offload_time: 0.2320 ms, put_time: 0.0051 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,195] LMCache INFO:[0m Reqid: cmpl-benchmark-serving80-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39044 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39048 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39050 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,387] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving80-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,387] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1681 ms, throughput: 9.2572 GB/s; offload_time: 0.1599 ms, put_time: 0.0082 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,389] LMCache INFO:[0m Reqid: cmpl-benchmark-serving81-0, Total tokens 14, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,390] LMCache INFO:[0m Reqid: cmpl-benchmark-serving82-0, Total tokens 24, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,391] LMCache INFO:[0m Reqid: cmpl-benchmark-serving83-0, Total tokens 13, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39052 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39054 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,593] LMCache INFO:[0m Storing KV cache for 14 out of 14 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving81-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,593] LMCache INFO:[0m Stored 14 out of total 14 tokens. size: 0.0013 gb, cost 0.1575 ms, throughput: 8.1406 GB/s; offload_time: 0.1491 ms, put_time: 0.0084 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,593] LMCache INFO:[0m Storing KV cache for 24 out of 24 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving82-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,593] LMCache INFO:[0m Stored 24 out of total 24 tokens. size: 0.0022 gb, cost 0.1350 ms, throughput: 16.2803 GB/s; offload_time: 0.1295 ms, put_time: 0.0054 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,593] LMCache INFO:[0m Storing KV cache for 13 out of 13 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving83-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,594] LMCache INFO:[0m Stored 13 out of total 13 tokens. size: 0.0012 gb, cost 0.0952 ms, throughput: 12.5056 GB/s; offload_time: 0.0904 ms, put_time: 0.0048 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,596] LMCache INFO:[0m Reqid: cmpl-benchmark-serving84-0, Total tokens 666, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:28,597] LMCache INFO:[0m Reqid: cmpl-benchmark-serving85-0, Total tokens 501, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39056 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39064 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,379] LMCache INFO:[0m Storing KV cache for 666 out of 666 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving84-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,381] LMCache INFO:[0m Stored 666 out of total 666 tokens. size: 0.0610 gb, cost 1.9595 ms, throughput: 31.1175 GB/s; offload_time: 1.9473 ms, put_time: 0.0122 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,381] LMCache INFO:[0m Storing KV cache for 501 out of 501 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving85-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,382] LMCache INFO:[0m Stored 501 out of total 501 tokens. size: 0.0459 gb, cost 1.4223 ms, throughput: 32.2493 GB/s; offload_time: 1.4156 ms, put_time: 0.0067 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,384] LMCache INFO:[0m Reqid: cmpl-benchmark-serving86-0, Total tokens 70, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,385] LMCache INFO:[0m Reqid: cmpl-benchmark-serving87-0, Total tokens 262, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39070 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39084 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39098 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,733] LMCache INFO:[0m Storing KV cache for 70 out of 70 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving86-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,733] LMCache INFO:[0m Stored 70 out of total 70 tokens. size: 0.0064 gb, cost 0.3067 ms, throughput: 20.8950 GB/s; offload_time: 0.2984 ms, put_time: 0.0083 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,733] LMCache INFO:[0m Storing KV cache for 262 out of 262 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving87-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,734] LMCache INFO:[0m Stored 262 out of total 262 tokens. size: 0.0240 gb, cost 0.7858 ms, throughput: 30.5272 GB/s; offload_time: 0.7788 ms, put_time: 0.0069 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,736] LMCache INFO:[0m Reqid: cmpl-benchmark-serving88-0, Total tokens 38, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,738] LMCache INFO:[0m Reqid: cmpl-benchmark-serving89-0, Total tokens 233, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:29,739] LMCache INFO:[0m Reqid: cmpl-benchmark-serving90-0, Total tokens 197, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39100 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,150] LMCache INFO:[0m Storing KV cache for 38 out of 38 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving88-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,150] LMCache INFO:[0m Stored 38 out of total 38 tokens. size: 0.0035 gb, cost 0.2119 ms, throughput: 16.4143 GB/s; offload_time: 0.2045 ms, put_time: 0.0075 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,150] LMCache INFO:[0m Storing KV cache for 233 out of 233 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving89-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,151] LMCache INFO:[0m Stored 233 out of total 233 tokens. size: 0.0213 gb, cost 0.6952 ms, throughput: 30.6843 GB/s; offload_time: 0.6897 ms, put_time: 0.0055 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,151] LMCache INFO:[0m Storing KV cache for 197 out of 197 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving90-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,151] LMCache INFO:[0m Stored 197 out of total 197 tokens. size: 0.0180 gb, cost 0.5919 ms, throughput: 30.4736 GB/s; offload_time: 0.5863 ms, put_time: 0.0055 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,154] LMCache INFO:[0m Reqid: cmpl-benchmark-serving91-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39108 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,348] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving91-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,349] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.1519 ms, throughput: 6.6318 GB/s; offload_time: 0.1438 ms, put_time: 0.0081 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,351] LMCache INFO:[0m Reqid: cmpl-benchmark-serving92-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39114 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,550] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving92-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,550] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.1475 ms, throughput: 6.8286 GB/s; offload_time: 0.1392 ms, put_time: 0.0083 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,553] LMCache INFO:[0m Reqid: cmpl-benchmark-serving93-0, Total tokens 9, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:42566 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,745] LMCache INFO:[0m Storing KV cache for 9 out of 9 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving93-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,745] LMCache INFO:[0m Stored 9 out of total 9 tokens. size: 0.0008 gb, cost 0.1485 ms, throughput: 5.5491 GB/s; offload_time: 0.1405 ms, put_time: 0.0080 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,747] LMCache INFO:[0m Reqid: cmpl-benchmark-serving94-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:53044 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,940] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving94-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,940] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1580 ms, throughput: 9.8499 GB/s; offload_time: 0.1501 ms, put_time: 0.0079 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:30,942] LMCache INFO:[0m Reqid: cmpl-benchmark-serving95-0, Total tokens 299, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39120 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39128 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39134 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO:     127.0.0.1:39148 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:31 [loggers.py:236] Engine 000: Avg prompt throughput: 761.5 tokens/s, Avg generation throughput: 215.3 tokens/s, Running: 78 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.9%, Prefix cache hit rate: 3.5%, External prefix cache hit rate: 0.0%
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:31,248] LMCache INFO:[0m Storing KV cache for 299 out of 299 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving95-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:31,249] LMCache INFO:[0m Stored 299 out of total 299 tokens. size: 0.0274 gb, cost 0.9501 ms, throughput: 28.8134 GB/s; offload_time: 0.9394 ms, put_time: 0.0106 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:31,251] LMCache INFO:[0m Reqid: cmpl-benchmark-serving96-0, Total tokens 78, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:31,252] LMCache INFO:[0m Reqid: cmpl-benchmark-serving97-0, Total tokens 583, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:31,253] LMCache INFO:[0m Reqid: cmpl-benchmark-serving98-0, Total tokens 454, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:31,255] LMCache INFO:[0m Reqid: cmpl-benchmark-serving99-0, Total tokens 239, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:32,169] LMCache INFO:[0m Storing KV cache for 78 out of 78 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving96-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:32,170] LMCache INFO:[0m Stored 78 out of total 78 tokens. size: 0.0071 gb, cost 0.3742 ms, throughput: 19.0837 GB/s; offload_time: 0.3636 ms, put_time: 0.0106 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:32,170] LMCache INFO:[0m Storing KV cache for 583 out of 583 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving97-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:32,171] LMCache INFO:[0m Stored 583 out of total 583 tokens. size: 0.0534 gb, cost 1.6610 ms, throughput: 32.1351 GB/s; offload_time: 1.6529 ms, put_time: 0.0080 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:32,171] LMCache INFO:[0m Storing KV cache for 454 out of 454 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving98-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:32,173] LMCache INFO:[0m Stored 454 out of total 454 tokens. size: 0.0416 gb, cost 1.2887 ms, throughput: 32.2546 GB/s; offload_time: 1.2818 ms, put_time: 0.0068 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:32,173] LMCache INFO:[0m Storing KV cache for 239 out of 239 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving99-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=482991)[0;0m [32;20m[2025-12-08 02:20:32,174] LMCache INFO:[0m Stored 239 out of total 239 tokens. size: 0.0219 gb, cost 0.6963 ms, throughput: 31.4242 GB/s; offload_time: 0.6914 ms, put_time: 0.0049 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:41 [loggers.py:236] Engine 000: Avg prompt throughput: 165.3 tokens/s, Avg generation throughput: 364.4 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.1%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:20:51 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 325.2 tokens/s, Running: 56 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.8%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:21:01 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 288.7 tokens/s, Running: 48 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.6%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:21:11 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 259.1 tokens/s, Running: 42 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.4%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:21:21 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 236.2 tokens/s, Running: 36 reqs, Waiting: 0 reqs, GPU KV cache usage: 25.9%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:21:31 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 29 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.6%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:21:41 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 168.0 tokens/s, Running: 20 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.1%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:21:51 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 138.8 tokens/s, Running: 15 reqs, Waiting: 0 reqs, GPU KV cache usage: 15.9%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:22:01 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 116.1 tokens/s, Running: 9 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.1%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:22:11 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 91.7 tokens/s, Running: 5 reqs, Waiting: 0 reqs, GPU KV cache usage: 8.5%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:22:21 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 47.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=482896)[0;0m INFO 12-08 02:22:22 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=482896)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=482896)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=482896)[0;0m INFO:     Application shutdown complete.
