[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:38 [api_server.py:1772] vLLM API server version 0.12.0
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:38 [utils.py:253] non-default args: {'model_tag': 'Qwen/Qwen3-30B-A3B-FP8', 'model': 'Qwen/Qwen3-30B-A3B-FP8', 'max_model_len': 2048, 'enforce_eager': True, 'cpu_offload_gb': 4.0, 'kv_transfer_config': KVTransferConfig(kv_connector='LMCacheConnectorV1', engine_id='397a76f7-e3e8-4ac8-9f49-9a16204473de', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None, enable_permute_local_kv=False)}
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:38 [model.py:637] Resolved architecture: Qwen3MoeForCausalLM
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:38 [model.py:1750] Using max model len 2048
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:39 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=485000)[0;0m WARNING 12-08 02:24:39 [vllm.py:601] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:39 [vllm.py:707] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=485000)[0;0m WARNING 12-08 02:24:39 [vllm.py:896] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:42 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='Qwen/Qwen3-30B-A3B-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'all', '+quant_fp8'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:43 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://128.114.59.195:50167 backend=nccl
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:43 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:43 [gpu_model_runner.py:3467] Starting to load model Qwen/Qwen3-30B-A3B-FP8...
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:43 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:43 [layer.py:379] Enabled separate cuda stream for MoE shared_experts
[0;36m(EngineCore_DP0 pid=485122)[0;0m WARNING 12-08 02:24:43 [fp8.py:183] DeepGEMM backend requested but not available.
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:43 [fp8.py:202] Using Triton backend for FP8 MoE
[0;36m(EngineCore_DP0 pid=485122)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=485122)[0;0m Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:02,  2.00it/s]
[0;36m(EngineCore_DP0 pid=485122)[0;0m Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:01<00:02,  1.69it/s]
[0;36m(EngineCore_DP0 pid=485122)[0;0m Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:01<00:02,  1.62it/s]
[0;36m(EngineCore_DP0 pid=485122)[0;0m Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:02<00:01,  2.04it/s]
[0;36m(EngineCore_DP0 pid=485122)[0;0m Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:02<00:01,  1.85it/s]
[0;36m(EngineCore_DP0 pid=485122)[0;0m Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:03<00:00,  1.73it/s]
[0;36m(EngineCore_DP0 pid=485122)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:04<00:00,  1.66it/s]
[0;36m(EngineCore_DP0 pid=485122)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:04<00:00,  1.74it/s]
[0;36m(EngineCore_DP0 pid=485122)[0;0m 
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:49 [default_loader.py:308] Loading weights took 4.07 seconds
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:49 [gpu_model_runner.py:3549] Model loading took 24.9801 GiB memory and 5.871163 seconds
[0;36m(EngineCore_DP0 pid=485122)[0;0m WARNING 12-08 02:24:49 [fused_moe.py:888] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_GeForce_RTX_5090,dtype=fp8_w8a8,block_shape=[128,128].json']
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:50 [gpu_worker.py:359] Available KV cache memory: 1.76 GiB
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:51 [kv_cache_utils.py:1286] GPU KV cache size: 19,168 tokens
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:51 [kv_cache_utils.py:1291] Maximum concurrency for 2,048 tokens per request: 9.36x
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:51 [factory.py:64] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 397a76f7-e3e8-4ac8-9f49-9a16204473de
[0;36m(EngineCore_DP0 pid=485122)[0;0m WARNING 12-08 02:24:51 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:51 [lmcache_connector.py:52] Initializing latest dev LMCache connector
[0;36m(EngineCore_DP0 pid=485122)[0;0m [33;20m[2025-12-08 02:24:51,142] LMCache WARNING:[0m No LMCache configuration file is set. Trying to read configurations from the environment variables. [3m(utils.py:61:lmcache.integration.vllm.utils)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [33;20m[2025-12-08 02:24:51,142] LMCache WARNING:[0m You can set the configuration file through the environment variable: LMCACHE_CONFIG_FILE [3m(utils.py:65:lmcache.integration.vllm.utils)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,142] LMCache INFO:[0m use mla: False, kv shape: (48, 2, 256, 4, 128), num_draft_layers:0 [3m(vllm_v1_adapter.py:503:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,142] LMCache INFO:[0m CUDA device is available. Using CUDA for LMCache engine. [3m(vllm_v1_adapter.py:509:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,143] LMCache INFO:[0m Creating LMCacheEngine instance vllm-instance [3m(cache_engine.py:1531:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,143] LMCache INFO:[0m NUMA mapping for instance vllm-instance: None [3m(cache_engine.py:1534:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [33;20m[2025-12-08 02:24:51,143] LMCache WARNING:[0m Could not load 'builtin' from vLLM. Using builtin hash. This may cause inconsistencies in distributed caching. [3m(token_database.py:136:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [33;20m[2025-12-08 02:24:51,143] LMCache WARNING:[0m Using builtin hash without PYTHONHASHSEED set. For production environments (non-testing scenarios), you MUST set PYTHONHASHSEED to ensure consistent hashing across processes. Example: export PYTHONHASHSEED=0 [3m(token_database.py:143:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,143] LMCache INFO:[0m Initialized NONE_HASH=b'\x1c\xa0\xe7A\xc9e\x13\xc6Q\x7fu\x86\x14[\xf6L~\x86D\xb3\xef\xc8\x96\x8b\xa5G\xe1\xfeu>W\xfc' from vLLM (>= PR#20511) [3m(token_database.py:74:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,143] LMCache INFO:[0m Using hash algorithm: builtin [3m(token_database.py:84:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,143] LMCache INFO:[0m sending cache usage stats to http://stats.lmcache.ai:8080/cache-usage [3m(usage_context.py:290:lmcache.usage_context)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,146] LMCache INFO:[0m Creating LMCacheEngine with config: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': 8.0, 'reserve_local_cpu_size': 0.0, 'local_disk': None, 'max_local_disk_size': 0.0, 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'p2p_host': None, 'p2p_init_ports': None, 'p2p_lookup_ports': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_instance_81e0526d2b31451ab00fe0f4513e0aab', 'controller_pull_url': None, 'controller_reply_url': None, 'lmcache_worker_ports': None, 'lmcache_worker_ids': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_pd': False, 'pd_role': None, 'pd_buffer_size': None, 'pd_buffer_device': None, 'pd_peer_host': None, 'pd_peer_init_port': None, 'pd_peer_alloc_port': None, 'pd_proxy_host': None, 'pd_proxy_port': None, 'transfer_channel': None, 'nixl_backends': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None, 'lookup_timeout_ms': 3000, 'hit_miss_ratio': None, 'lookup_server_worker_ids': None, 'enable_scheduler_bypass_lookup': False, 'script_allowed_imports': None, 'enable_lazy_memory_allocator': False, 'lazy_memory_initial_ratio': 0.2, 'lazy_memory_expand_trigger_ratio': 0.5, 'lazy_memory_step_ratio': 0.1, 'lazy_memory_safe_size': 0.0, 'enable_chunk_statistics': False, 'chunk_statistics_auto_start_statistics': False, 'chunk_statistics_auto_exit_timeout_hours': 0.0, 'chunk_statistics_auto_exit_target_unique_chunks': 0, 'chunk_statistics_strategy': 'memory_bloom_filter'} [3m(cache_engine.py:93:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,146] LMCache INFO:[0m LMCacheWorker is not initialized (related configs: enable_controller: False, role: worker, worker_id: 0, worker_ids: [0]). [3m(cache_engine.py:135:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,146] LMCache INFO:[0m Initialize storage manager on rank 0, use layerwise: False,save only first rank: False [3m(cache_engine.py:167:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,147] LMCache INFO:[0m Initializing LRUCachePolicy [3m(lru.py:22:lmcache.v1.storage_backend.cache_policy.lru)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:51,147] LMCache INFO:[0m NUMA mapping None [3m(local_cpu_backend.py:351:lmcache.v1.storage_backend.local_cpu_backend)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [33;20m[2025-12-08 02:24:52,586] LMCache WARNING:[0m Controller message sender is not initialized [3m(local_cpu_backend.py:103:lmcache.v1.storage_backend.local_cpu_backend)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:52,586] LMCache INFO:[0m Initializing usage context. [3m(usage_context.py:362:lmcache.usage_context)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:53,983] LMCache INFO:[0m lmcache lookup server start on /tmp/engine_397a76f7-e3e8-4ac8-9f49-9a16204473de_service_lookup_lmcache_rpc_port_0 [3m(lmcache_lookup_client.py:329:lmcache.v1.lookup_client.lmcache_lookup_client)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:53,984] LMCache INFO:[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=1, port=7000, socket_path=None, include_index_list=None [3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:53,984] LMCache INFO:[0m LMCache initialized for role KVConnectorRole.WORKER with version 0.3.11.dev18-g58cae13ba, vllm version 0.12.0, lmcache cache_engine metadata: LMCacheEngineMetadata(model_name='Qwen/Qwen3-30B-A3B-FP8', world_size=1, worker_id=0, fmt='vllm', kv_dtype=torch.bfloat16, kv_shape=(48, 2, 256, 4, 128), use_mla=False, role='worker') [3m(vllm_v1_adapter.py:793:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:53 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=485122)[0;0m 2025-12-08 02:24:53,987 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=485122)[0;0m 2025-12-08 02:24:54,004 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:54 [core.py:254] init engine (profile, create kv cache, warmup model) took 5.40 seconds
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:55 [factory.py:64] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 397a76f7-e3e8-4ac8-9f49-9a16204473de
[0;36m(EngineCore_DP0 pid=485122)[0;0m WARNING 12-08 02:24:55 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:55 [lmcache_connector.py:52] Initializing latest dev LMCache connector
[0;36m(EngineCore_DP0 pid=485122)[0;0m [31;20m[2025-12-08 02:24:55,486] LMCache ERROR:[0m PrometheusLogger instance already created withdifferent metadata. This should not happen except in test [3m(observability.py:1325:lmcache.observability)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:55,487] LMCache INFO:[0m lmcache lookup client connect to rank 0 with socket path /tmp/engine_397a76f7-e3e8-4ac8-9f49-9a16204473de_service_lookup_lmcache_rpc_port_0 [3m(lmcache_lookup_client.py:93:lmcache.v1.lookup_client.lmcache_lookup_client)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [33;20m[2025-12-08 02:24:55,487] LMCache WARNING:[0m Could not load 'builtin' from vLLM. Using builtin hash. This may cause inconsistencies in distributed caching. [3m(token_database.py:136:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [33;20m[2025-12-08 02:24:55,487] LMCache WARNING:[0m Using builtin hash without PYTHONHASHSEED set. For production environments (non-testing scenarios), you MUST set PYTHONHASHSEED to ensure consistent hashing across processes. Example: export PYTHONHASHSEED=0 [3m(token_database.py:143:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:55,487] LMCache INFO:[0m Initialized NONE_HASH=b'\xd4eRc1\xfb\xb3\x9cp\xbd\xc7b\xf5\xb0\x1d\x81\xb3\xcf\xbe\xfc\xdev\x8c\x18z\xc9\x9e|\xa7\xdb\xb8\x82' from vLLM (>= PR#20511) [3m(token_database.py:74:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:55,487] LMCache INFO:[0m Using hash algorithm: builtin [3m(token_database.py:84:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:55,487] LMCache INFO:[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=0, port=6999, socket_path=None, include_index_list=None [3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:24:55,487] LMCache INFO:[0m LMCache initialized for role KVConnectorRole.SCHEDULER with version 0.3.11.dev18-g58cae13ba, vllm version 0.12.0, lmcache cache_engine metadata: None [3m(vllm_v1_adapter.py:793:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m WARNING 12-08 02:24:55 [vllm.py:608] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=485122)[0;0m INFO 12-08 02:24:55 [vllm.py:707] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:55 [api_server.py:1520] Supported tasks: ['generate']
[0;36m(APIServer pid=485000)[0;0m WARNING 12-08 02:24:56 [model.py:1576] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [serving_responses.py:194] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [serving_completion.py:73] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [api_server.py:1847] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /openapi.json, Methods: HEAD, GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /docs, Methods: HEAD, GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /redoc, Methods: HEAD, GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:24:56 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=485000)[0;0m INFO:     Started server process [485000]
[0;36m(APIServer pid=485000)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=485000)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:40838 - "GET /health HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:40848 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:05,332] LMCache INFO:[0m Reqid: cmpl-9db2dc1be9a68482-0, Total tokens 729, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:05,334] LMCache INFO:[0m Post-initializing LMCacheEngine [3m(cache_engine.py:226:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:05,630] LMCache INFO:[0m Storing KV cache for 729 out of 729 tokens (skip_leading_tokens=0) for request cmpl-9db2dc1be9a68482-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:05,633] LMCache INFO:[0m Stored 729 out of total 729 tokens. size: 0.0667 gb, cost 2.4106 ms, throughput: 27.6864 GB/s; offload_time: 2.3874 ms, put_time: 0.0232 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:25:06 [loggers.py:236] Engine 000: Avg prompt throughput: 68.8 tokens/s, Avg generation throughput: 3.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 0.0%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:40848 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:07,995] LMCache INFO:[0m Reqid: cmpl-benchmark-serving0-0, Total tokens 729, LMCache hit tokens: 729, need to load: 8 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:07,997] LMCache INFO:[0m Retrieved 217 out of 217 required tokens (from 729 total tokens). size: 0.0199 gb, cost 0.6345 ms, throughput: 31.3100 GB/s; [3m(cache_engine.py:582:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60552 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:08,656] LMCache INFO:[0m Reqid: cmpl-benchmark-serving1-0, Total tokens 9, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:08,702] LMCache INFO:[0m Storing KV cache for 9 out of 9 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving1-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:08,702] LMCache INFO:[0m Stored 9 out of total 9 tokens. size: 0.0008 gb, cost 0.1587 ms, throughput: 5.1931 GB/s; offload_time: 0.1474 ms, put_time: 0.0112 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60568 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:08,951] LMCache INFO:[0m Reqid: cmpl-benchmark-serving2-0, Total tokens 110, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,038] LMCache INFO:[0m Storing KV cache for 110 out of 110 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving2-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,039] LMCache INFO:[0m Stored 110 out of total 110 tokens. size: 0.0101 gb, cost 0.4148 ms, throughput: 24.2790 GB/s; offload_time: 0.4065 ms, put_time: 0.0083 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60580 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,162] LMCache INFO:[0m Reqid: cmpl-benchmark-serving3-0, Total tokens 59, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60584 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60598 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60612 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,246] LMCache INFO:[0m Storing KV cache for 59 out of 59 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving3-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,246] LMCache INFO:[0m Stored 59 out of total 59 tokens. size: 0.0054 gb, cost 0.2834 ms, throughput: 19.0623 GB/s; offload_time: 0.2747 ms, put_time: 0.0086 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,248] LMCache INFO:[0m Reqid: cmpl-benchmark-serving4-0, Total tokens 21, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,249] LMCache INFO:[0m Reqid: cmpl-benchmark-serving5-0, Total tokens 57, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,251] LMCache INFO:[0m Reqid: cmpl-benchmark-serving6-0, Total tokens 36, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,349] LMCache INFO:[0m Storing KV cache for 21 out of 21 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving4-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,349] LMCache INFO:[0m Stored 21 out of total 21 tokens. size: 0.0019 gb, cost 0.1493 ms, throughput: 12.8808 GB/s; offload_time: 0.1425 ms, put_time: 0.0068 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,349] LMCache INFO:[0m Storing KV cache for 57 out of 57 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving5-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,349] LMCache INFO:[0m Stored 57 out of total 57 tokens. size: 0.0052 gb, cost 0.2188 ms, throughput: 23.8468 GB/s; offload_time: 0.2135 ms, put_time: 0.0054 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,349] LMCache INFO:[0m Storing KV cache for 36 out of 36 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving6-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,350] LMCache INFO:[0m Stored 36 out of total 36 tokens. size: 0.0033 gb, cost 0.1554 ms, throughput: 21.2053 GB/s; offload_time: 0.1509 ms, put_time: 0.0045 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60618 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,682] LMCache INFO:[0m Reqid: cmpl-benchmark-serving7-0, Total tokens 78, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,773] LMCache INFO:[0m Storing KV cache for 78 out of 78 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving7-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,774] LMCache INFO:[0m Stored 78 out of total 78 tokens. size: 0.0071 gb, cost 0.3129 ms, throughput: 22.8188 GB/s; offload_time: 0.3062 ms, put_time: 0.0068 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60622 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:09,876] LMCache INFO:[0m Reqid: cmpl-benchmark-serving8-0, Total tokens 667, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,117] LMCache INFO:[0m Storing KV cache for 667 out of 667 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving8-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,119] LMCache INFO:[0m Stored 667 out of total 667 tokens. size: 0.0611 gb, cost 1.9729 ms, throughput: 30.9526 GB/s; offload_time: 1.9587 ms, put_time: 0.0142 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60630 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60634 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,169] LMCache INFO:[0m Reqid: cmpl-benchmark-serving9-0, Total tokens 414, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,170] LMCache INFO:[0m Reqid: cmpl-benchmark-serving10-0, Total tokens 227, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,403] LMCache INFO:[0m Storing KV cache for 414 out of 414 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving9-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,404] LMCache INFO:[0m Stored 414 out of total 414 tokens. size: 0.0379 gb, cost 1.2535 ms, throughput: 30.2366 GB/s; offload_time: 1.2447 ms, put_time: 0.0089 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,404] LMCache INFO:[0m Storing KV cache for 227 out of 227 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving10-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,405] LMCache INFO:[0m Stored 227 out of total 227 tokens. size: 0.0208 gb, cost 0.6708 ms, throughput: 30.9816 GB/s; offload_time: 0.6659 ms, put_time: 0.0049 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60630 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,909] LMCache INFO:[0m Reqid: cmpl-benchmark-serving11-0, Total tokens 7, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,979] LMCache INFO:[0m Storing KV cache for 7 out of 7 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving11-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:10,979] LMCache INFO:[0m Stored 7 out of total 7 tokens. size: 0.0006 gb, cost 0.1372 ms, throughput: 4.6713 GB/s; offload_time: 0.1293 ms, put_time: 0.0079 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60636 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,328] LMCache INFO:[0m Reqid: cmpl-benchmark-serving12-0, Total tokens 379, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60640 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60644 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60660 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,498] LMCache INFO:[0m Storing KV cache for 379 out of 379 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving12-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,499] LMCache INFO:[0m Stored 379 out of total 379 tokens. size: 0.0347 gb, cost 1.1822 ms, throughput: 29.3500 GB/s; offload_time: 1.1711 ms, put_time: 0.0111 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,501] LMCache INFO:[0m Reqid: cmpl-benchmark-serving13-0, Total tokens 401, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,502] LMCache INFO:[0m Reqid: cmpl-benchmark-serving14-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,503] LMCache INFO:[0m Reqid: cmpl-benchmark-serving15-0, Total tokens 743, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60666 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60682 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60690 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60704 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,879] LMCache INFO:[0m Storing KV cache for 401 out of 401 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving13-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,880] LMCache INFO:[0m Stored 401 out of total 401 tokens. size: 0.0367 gb, cost 1.2427 ms, throughput: 29.5428 GB/s; offload_time: 1.2316 ms, put_time: 0.0111 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,880] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving14-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,880] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1155 ms, throughput: 13.4728 GB/s; offload_time: 0.1081 ms, put_time: 0.0074 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,880] LMCache INFO:[0m Storing KV cache for 743 out of 743 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving15-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,882] LMCache INFO:[0m Stored 743 out of total 743 tokens. size: 0.0680 gb, cost 2.0692 ms, throughput: 32.8751 GB/s; offload_time: 2.0620 ms, put_time: 0.0072 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,884] LMCache INFO:[0m Reqid: cmpl-benchmark-serving16-0, Total tokens 245, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,885] LMCache INFO:[0m Reqid: cmpl-benchmark-serving17-0, Total tokens 20, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,886] LMCache INFO:[0m Reqid: cmpl-benchmark-serving18-0, Total tokens 403, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:11,887] LMCache INFO:[0m Reqid: cmpl-benchmark-serving19-0, Total tokens 325, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60708 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60712 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60726 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,217] LMCache INFO:[0m Storing KV cache for 245 out of 245 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving16-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,218] LMCache INFO:[0m Stored 245 out of total 245 tokens. size: 0.0224 gb, cost 0.8069 ms, throughput: 27.7971 GB/s; offload_time: 0.7966 ms, put_time: 0.0103 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,218] LMCache INFO:[0m Storing KV cache for 20 out of 20 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving17-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,218] LMCache INFO:[0m Stored 20 out of total 20 tokens. size: 0.0018 gb, cost 0.1304 ms, throughput: 14.0380 GB/s; offload_time: 0.1254 ms, put_time: 0.0051 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,218] LMCache INFO:[0m Storing KV cache for 403 out of 403 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving18-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,220] LMCache INFO:[0m Stored 403 out of total 403 tokens. size: 0.0369 gb, cost 1.1521 ms, throughput: 32.0238 GB/s; offload_time: 1.1461 ms, put_time: 0.0060 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,220] LMCache INFO:[0m Storing KV cache for 325 out of 325 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving19-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,221] LMCache INFO:[0m Stored 325 out of total 325 tokens. size: 0.0298 gb, cost 0.9503 ms, throughput: 31.3120 GB/s; offload_time: 0.9447 ms, put_time: 0.0056 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,223] LMCache INFO:[0m Reqid: cmpl-benchmark-serving20-0, Total tokens 61, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,224] LMCache INFO:[0m Reqid: cmpl-benchmark-serving21-0, Total tokens 412, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,225] LMCache INFO:[0m Reqid: cmpl-benchmark-serving22-0, Total tokens 88, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60742 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60758 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,452] LMCache INFO:[0m Storing KV cache for 61 out of 61 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving20-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,452] LMCache INFO:[0m Stored 61 out of total 61 tokens. size: 0.0056 gb, cost 0.2757 ms, throughput: 20.2530 GB/s; offload_time: 0.2677 ms, put_time: 0.0081 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,452] LMCache INFO:[0m Storing KV cache for 412 out of 412 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving21-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,453] LMCache INFO:[0m Stored 412 out of total 412 tokens. size: 0.0377 gb, cost 1.1802 ms, throughput: 31.9596 GB/s; offload_time: 1.1735 ms, put_time: 0.0068 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,453] LMCache INFO:[0m Storing KV cache for 88 out of 88 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving22-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,454] LMCache INFO:[0m Stored 88 out of total 88 tokens. size: 0.0081 gb, cost 0.2932 ms, throughput: 27.4820 GB/s; offload_time: 0.2885 ms, put_time: 0.0046 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,456] LMCache INFO:[0m Reqid: cmpl-benchmark-serving23-0, Total tokens 181, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,457] LMCache INFO:[0m Reqid: cmpl-benchmark-serving24-0, Total tokens 26, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,572] LMCache INFO:[0m Storing KV cache for 181 out of 181 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving23-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,573] LMCache INFO:[0m Stored 181 out of total 181 tokens. size: 0.0166 gb, cost 0.5768 ms, throughput: 28.7311 GB/s; offload_time: 0.5703 ms, put_time: 0.0065 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,573] LMCache INFO:[0m Storing KV cache for 26 out of 26 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving24-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,573] LMCache INFO:[0m Stored 26 out of total 26 tokens. size: 0.0024 gb, cost 0.1324 ms, throughput: 17.9809 GB/s; offload_time: 0.1271 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60766 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,798] LMCache INFO:[0m Reqid: cmpl-benchmark-serving25-0, Total tokens 28, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60770 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,890] LMCache INFO:[0m Storing KV cache for 28 out of 28 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving25-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,890] LMCache INFO:[0m Stored 28 out of total 28 tokens. size: 0.0026 gb, cost 0.1980 ms, throughput: 12.9459 GB/s; offload_time: 0.1896 ms, put_time: 0.0084 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:12,892] LMCache INFO:[0m Reqid: cmpl-benchmark-serving26-0, Total tokens 188, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60780 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,004] LMCache INFO:[0m Storing KV cache for 188 out of 188 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving26-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,004] LMCache INFO:[0m Stored 188 out of total 188 tokens. size: 0.0172 gb, cost 0.6047 ms, throughput: 28.4639 GB/s; offload_time: 0.5974 ms, put_time: 0.0072 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,007] LMCache INFO:[0m Reqid: cmpl-benchmark-serving27-0, Total tokens 507, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60796 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60800 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,219] LMCache INFO:[0m Storing KV cache for 507 out of 507 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving27-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,220] LMCache INFO:[0m Stored 507 out of total 507 tokens. size: 0.0464 gb, cost 1.5070 ms, throughput: 30.8016 GB/s; offload_time: 1.4967 ms, put_time: 0.0103 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,223] LMCache INFO:[0m Reqid: cmpl-benchmark-serving28-0, Total tokens 249, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,224] LMCache INFO:[0m Reqid: cmpl-benchmark-serving29-0, Total tokens 384, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60802 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60814 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60822 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,463] LMCache INFO:[0m Storing KV cache for 249 out of 249 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving28-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,464] LMCache INFO:[0m Stored 249 out of total 249 tokens. size: 0.0228 gb, cost 0.7932 ms, throughput: 28.7407 GB/s; offload_time: 0.7841 ms, put_time: 0.0091 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,464] LMCache INFO:[0m Storing KV cache for 384 out of 384 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving29-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,465] LMCache INFO:[0m Stored 384 out of total 384 tokens. size: 0.0352 gb, cost 1.1099 ms, throughput: 31.6747 GB/s; offload_time: 1.0993 ms, put_time: 0.0106 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,468] LMCache INFO:[0m Reqid: cmpl-benchmark-serving30-0, Total tokens 835, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,469] LMCache INFO:[0m Reqid: cmpl-benchmark-serving31-0, Total tokens 386, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,470] LMCache INFO:[0m Reqid: cmpl-benchmark-serving32-0, Total tokens 5, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,865] LMCache INFO:[0m Storing KV cache for 835 out of 835 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving30-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,868] LMCache INFO:[0m Stored 835 out of total 835 tokens. size: 0.0764 gb, cost 2.3972 ms, throughput: 31.8896 GB/s; offload_time: 2.3862 ms, put_time: 0.0110 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,868] LMCache INFO:[0m Storing KV cache for 386 out of 386 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving31-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,869] LMCache INFO:[0m Stored 386 out of total 386 tokens. size: 0.0353 gb, cost 1.1040 ms, throughput: 32.0109 GB/s; offload_time: 1.0975 ms, put_time: 0.0065 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,869] LMCache INFO:[0m Storing KV cache for 5 out of 5 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving32-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:13,869] LMCache INFO:[0m Stored 5 out of total 5 tokens. size: 0.0005 gb, cost 0.0687 ms, throughput: 6.6674 GB/s; offload_time: 0.0635 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60742 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:14,119] LMCache INFO:[0m Reqid: cmpl-benchmark-serving33-0, Total tokens 679, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:14,393] LMCache INFO:[0m Storing KV cache for 679 out of 679 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving33-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:14,395] LMCache INFO:[0m Stored 679 out of total 679 tokens. size: 0.0622 gb, cost 1.9897 ms, throughput: 31.2428 GB/s; offload_time: 1.9780 ms, put_time: 0.0118 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60814 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:14,831] LMCache INFO:[0m Reqid: cmpl-benchmark-serving34-0, Total tokens 25, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:14,933] LMCache INFO:[0m Storing KV cache for 25 out of 25 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving34-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:14,934] LMCache INFO:[0m Stored 25 out of total 25 tokens. size: 0.0023 gb, cost 0.1929 ms, throughput: 11.8649 GB/s; offload_time: 0.1848 ms, put_time: 0.0081 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60824 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,200] LMCache INFO:[0m Reqid: cmpl-benchmark-serving35-0, Total tokens 742, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60690 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60830 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,469] LMCache INFO:[0m Storing KV cache for 742 out of 742 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving35-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,472] LMCache INFO:[0m Stored 742 out of total 742 tokens. size: 0.0679 gb, cost 2.1602 ms, throughput: 31.4472 GB/s; offload_time: 2.1486 ms, put_time: 0.0116 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,474] LMCache INFO:[0m Reqid: cmpl-benchmark-serving36-0, Total tokens 385, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,475] LMCache INFO:[0m Reqid: cmpl-benchmark-serving37-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60836 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60842 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,671] LMCache INFO:[0m Storing KV cache for 385 out of 385 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving36-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,673] LMCache INFO:[0m Stored 385 out of total 385 tokens. size: 0.0352 gb, cost 1.1756 ms, throughput: 29.9828 GB/s; offload_time: 1.1662 ms, put_time: 0.0094 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,673] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving37-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,673] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.2177 ms, throughput: 24.3870 GB/s; offload_time: 0.2128 ms, put_time: 0.0049 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,675] LMCache INFO:[0m Reqid: cmpl-benchmark-serving38-0, Total tokens 332, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,676] LMCache INFO:[0m Reqid: cmpl-benchmark-serving39-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60852 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,832] LMCache INFO:[0m Storing KV cache for 332 out of 332 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving38-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,833] LMCache INFO:[0m Stored 332 out of total 332 tokens. size: 0.0304 gb, cost 1.0022 ms, throughput: 30.3291 GB/s; offload_time: 0.9940 ms, put_time: 0.0082 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,833] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving39-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,833] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.0903 ms, throughput: 11.1488 GB/s; offload_time: 0.0856 ms, put_time: 0.0047 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,835] LMCache INFO:[0m Reqid: cmpl-benchmark-serving40-0, Total tokens 37, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60864 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60636 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,939] LMCache INFO:[0m Storing KV cache for 37 out of 37 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving40-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,939] LMCache INFO:[0m Stored 37 out of total 37 tokens. size: 0.0034 gb, cost 0.1945 ms, throughput: 17.4182 GB/s; offload_time: 0.1880 ms, put_time: 0.0065 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,941] LMCache INFO:[0m Reqid: cmpl-benchmark-serving41-0, Total tokens 161, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:15,942] LMCache INFO:[0m Reqid: cmpl-benchmark-serving42-0, Total tokens 788, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,262] LMCache INFO:[0m Storing KV cache for 161 out of 161 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving41-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,262] LMCache INFO:[0m Stored 161 out of total 161 tokens. size: 0.0147 gb, cost 0.5310 ms, throughput: 27.7584 GB/s; offload_time: 0.5240 ms, put_time: 0.0070 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,262] LMCache INFO:[0m Storing KV cache for 788 out of 788 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving42-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,265] LMCache INFO:[0m Stored 788 out of total 788 tokens. size: 0.0721 gb, cost 2.1967 ms, throughput: 32.8416 GB/s; offload_time: 2.1882 ms, put_time: 0.0085 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60880 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:25:16 [loggers.py:236] Engine 000: Avg prompt throughput: 1151.4 tokens/s, Avg generation throughput: 136.9 tokens/s, Running: 38 reqs, Waiting: 0 reqs, GPU KV cache usage: 58.8%, Prefix cache hit rate: 5.9%, External prefix cache hit rate: 0.1%
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60896 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,440] LMCache INFO:[0m Reqid: cmpl-benchmark-serving43-0, Total tokens 30, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,539] LMCache INFO:[0m Storing KV cache for 30 out of 30 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving43-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,539] LMCache INFO:[0m Stored 30 out of total 30 tokens. size: 0.0027 gb, cost 0.2111 ms, throughput: 13.0114 GB/s; offload_time: 0.2026 ms, put_time: 0.0085 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,541] LMCache INFO:[0m Reqid: cmpl-benchmark-serving44-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,647] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving44-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,647] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.2483 ms, throughput: 21.3873 GB/s; offload_time: 0.2421 ms, put_time: 0.0062 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60712 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,739] LMCache INFO:[0m Reqid: cmpl-benchmark-serving45-0, Total tokens 24, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60666 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,841] LMCache INFO:[0m Storing KV cache for 24 out of 24 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving45-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,841] LMCache INFO:[0m Stored 24 out of total 24 tokens. size: 0.0022 gb, cost 0.2109 ms, throughput: 10.4197 GB/s; offload_time: 0.2009 ms, put_time: 0.0099 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:16,843] LMCache INFO:[0m Reqid: cmpl-benchmark-serving46-0, Total tokens 335, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60770 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,000] LMCache INFO:[0m Storing KV cache for 335 out of 335 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving46-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,001] LMCache INFO:[0m Stored 335 out of total 335 tokens. size: 0.0307 gb, cost 1.0324 ms, throughput: 29.7082 GB/s; offload_time: 1.0233 ms, put_time: 0.0091 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,003] LMCache INFO:[0m Reqid: cmpl-benchmark-serving47-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:40848 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,108] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving47-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,108] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.2548 ms, throughput: 20.8368 GB/s; offload_time: 0.2481 ms, put_time: 0.0068 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,110] LMCache INFO:[0m Reqid: cmpl-benchmark-serving48-0, Total tokens 273, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60824 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,256] LMCache INFO:[0m Storing KV cache for 273 out of 273 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving48-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,257] LMCache INFO:[0m Stored 273 out of total 273 tokens. size: 0.0250 gb, cost 0.8349 ms, throughput: 29.9365 GB/s; offload_time: 0.8269 ms, put_time: 0.0080 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,259] LMCache INFO:[0m Reqid: cmpl-benchmark-serving49-0, Total tokens 5, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,355] LMCache INFO:[0m Storing KV cache for 5 out of 5 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving49-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,355] LMCache INFO:[0m Stored 5 out of total 5 tokens. size: 0.0005 gb, cost 0.0979 ms, throughput: 4.6776 GB/s; offload_time: 0.0917 ms, put_time: 0.0062 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60852 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:17,981] LMCache INFO:[0m Reqid: cmpl-benchmark-serving50-0, Total tokens 48, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:18,082] LMCache INFO:[0m Storing KV cache for 48 out of 48 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving50-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:18,083] LMCache INFO:[0m Stored 48 out of total 48 tokens. size: 0.0044 gb, cost 0.2654 ms, throughput: 16.5576 GB/s; offload_time: 0.2566 ms, put_time: 0.0088 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60800 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:18,266] LMCache INFO:[0m Reqid: cmpl-benchmark-serving51-0, Total tokens 158, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:18,384] LMCache INFO:[0m Storing KV cache for 158 out of 158 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving51-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:18,385] LMCache INFO:[0m Stored 158 out of total 158 tokens. size: 0.0145 gb, cost 0.5361 ms, throughput: 26.9837 GB/s; offload_time: 0.5279 ms, put_time: 0.0082 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60636 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:18,919] LMCache INFO:[0m Reqid: cmpl-benchmark-serving52-0, Total tokens 306, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:19,067] LMCache INFO:[0m Storing KV cache for 306 out of 306 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving52-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:19,068] LMCache INFO:[0m Stored 306 out of total 306 tokens. size: 0.0280 gb, cost 0.9646 ms, throughput: 29.0431 GB/s; offload_time: 0.9543 ms, put_time: 0.0103 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60660 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:19,424] LMCache INFO:[0m Reqid: cmpl-benchmark-serving53-0, Total tokens 772, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60690 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:19,697] LMCache INFO:[0m Storing KV cache for 772 out of 772 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving53-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:19,699] LMCache INFO:[0m Stored 772 out of total 772 tokens. size: 0.0707 gb, cost 2.2567 ms, throughput: 31.3201 GB/s; offload_time: 2.2361 ms, put_time: 0.0206 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:19,701] LMCache INFO:[0m Reqid: cmpl-benchmark-serving54-0, Total tokens 315, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:19,851] LMCache INFO:[0m Storing KV cache for 315 out of 315 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving54-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:19,852] LMCache INFO:[0m Stored 315 out of total 315 tokens. size: 0.0288 gb, cost 0.9697 ms, throughput: 29.7392 GB/s; offload_time: 0.9609 ms, put_time: 0.0088 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:40848 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60766 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,127] LMCache INFO:[0m Reqid: cmpl-benchmark-serving55-0, Total tokens 20, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,127] LMCache INFO:[0m Reqid: cmpl-benchmark-serving56-0, Total tokens 75, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60704 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60636 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,244] LMCache INFO:[0m Storing KV cache for 20 out of 20 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving55-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,244] LMCache INFO:[0m Stored 20 out of total 20 tokens. size: 0.0018 gb, cost 0.1774 ms, throughput: 10.3223 GB/s; offload_time: 0.1693 ms, put_time: 0.0080 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,244] LMCache INFO:[0m Storing KV cache for 75 out of 75 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving56-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,245] LMCache INFO:[0m Stored 75 out of total 75 tokens. size: 0.0069 gb, cost 0.2714 ms, throughput: 25.3036 GB/s; offload_time: 0.2659 ms, put_time: 0.0055 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,246] LMCache INFO:[0m Reqid: cmpl-benchmark-serving57-0, Total tokens 85, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,247] LMCache INFO:[0m Reqid: cmpl-benchmark-serving58-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53416 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53426 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,363] LMCache INFO:[0m Storing KV cache for 85 out of 85 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving57-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,363] LMCache INFO:[0m Stored 85 out of total 85 tokens. size: 0.0078 gb, cost 0.3251 ms, throughput: 23.9337 GB/s; offload_time: 0.3181 ms, put_time: 0.0070 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,363] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving58-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,363] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.0924 ms, throughput: 10.8984 GB/s; offload_time: 0.0875 ms, put_time: 0.0049 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,365] LMCache INFO:[0m Reqid: cmpl-benchmark-serving59-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,366] LMCache INFO:[0m Reqid: cmpl-benchmark-serving60-0, Total tokens 335, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53432 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,542] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving59-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,542] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1517 ms, throughput: 10.2584 GB/s; offload_time: 0.1441 ms, put_time: 0.0076 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,542] LMCache INFO:[0m Storing KV cache for 335 out of 335 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving60-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,543] LMCache INFO:[0m Stored 335 out of total 335 tokens. size: 0.0307 gb, cost 0.9906 ms, throughput: 30.9617 GB/s; offload_time: 0.9835 ms, put_time: 0.0071 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,545] LMCache INFO:[0m Reqid: cmpl-benchmark-serving61-0, Total tokens 187, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,665] LMCache INFO:[0m Storing KV cache for 187 out of 187 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving61-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,666] LMCache INFO:[0m Stored 187 out of total 187 tokens. size: 0.0171 gb, cost 0.6084 ms, throughput: 28.1378 GB/s; offload_time: 0.6009 ms, put_time: 0.0076 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53434 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,856] LMCache INFO:[0m Reqid: cmpl-benchmark-serving62-0, Total tokens 164, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60612 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,977] LMCache INFO:[0m Storing KV cache for 164 out of 164 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving62-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,978] LMCache INFO:[0m Stored 164 out of total 164 tokens. size: 0.0150 gb, cost 0.5618 ms, throughput: 26.7243 GB/s; offload_time: 0.5537 ms, put_time: 0.0082 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:20,980] LMCache INFO:[0m Reqid: cmpl-benchmark-serving63-0, Total tokens 14, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53442 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,083] LMCache INFO:[0m Storing KV cache for 14 out of 14 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving63-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,083] LMCache INFO:[0m Stored 14 out of total 14 tokens. size: 0.0013 gb, cost 0.1455 ms, throughput: 8.8077 GB/s; offload_time: 0.1384 ms, put_time: 0.0071 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,086] LMCache INFO:[0m Reqid: cmpl-benchmark-serving64-0, Total tokens 548, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60824 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53456 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,315] LMCache INFO:[0m Storing KV cache for 548 out of 548 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving64-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,317] LMCache INFO:[0m Stored 548 out of total 548 tokens. size: 0.0502 gb, cost 1.5903 ms, throughput: 31.5485 GB/s; offload_time: 1.5810 ms, put_time: 0.0093 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,319] LMCache INFO:[0m Reqid: cmpl-benchmark-serving65-0, Total tokens 691, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,321] LMCache INFO:[0m Reqid: cmpl-benchmark-serving66-0, Total tokens 135, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53470 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53480 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,609] LMCache INFO:[0m Storing KV cache for 691 out of 691 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving65-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,611] LMCache INFO:[0m Stored 691 out of total 691 tokens. size: 0.0633 gb, cost 1.9914 ms, throughput: 31.7678 GB/s; offload_time: 1.9816 ms, put_time: 0.0098 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,611] LMCache INFO:[0m Storing KV cache for 135 out of 135 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving66-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,612] LMCache INFO:[0m Stored 135 out of total 135 tokens. size: 0.0124 gb, cost 0.4247 ms, throughput: 29.1023 GB/s; offload_time: 0.4199 ms, put_time: 0.0048 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,614] LMCache INFO:[0m Reqid: cmpl-benchmark-serving67-0, Total tokens 256, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,615] LMCache INFO:[0m Reqid: cmpl-benchmark-serving68-0, Total tokens 22, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,765] LMCache INFO:[0m Storing KV cache for 256 out of 256 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving67-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,766] LMCache INFO:[0m Stored 256 out of total 256 tokens. size: 0.0234 gb, cost 0.7923 ms, throughput: 29.5812 GB/s; offload_time: 0.7830 ms, put_time: 0.0093 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,766] LMCache INFO:[0m Storing KV cache for 22 out of 22 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving68-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:21,766] LMCache INFO:[0m Stored 22 out of total 22 tokens. size: 0.0020 gb, cost 0.1309 ms, throughput: 15.3903 GB/s; offload_time: 0.1257 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60836 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:22,525] LMCache INFO:[0m Reqid: cmpl-benchmark-serving69-0, Total tokens 27, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:22,628] LMCache INFO:[0m Storing KV cache for 27 out of 27 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving69-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:22,628] LMCache INFO:[0m Stored 27 out of total 27 tokens. size: 0.0025 gb, cost 0.2163 ms, throughput: 11.4296 GB/s; offload_time: 0.2072 ms, put_time: 0.0091 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53494 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53504 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53508 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:22,912] LMCache INFO:[0m Reqid: cmpl-benchmark-serving70-0, Total tokens 268, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:22,913] LMCache INFO:[0m Reqid: cmpl-benchmark-serving71-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:22,914] LMCache INFO:[0m Reqid: cmpl-benchmark-serving72-0, Total tokens 4, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,070] LMCache INFO:[0m Storing KV cache for 268 out of 268 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving70-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,071] LMCache INFO:[0m Stored 268 out of total 268 tokens. size: 0.0245 gb, cost 0.8848 ms, throughput: 27.7310 GB/s; offload_time: 0.8743 ms, put_time: 0.0105 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,071] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving71-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,071] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.0969 ms, throughput: 10.3912 GB/s; offload_time: 0.0918 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,071] LMCache INFO:[0m Storing KV cache for 4 out of 4 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving72-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,071] LMCache INFO:[0m Stored 4 out of total 4 tokens. size: 0.0004 gb, cost 0.0665 ms, throughput: 5.5060 GB/s; offload_time: 0.0620 ms, put_time: 0.0045 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53510 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,262] LMCache INFO:[0m Reqid: cmpl-benchmark-serving73-0, Total tokens 20, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,366] LMCache INFO:[0m Storing KV cache for 20 out of 20 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving73-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,366] LMCache INFO:[0m Stored 20 out of total 20 tokens. size: 0.0018 gb, cost 0.1700 ms, throughput: 10.7684 GB/s; offload_time: 0.1620 ms, put_time: 0.0081 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53526 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,560] LMCache INFO:[0m Reqid: cmpl-benchmark-serving74-0, Total tokens 343, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,741] LMCache INFO:[0m Storing KV cache for 343 out of 343 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving74-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,742] LMCache INFO:[0m Stored 343 out of total 343 tokens. size: 0.0314 gb, cost 1.0555 ms, throughput: 29.7512 GB/s; offload_time: 1.0459 ms, put_time: 0.0096 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53528 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,843] LMCache INFO:[0m Reqid: cmpl-benchmark-serving75-0, Total tokens 21, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,948] LMCache INFO:[0m Storing KV cache for 21 out of 21 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving75-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:23,948] LMCache INFO:[0m Stored 21 out of total 21 tokens. size: 0.0019 gb, cost 0.1750 ms, throughput: 10.9845 GB/s; offload_time: 0.1674 ms, put_time: 0.0076 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53538 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53550 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,150] LMCache INFO:[0m Reqid: cmpl-benchmark-serving76-0, Total tokens 38, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,151] LMCache INFO:[0m Reqid: cmpl-benchmark-serving77-0, Total tokens 289, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53552 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53562 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,314] LMCache INFO:[0m Storing KV cache for 38 out of 38 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving76-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,314] LMCache INFO:[0m Stored 38 out of total 38 tokens. size: 0.0035 gb, cost 0.2227 ms, throughput: 15.6194 GB/s; offload_time: 0.2144 ms, put_time: 0.0084 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,314] LMCache INFO:[0m Storing KV cache for 289 out of 289 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving77-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,315] LMCache INFO:[0m Stored 289 out of total 289 tokens. size: 0.0265 gb, cost 0.8622 ms, throughput: 30.6884 GB/s; offload_time: 0.8542 ms, put_time: 0.0080 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,317] LMCache INFO:[0m Reqid: cmpl-benchmark-serving78-0, Total tokens 289, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,319] LMCache INFO:[0m Reqid: cmpl-benchmark-serving79-0, Total tokens 63, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,507] LMCache INFO:[0m Storing KV cache for 289 out of 289 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving78-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,508] LMCache INFO:[0m Stored 289 out of total 289 tokens. size: 0.0265 gb, cost 0.8978 ms, throughput: 29.4720 GB/s; offload_time: 0.8894 ms, put_time: 0.0083 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,509] LMCache INFO:[0m Storing KV cache for 63 out of 63 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving79-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,509] LMCache INFO:[0m Stored 63 out of total 63 tokens. size: 0.0058 gb, cost 0.2300 ms, throughput: 25.0743 GB/s; offload_time: 0.2253 ms, put_time: 0.0048 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53570 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,713] LMCache INFO:[0m Reqid: cmpl-benchmark-serving80-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,820] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving80-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,821] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1571 ms, throughput: 9.9099 GB/s; offload_time: 0.1499 ms, put_time: 0.0072 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60742 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:24,921] LMCache INFO:[0m Reqid: cmpl-benchmark-serving81-0, Total tokens 14, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60660 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53572 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,029] LMCache INFO:[0m Storing KV cache for 14 out of 14 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving81-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,030] LMCache INFO:[0m Stored 14 out of total 14 tokens. size: 0.0013 gb, cost 0.1617 ms, throughput: 7.9268 GB/s; offload_time: 0.1533 ms, put_time: 0.0084 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,032] LMCache INFO:[0m Reqid: cmpl-benchmark-serving82-0, Total tokens 24, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,033] LMCache INFO:[0m Reqid: cmpl-benchmark-serving83-0, Total tokens 13, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60690 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,145] LMCache INFO:[0m Storing KV cache for 24 out of 24 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving82-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,146] LMCache INFO:[0m Stored 24 out of total 24 tokens. size: 0.0022 gb, cost 0.1724 ms, throughput: 12.7458 GB/s; offload_time: 0.1647 ms, put_time: 0.0076 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,146] LMCache INFO:[0m Storing KV cache for 13 out of 13 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving83-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,146] LMCache INFO:[0m Stored 13 out of total 13 tokens. size: 0.0012 gb, cost 0.0988 ms, throughput: 12.0414 GB/s; offload_time: 0.0939 ms, put_time: 0.0049 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,148] LMCache INFO:[0m Reqid: cmpl-benchmark-serving84-0, Total tokens 666, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60824 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,417] LMCache INFO:[0m Storing KV cache for 666 out of 666 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving84-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,419] LMCache INFO:[0m Stored 666 out of total 666 tokens. size: 0.0610 gb, cost 1.9196 ms, throughput: 31.7642 GB/s; offload_time: 1.9103 ms, put_time: 0.0093 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,421] LMCache INFO:[0m Reqid: cmpl-benchmark-serving85-0, Total tokens 501, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53578 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,645] LMCache INFO:[0m Storing KV cache for 501 out of 501 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving85-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,647] LMCache INFO:[0m Stored 501 out of total 501 tokens. size: 0.0459 gb, cost 1.4642 ms, throughput: 31.3267 GB/s; offload_time: 1.4557 ms, put_time: 0.0085 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,649] LMCache INFO:[0m Reqid: cmpl-benchmark-serving86-0, Total tokens 70, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53586 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,776] LMCache INFO:[0m Storing KV cache for 70 out of 70 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving86-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,776] LMCache INFO:[0m Stored 70 out of total 70 tokens. size: 0.0064 gb, cost 0.2855 ms, throughput: 22.4449 GB/s; offload_time: 0.2787 ms, put_time: 0.0069 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,778] LMCache INFO:[0m Reqid: cmpl-benchmark-serving87-0, Total tokens 262, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,933] LMCache INFO:[0m Storing KV cache for 262 out of 262 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving87-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:25,934] LMCache INFO:[0m Stored 262 out of total 262 tokens. size: 0.0240 gb, cost 0.8110 ms, throughput: 29.5779 GB/s; offload_time: 0.8027 ms, put_time: 0.0083 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53588 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,246] LMCache INFO:[0m Reqid: cmpl-benchmark-serving88-0, Total tokens 38, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53494 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53598 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,358] LMCache INFO:[0m Storing KV cache for 38 out of 38 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving88-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,359] LMCache INFO:[0m Stored 38 out of total 38 tokens. size: 0.0035 gb, cost 0.2733 ms, throughput: 12.7293 GB/s; offload_time: 0.2609 ms, put_time: 0.0124 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,361] LMCache INFO:[0m Reqid: cmpl-benchmark-serving89-0, Total tokens 233, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,361] LMCache INFO:[0m Reqid: cmpl-benchmark-serving90-0, Total tokens 197, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:25:26 [loggers.py:236] Engine 000: Avg prompt throughput: 791.9 tokens/s, Avg generation throughput: 388.5 tokens/s, Running: 62 reqs, Waiting: 0 reqs, GPU KV cache usage: 70.7%, Prefix cache hit rate: 3.6%, External prefix cache hit rate: 0.0%
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,569] LMCache INFO:[0m Storing KV cache for 233 out of 233 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving89-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,569] LMCache INFO:[0m Stored 233 out of total 233 tokens. size: 0.0213 gb, cost 0.7435 ms, throughput: 28.6910 GB/s; offload_time: 0.7356 ms, put_time: 0.0079 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,570] LMCache INFO:[0m Storing KV cache for 197 out of 197 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving90-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,570] LMCache INFO:[0m Stored 197 out of total 197 tokens. size: 0.0180 gb, cost 0.5937 ms, throughput: 30.3765 GB/s; offload_time: 0.5887 ms, put_time: 0.0051 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53426 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,674] LMCache INFO:[0m Reqid: cmpl-benchmark-serving91-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,783] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving91-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,784] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.1829 ms, throughput: 5.5073 GB/s; offload_time: 0.1722 ms, put_time: 0.0107 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60824 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:26,991] LMCache INFO:[0m Reqid: cmpl-benchmark-serving92-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,101] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving92-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,102] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.1784 ms, throughput: 5.6460 GB/s; offload_time: 0.1670 ms, put_time: 0.0113 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:40848 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,206] LMCache INFO:[0m Reqid: cmpl-benchmark-serving93-0, Total tokens 9, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,312] LMCache INFO:[0m Storing KV cache for 9 out of 9 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving93-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,312] LMCache INFO:[0m Stored 9 out of total 9 tokens. size: 0.0008 gb, cost 0.1351 ms, throughput: 6.1007 GB/s; offload_time: 0.1274 ms, put_time: 0.0076 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53614 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,517] LMCache INFO:[0m Reqid: cmpl-benchmark-serving94-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53586 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,628] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving94-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,628] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1650 ms, throughput: 9.4350 GB/s; offload_time: 0.1561 ms, put_time: 0.0089 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,630] LMCache INFO:[0m Reqid: cmpl-benchmark-serving95-0, Total tokens 299, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53630 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,794] LMCache INFO:[0m Storing KV cache for 299 out of 299 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving95-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,795] LMCache INFO:[0m Stored 299 out of total 299 tokens. size: 0.0274 gb, cost 0.9331 ms, throughput: 29.3353 GB/s; offload_time: 0.9232 ms, put_time: 0.0099 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,797] LMCache INFO:[0m Reqid: cmpl-benchmark-serving96-0, Total tokens 78, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:60880 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53636 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=485000)[0;0m INFO:     127.0.0.1:53638 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,918] LMCache INFO:[0m Storing KV cache for 78 out of 78 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving96-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,919] LMCache INFO:[0m Stored 78 out of total 78 tokens. size: 0.0071 gb, cost 0.3175 ms, throughput: 22.4946 GB/s; offload_time: 0.3099 ms, put_time: 0.0075 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,920] LMCache INFO:[0m Reqid: cmpl-benchmark-serving97-0, Total tokens 583, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,921] LMCache INFO:[0m Reqid: cmpl-benchmark-serving98-0, Total tokens 454, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:27,922] LMCache INFO:[0m Reqid: cmpl-benchmark-serving99-0, Total tokens 239, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:28,365] LMCache INFO:[0m Storing KV cache for 583 out of 583 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving97-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:28,367] LMCache INFO:[0m Stored 583 out of total 583 tokens. size: 0.0534 gb, cost 1.7198 ms, throughput: 31.0361 GB/s; offload_time: 1.7087 ms, put_time: 0.0111 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:28,367] LMCache INFO:[0m Storing KV cache for 454 out of 454 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving98-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:28,369] LMCache INFO:[0m Stored 454 out of total 454 tokens. size: 0.0416 gb, cost 1.2918 ms, throughput: 32.1770 GB/s; offload_time: 1.2854 ms, put_time: 0.0064 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:28,369] LMCache INFO:[0m Storing KV cache for 239 out of 239 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving99-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=485122)[0;0m [32;20m[2025-12-08 02:25:28,369] LMCache INFO:[0m Stored 239 out of total 239 tokens. size: 0.0219 gb, cost 0.6973 ms, throughput: 31.3790 GB/s; offload_time: 0.6928 ms, put_time: 0.0045 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:25:36 [loggers.py:236] Engine 000: Avg prompt throughput: 213.1 tokens/s, Avg generation throughput: 549.1 tokens/s, Running: 52 reqs, Waiting: 0 reqs, GPU KV cache usage: 76.6%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:25:46 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 471.0 tokens/s, Running: 42 reqs, Waiting: 0 reqs, GPU KV cache usage: 86.0%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:25:56 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 371.6 tokens/s, Running: 25 reqs, Waiting: 0 reqs, GPU KV cache usage: 64.7%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:26:06 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 279.1 tokens/s, Running: 17 reqs, Waiting: 0 reqs, GPU KV cache usage: 58.2%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:26:16 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 201.4 tokens/s, Running: 9 reqs, Waiting: 0 reqs, GPU KV cache usage: 41.5%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:26:26 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 104.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 7.4%, Prefix cache hit rate: 3.2%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=485000)[0;0m INFO 12-08 02:26:28 [launcher.py:110] Shutting down FastAPI HTTP server.
[0;36m(APIServer pid=485000)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=485000)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=485000)[0;0m INFO:     Application shutdown complete.
