INFO 12-08 02:28:45 [uvm.py:115] Loading UVM allocator library from: /home/yunwei37/workspace/vllm/vllm/uvm_allocator.abi3.so
INFO 12-08 02:28:45 [uvm.py:200] UVM allocator enabled successfully
INFO 12-08 02:28:45 [uvm.py:206] Pre-initializing cuBLAS with UVM...
INFO 12-08 02:28:46 [uvm.py:213] cuBLAS pre-initialized successfully with UVM
WARNING 12-08 02:28:46 [env_override.py:38] UVM allocator is enabled. This allows memory oversubscription but has significant performance overhead. NOT recommended for production use.
INFO 12-08 02:28:46 [__init__.py:216] Automatically detected platform cuda.
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:28:48 [api_server.py:1839] vLLM API server version 0.11.0rc2.dev37+g0efd540db.d20251126
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:28:48 [utils.py:233] non-default args: {'model_tag': 'Qwen/Qwen3-30B-A3B-FP8', 'model': 'Qwen/Qwen3-30B-A3B-FP8', 'enforce_eager': True}
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:28:51 [model.py:552] Resolved architecture: Qwen3MoeForCausalLM
[1;36m(APIServer pid=486967)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:28:51 [model.py:1515] Using max model len 40960
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:28:51 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=2048.
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:28:51 [__init__.py:382] Cudagraph is disabled under eager mode
INFO 12-08 02:28:53 [uvm.py:115] Loading UVM allocator library from: /home/yunwei37/workspace/vllm/vllm/uvm_allocator.abi3.so
INFO 12-08 02:28:53 [uvm.py:200] UVM allocator enabled successfully
INFO 12-08 02:28:53 [uvm.py:206] Pre-initializing cuBLAS with UVM...
INFO 12-08 02:28:53 [uvm.py:213] cuBLAS pre-initialized successfully with UVM
WARNING 12-08 02:28:53 [env_override.py:38] UVM allocator is enabled. This allows memory oversubscription but has significant performance overhead. NOT recommended for production use.
INFO 12-08 02:28:53 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:28:55 [core.py:648] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:28:55 [core.py:78] Initializing a V1 LLM engine (v0.11.0rc2.dev37+g0efd540db.d20251126) with config: model='Qwen/Qwen3-30B-A3B-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-FP8, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":null,"cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":false,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"use_inductor_graph_partition":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:28:55 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=487172)[0;0m WARNING 12-08 02:28:55 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:28:55 [gpu_model_runner.py:2679] Starting to load model Qwen/Qwen3-30B-A3B-FP8...
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:28:55 [gpu_model_runner.py:2711] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:28:55 [cuda.py:367] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=487172)[0;0m WARNING 12-08 02:28:55 [fp8.py:457] Failed to import DeepGemm kernels.
[1;36m(EngineCore_DP0 pid=487172)[0;0m WARNING 12-08 02:28:55 [fp8.py:480] CutlassBlockScaledGroupedGemm not supported on the current platform.
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:28:56 [weight_utils.py:392] Using model weights format ['*.safetensors']
[1;36m(EngineCore_DP0 pid=487172)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=487172)[0;0m Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:04,  1.41it/s]
[1;36m(EngineCore_DP0 pid=487172)[0;0m Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:01<00:03,  1.31it/s]
[1;36m(EngineCore_DP0 pid=487172)[0;0m Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:02<00:03,  1.27it/s]
[1;36m(EngineCore_DP0 pid=487172)[0;0m Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:02<00:01,  1.64it/s]
[1;36m(EngineCore_DP0 pid=487172)[0;0m Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:03<00:01,  1.52it/s]
[1;36m(EngineCore_DP0 pid=487172)[0;0m Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:04<00:00,  1.45it/s]
[1;36m(EngineCore_DP0 pid=487172)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:04<00:00,  1.41it/s]
[1;36m(EngineCore_DP0 pid=487172)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:04<00:00,  1.43it/s]
[1;36m(EngineCore_DP0 pid=487172)[0;0m 
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:29:01 [default_loader.py:267] Loading weights took 4.94 seconds
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:29:01 [gpu_model_runner.py:2730] Model loading took 29.0433 GiB and 5.441330 seconds
[1;36m(EngineCore_DP0 pid=487172)[0;0m UVM enabled: setting KV cache size to 6.0 GB
[1;36m(EngineCore_DP0 pid=487172)[0;0m WARNING 12-08 02:29:01 [kv_cache_utils.py:704] UVM is enabled - skipping KV cache memory check. Memory oversubscription is allowed but may cause slowdown.
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:29:01 [kv_cache_utils.py:1121] GPU KV cache size: 65,536 tokens
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:29:01 [kv_cache_utils.py:1125] Maximum concurrency for 40,960 tokens per request: 1.60x
[1;36m(EngineCore_DP0 pid=487172)[0;0m WARNING 12-08 02:29:01 [cudagraph_dispatcher.py:105] cudagraph dispatching keys are not initialized. No cudagraph will be used.
[1;36m(EngineCore_DP0 pid=487172)[0;0m WARNING 12-08 02:29:02 [fused_moe.py:798] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/yunwei37/workspace/vllm/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_GeForce_RTX_5090,dtype=fp8_w8a8,block_shape=[128,128].json']
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:29:02 [core.py:211] init engine (profile, create kv cache, warmup model) took 1.32 seconds
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:29:03 [__init__.py:382] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=487172)[0;0m INFO 12-08 02:29:03 [gc_utils.py:41] GC Debug Config. enabled:False,top_objects:-1
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 4096
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=486967)[0;0m WARNING 12-08 02:29:03 [model.py:1394] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [serving_responses.py:137] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8000
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /openapi.json, Methods: GET, HEAD
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /docs, Methods: GET, HEAD
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /redoc, Methods: GET, HEAD
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:03 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=486967)[0;0m INFO:     Started server process [486967]
[1;36m(APIServer pid=486967)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=486967)[0;0m INFO:     Application startup complete.
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55082 - "GET /health HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46006 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:14 [loggers.py:127] Engine 000: Avg prompt throughput: 69.9 tokens/s, Avg generation throughput: 7.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46006 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46016 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46024 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46040 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46046 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46052 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46056 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46072 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46080 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38366 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38372 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38388 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38366 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38394 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38400 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38410 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38422 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38434 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38450 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38456 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38470 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38474 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38490 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38500 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38510 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38520 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38530 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38540 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38550 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38556 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38568 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38576 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38584 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38500 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38576 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38450 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38366 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38474 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38422 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38530 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38598 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38606 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38608 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:24 [loggers.py:127] Engine 000: Avg prompt throughput: 1056.6 tokens/s, Avg generation throughput: 133.0 tokens/s, Running: 32 reqs, Waiting: 0 reqs, GPU KV cache usage: 13.5%, Prefix cache hit rate: 6.4%
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38556 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38410 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38598 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38450 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38608 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38366 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38616 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38520 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38456 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38366 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46006 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38628 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38422 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38366 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55366 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55372 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55380 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46056 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38616 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55384 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55392 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55396 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55404 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55410 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55422 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55438 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55440 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55448 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55460 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55472 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:38500 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55516 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55522 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55538 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55542 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55554 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55566 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55574 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55586 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55590 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55606 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55610 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:34 [loggers.py:127] Engine 000: Avg prompt throughput: 856.8 tokens/s, Avg generation throughput: 342.4 tokens/s, Running: 64 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.0%, Prefix cache hit rate: 3.6%
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55614 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55628 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55642 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55648 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55662 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55670 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55404 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55448 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55672 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:46006 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55682 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO:     127.0.0.1:55686 - "POST /v1/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:44 [loggers.py:127] Engine 000: Avg prompt throughput: 243.1 tokens/s, Avg generation throughput: 284.9 tokens/s, Running: 61 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.6%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:29:54 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 255.6 tokens/s, Running: 53 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.5%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:30:04 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 206.7 tokens/s, Running: 48 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.7%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:30:14 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 189.4 tokens/s, Running: 45 reqs, Waiting: 0 reqs, GPU KV cache usage: 24.3%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:30:24 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 173.3 tokens/s, Running: 38 reqs, Waiting: 0 reqs, GPU KV cache usage: 23.5%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:30:34 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 152.9 tokens/s, Running: 33 reqs, Waiting: 0 reqs, GPU KV cache usage: 22.5%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:30:44 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 134.5 tokens/s, Running: 26 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.0%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:30:54 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 140.0 tokens/s, Running: 24 reqs, Waiting: 0 reqs, GPU KV cache usage: 19.8%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:31:04 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 141.0 tokens/s, Running: 19 reqs, Waiting: 0 reqs, GPU KV cache usage: 18.3%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:31:14 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 124.0 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 14.9%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:31:24 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 106.6 tokens/s, Running: 9 reqs, Waiting: 0 reqs, GPU KV cache usage: 12.0%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:31:34 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 93.8 tokens/s, Running: 2 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.4%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:31:44 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 27.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 3.2%
[1;36m(APIServer pid=486967)[0;0m INFO 12-08 02:31:46 [launcher.py:99] Shutting down FastAPI HTTP server.
[1;36m(APIServer pid=486967)[0;0m INFO:     Shutting down
[1;36m(APIServer pid=486967)[0;0m INFO:     Waiting for application shutdown.
[1;36m(APIServer pid=486967)[0;0m INFO:     Application shutdown complete.
