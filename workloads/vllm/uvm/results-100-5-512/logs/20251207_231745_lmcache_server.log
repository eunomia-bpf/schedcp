[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:38 [api_server.py:1772] vLLM API server version 0.12.0
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:38 [utils.py:253] non-default args: {'model_tag': 'Qwen/Qwen3-30B-A3B-FP8', 'model': 'Qwen/Qwen3-30B-A3B-FP8', 'max_model_len': 2048, 'enforce_eager': True, 'cpu_offload_gb': 4.0, 'kv_transfer_config': KVTransferConfig(kv_connector='LMCacheConnectorV1', engine_id='3fb66999-b0ae-45a5-96f3-33396372fd8d', kv_buffer_device='cuda', kv_buffer_size=1000000000.0, kv_role='kv_both', kv_rank=None, kv_parallel_size=1, kv_ip='127.0.0.1', kv_port=14579, kv_connector_extra_config={}, kv_connector_module_path=None, enable_permute_local_kv=False)}
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:42 [model.py:637] Resolved architecture: Qwen3MoeForCausalLM
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:42 [model.py:1750] Using max model len 2048
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:42 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=2048.
[0;36m(APIServer pid=398063)[0;0m WARNING 12-07 23:27:42 [vllm.py:601] Enforce eager set, overriding optimization level to -O0
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:42 [vllm.py:707] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=398063)[0;0m WARNING 12-07 23:27:42 [vllm.py:896] Turning off hybrid kv cache manager because `--kv-transfer-config` is set. This will reduce the performance of vLLM on LLMs with sliding window attention or Mamba attention. If you are a developer of kv connector, please consider supporting hybrid kv cache manager for your connector by making sure your connector is a subclass of `SupportsHMA` defined in kv_connector/v1/base.py.
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:46 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='Qwen/Qwen3-30B-A3B-FP8', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=fp8, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-FP8, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['+quant_fp8', 'all', '+quant_fp8'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': False, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 0, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:46 [parallel_state.py:1200] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://128.114.59.195:37839 backend=nccl
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:46 [parallel_state.py:1408] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:46 [gpu_model_runner.py:3467] Starting to load model Qwen/Qwen3-30B-A3B-FP8...
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:46 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:46 [layer.py:379] Enabled separate cuda stream for MoE shared_experts
[0;36m(EngineCore_DP0 pid=398219)[0;0m WARNING 12-07 23:27:46 [fp8.py:183] DeepGEMM backend requested but not available.
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:46 [fp8.py:202] Using Triton backend for FP8 MoE
[0;36m(EngineCore_DP0 pid=398219)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/7 [00:00<?, ?it/s]
[0;36m(EngineCore_DP0 pid=398219)[0;0m Loading safetensors checkpoint shards:  14% Completed | 1/7 [00:00<00:03,  1.98it/s]
[0;36m(EngineCore_DP0 pid=398219)[0;0m Loading safetensors checkpoint shards:  29% Completed | 2/7 [00:01<00:02,  1.70it/s]
[0;36m(EngineCore_DP0 pid=398219)[0;0m Loading safetensors checkpoint shards:  43% Completed | 3/7 [00:01<00:02,  1.62it/s]
[0;36m(EngineCore_DP0 pid=398219)[0;0m Loading safetensors checkpoint shards:  57% Completed | 4/7 [00:02<00:01,  2.05it/s]
[0;36m(EngineCore_DP0 pid=398219)[0;0m Loading safetensors checkpoint shards:  71% Completed | 5/7 [00:02<00:01,  1.85it/s]
[0;36m(EngineCore_DP0 pid=398219)[0;0m Loading safetensors checkpoint shards:  86% Completed | 6/7 [00:03<00:00,  1.72it/s]
[0;36m(EngineCore_DP0 pid=398219)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:04<00:00,  1.66it/s]
[0;36m(EngineCore_DP0 pid=398219)[0;0m Loading safetensors checkpoint shards: 100% Completed | 7/7 [00:04<00:00,  1.74it/s]
[0;36m(EngineCore_DP0 pid=398219)[0;0m 
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:52 [default_loader.py:308] Loading weights took 4.06 seconds
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:52 [gpu_model_runner.py:3549] Model loading took 24.9801 GiB memory and 5.825632 seconds
[0;36m(EngineCore_DP0 pid=398219)[0;0m WARNING 12-07 23:27:53 [fused_moe.py:888] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=768,device_name=NVIDIA_GeForce_RTX_5090,dtype=fp8_w8a8,block_shape=[128,128].json']
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:54 [gpu_worker.py:359] Available KV cache memory: 1.76 GiB
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:54 [kv_cache_utils.py:1286] GPU KV cache size: 19,168 tokens
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:54 [kv_cache_utils.py:1291] Maximum concurrency for 2,048 tokens per request: 9.36x
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:54 [factory.py:64] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 3fb66999-b0ae-45a5-96f3-33396372fd8d
[0;36m(EngineCore_DP0 pid=398219)[0;0m WARNING 12-07 23:27:54 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:54 [lmcache_connector.py:52] Initializing latest dev LMCache connector
[0;36m(EngineCore_DP0 pid=398219)[0;0m [33;20m[2025-12-07 23:27:54,573] LMCache WARNING:[0m No LMCache configuration file is set. Trying to read configurations from the environment variables. [3m(utils.py:61:lmcache.integration.vllm.utils)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [33;20m[2025-12-07 23:27:54,573] LMCache WARNING:[0m You can set the configuration file through the environment variable: LMCACHE_CONFIG_FILE [3m(utils.py:65:lmcache.integration.vllm.utils)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,573] LMCache INFO:[0m use mla: False, kv shape: (48, 2, 256, 4, 128), num_draft_layers:0 [3m(vllm_v1_adapter.py:503:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,573] LMCache INFO:[0m CUDA device is available. Using CUDA for LMCache engine. [3m(vllm_v1_adapter.py:509:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,574] LMCache INFO:[0m Creating LMCacheEngine instance vllm-instance [3m(cache_engine.py:1531:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,574] LMCache INFO:[0m NUMA mapping for instance vllm-instance: None [3m(cache_engine.py:1534:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [33;20m[2025-12-07 23:27:54,574] LMCache WARNING:[0m Could not load 'builtin' from vLLM. Using builtin hash. This may cause inconsistencies in distributed caching. [3m(token_database.py:136:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [33;20m[2025-12-07 23:27:54,574] LMCache WARNING:[0m Using builtin hash without PYTHONHASHSEED set. For production environments (non-testing scenarios), you MUST set PYTHONHASHSEED to ensure consistent hashing across processes. Example: export PYTHONHASHSEED=0 [3m(token_database.py:143:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,574] LMCache INFO:[0m Initialized NONE_HASH=b'\xe1\xa2E\xda}\xa4\xb1\xc5=\x8f\xb3\x88\xf6\x86\x1b0|\xcf\xef\xad~"\xb7\x0c\xc1\xd7\xabc\x02\xa3\xf2\xf3' from vLLM (>= PR#20511) [3m(token_database.py:74:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,574] LMCache INFO:[0m Using hash algorithm: builtin [3m(token_database.py:84:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,574] LMCache INFO:[0m sending cache usage stats to http://stats.lmcache.ai:8080/cache-usage [3m(usage_context.py:290:lmcache.usage_context)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,578] LMCache INFO:[0m Creating LMCacheEngine with config: {'chunk_size': 256, 'local_cpu': True, 'max_local_cpu_size': 8.0, 'reserve_local_cpu_size': 0.0, 'local_disk': None, 'max_local_disk_size': 0.0, 'remote_url': None, 'remote_serde': 'naive', 'use_layerwise': False, 'save_decode_cache': False, 'pre_caching_hash_algorithm': 'builtin', 'enable_blending': False, 'blend_recompute_ratios': None, 'blend_thresholds': None, 'blend_check_layers': None, 'blend_min_tokens': 256, 'blend_special_str': ' # # ', 'enable_p2p': False, 'p2p_host': None, 'p2p_init_ports': None, 'p2p_lookup_ports': None, 'enable_controller': False, 'lmcache_instance_id': 'lmcache_instance_2553b10a5c8447a391a50f58659a0a73', 'controller_pull_url': None, 'controller_reply_url': None, 'lmcache_worker_ports': None, 'lmcache_worker_ids': None, 'lmcache_worker_heartbeat_delay_time': 10, 'lmcache_worker_heartbeat_time': None, 'enable_pd': False, 'pd_role': None, 'pd_buffer_size': None, 'pd_buffer_device': None, 'pd_peer_host': None, 'pd_peer_init_port': None, 'pd_peer_alloc_port': None, 'pd_proxy_host': None, 'pd_proxy_port': None, 'transfer_channel': None, 'nixl_backends': None, 'nixl_buffer_size': None, 'nixl_buffer_device': None, 'weka_path': None, 'gds_path': None, 'cufile_buffer_size': None, 'audit_actual_remote_url': None, 'internal_api_server_host': '0.0.0.0', 'extra_config': None, 'save_unfull_chunk': True, 'blocking_timeout_secs': 10, 'external_lookup_client': None, 'py_enable_gc': True, 'cache_policy': 'LRU', 'numa_mode': None, 'enable_async_loading': False, 'internal_api_server_enabled': False, 'internal_api_server_port_start': 6999, 'priority_limit': None, 'internal_api_server_include_index_list': None, 'internal_api_server_socket_path_prefix': None, 'plugin_locations': None, 'external_backends': None, 'lookup_timeout_ms': 3000, 'hit_miss_ratio': None, 'lookup_server_worker_ids': None, 'enable_scheduler_bypass_lookup': False, 'script_allowed_imports': None, 'enable_lazy_memory_allocator': False, 'lazy_memory_initial_ratio': 0.2, 'lazy_memory_expand_trigger_ratio': 0.5, 'lazy_memory_step_ratio': 0.1, 'lazy_memory_safe_size': 0.0, 'enable_chunk_statistics': False, 'chunk_statistics_auto_start_statistics': False, 'chunk_statistics_auto_exit_timeout_hours': 0.0, 'chunk_statistics_auto_exit_target_unique_chunks': 0, 'chunk_statistics_strategy': 'memory_bloom_filter'} [3m(cache_engine.py:93:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,578] LMCache INFO:[0m LMCacheWorker is not initialized (related configs: enable_controller: False, role: worker, worker_id: 0, worker_ids: [0]). [3m(cache_engine.py:135:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,578] LMCache INFO:[0m Initialize storage manager on rank 0, use layerwise: False,save only first rank: False [3m(cache_engine.py:167:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,578] LMCache INFO:[0m Initializing LRUCachePolicy [3m(lru.py:22:lmcache.v1.storage_backend.cache_policy.lru)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:54,578] LMCache INFO:[0m NUMA mapping None [3m(local_cpu_backend.py:351:lmcache.v1.storage_backend.local_cpu_backend)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [33;20m[2025-12-07 23:27:56,021] LMCache WARNING:[0m Controller message sender is not initialized [3m(local_cpu_backend.py:103:lmcache.v1.storage_backend.local_cpu_backend)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:56,021] LMCache INFO:[0m Initializing usage context. [3m(usage_context.py:362:lmcache.usage_context)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:57,499] LMCache INFO:[0m lmcache lookup server start on /tmp/engine_3fb66999-b0ae-45a5-96f3-33396372fd8d_service_lookup_lmcache_rpc_port_0 [3m(lmcache_lookup_client.py:329:lmcache.v1.lookup_client.lmcache_lookup_client)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:57,501] LMCache INFO:[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=1, port=7000, socket_path=None, include_index_list=None [3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:57,501] LMCache INFO:[0m LMCache initialized for role KVConnectorRole.WORKER with version 0.3.11.dev18-g58cae13ba, vllm version 0.12.0, lmcache cache_engine metadata: LMCacheEngineMetadata(model_name='Qwen/Qwen3-30B-A3B-FP8', world_size=1, worker_id=0, fmt='vllm', kv_dtype=torch.bfloat16, kv_shape=(48, 2, 256, 4, 128), use_mla=False, role='worker') [3m(vllm_v1_adapter.py:793:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:57 [utils.py:117] Connectors do not specify a kv cache layout, defaulting to NHD.
[0;36m(EngineCore_DP0 pid=398219)[0;0m 2025-12-07 23:27:57,505 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(EngineCore_DP0 pid=398219)[0;0m 2025-12-07 23:27:57,523 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:58 [core.py:254] init engine (profile, create kv cache, warmup model) took 5.48 seconds
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:59 [factory.py:64] Creating v1 connector with name: LMCacheConnectorV1 and engine_id: 3fb66999-b0ae-45a5-96f3-33396372fd8d
[0;36m(EngineCore_DP0 pid=398219)[0;0m WARNING 12-07 23:27:59 [base.py:163] Initializing KVConnectorBase_V1. This API is experimental and subject to change in the future as we iterate the design.
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:59 [lmcache_connector.py:52] Initializing latest dev LMCache connector
[0;36m(EngineCore_DP0 pid=398219)[0;0m [31;20m[2025-12-07 23:27:59,005] LMCache ERROR:[0m PrometheusLogger instance already created withdifferent metadata. This should not happen except in test [3m(observability.py:1325:lmcache.observability)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:59,005] LMCache INFO:[0m lmcache lookup client connect to rank 0 with socket path /tmp/engine_3fb66999-b0ae-45a5-96f3-33396372fd8d_service_lookup_lmcache_rpc_port_0 [3m(lmcache_lookup_client.py:93:lmcache.v1.lookup_client.lmcache_lookup_client)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [33;20m[2025-12-07 23:27:59,006] LMCache WARNING:[0m Could not load 'builtin' from vLLM. Using builtin hash. This may cause inconsistencies in distributed caching. [3m(token_database.py:136:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [33;20m[2025-12-07 23:27:59,006] LMCache WARNING:[0m Using builtin hash without PYTHONHASHSEED set. For production environments (non-testing scenarios), you MUST set PYTHONHASHSEED to ensure consistent hashing across processes. Example: export PYTHONHASHSEED=0 [3m(token_database.py:143:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:59,006] LMCache INFO:[0m Initialized NONE_HASH=b'\xa9f\x0b>2\x01\xbbK\xb5s\ry\xc4\xdb"\x85\xaf\xdd6xo\x83K\xecL\xd9s\x85\x96\xb6&2' from vLLM (>= PR#20511) [3m(token_database.py:74:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:59,006] LMCache INFO:[0m Using hash algorithm: builtin [3m(token_database.py:84:lmcache.v1.token_database)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:59,006] LMCache INFO:[0m Internal API server disabled. internal_api_server_enabled=False, port_offset=0, port=6999, socket_path=None, include_index_list=None [3m(api_server.py:50:lmcache.v1.internal_api_server.api_server)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:27:59,006] LMCache INFO:[0m LMCache initialized for role KVConnectorRole.SCHEDULER with version 0.3.11.dev18-g58cae13ba, vllm version 0.12.0, lmcache cache_engine metadata: None [3m(vllm_v1_adapter.py:793:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m WARNING 12-07 23:27:59 [vllm.py:608] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.
[0;36m(EngineCore_DP0 pid=398219)[0;0m INFO 12-07 23:27:59 [vllm.py:707] Cudagraph is disabled under eager mode
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [api_server.py:1520] Supported tasks: ['generate']
[0;36m(APIServer pid=398063)[0;0m WARNING 12-07 23:27:59 [model.py:1576] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [serving_responses.py:194] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [serving_completion.py:73] Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [api_server.py:1847] Starting vLLM API server 0 on http://0.0.0.0:8000
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:27:59 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=398063)[0;0m INFO:     Started server process [398063]
[0;36m(APIServer pid=398063)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=398063)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:39536 - "GET /health HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:39544 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:06,065] LMCache INFO:[0m Reqid: cmpl-80611d4cb0cab945-0, Total tokens 729, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:06,066] LMCache INFO:[0m Post-initializing LMCacheEngine [3m(cache_engine.py:226:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:06,369] LMCache INFO:[0m Storing KV cache for 729 out of 729 tokens (skip_leading_tokens=0) for request cmpl-80611d4cb0cab945-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:06,372] LMCache INFO:[0m Stored 729 out of total 729 tokens. size: 0.0667 gb, cost 2.4403 ms, throughput: 27.3498 GB/s; offload_time: 2.4142 ms, put_time: 0.0261 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:28:09 [loggers.py:236] Engine 000: Avg prompt throughput: 68.8 tokens/s, Avg generation throughput: 18.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.8%, Prefix cache hit rate: 0.0%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:39544 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:15,702] LMCache INFO:[0m Reqid: cmpl-benchmark-serving0-0, Total tokens 729, LMCache hit tokens: 729, need to load: 8 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:15,704] LMCache INFO:[0m Retrieved 217 out of 217 required tokens (from 729 total tokens). size: 0.0199 gb, cost 0.6470 ms, throughput: 30.7057 GB/s; [3m(cache_engine.py:582:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43728 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,366] LMCache INFO:[0m Reqid: cmpl-benchmark-serving1-0, Total tokens 9, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,411] LMCache INFO:[0m Storing KV cache for 9 out of 9 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving1-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,411] LMCache INFO:[0m Stored 9 out of total 9 tokens. size: 0.0008 gb, cost 0.1736 ms, throughput: 4.7456 GB/s; offload_time: 0.1607 ms, put_time: 0.0129 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43744 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,659] LMCache INFO:[0m Reqid: cmpl-benchmark-serving2-0, Total tokens 110, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,747] LMCache INFO:[0m Storing KV cache for 110 out of 110 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving2-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,747] LMCache INFO:[0m Stored 110 out of total 110 tokens. size: 0.0101 gb, cost 0.4382 ms, throughput: 22.9825 GB/s; offload_time: 0.4279 ms, put_time: 0.0103 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43746 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,871] LMCache INFO:[0m Reqid: cmpl-benchmark-serving3-0, Total tokens 59, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43754 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43768 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43778 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,955] LMCache INFO:[0m Storing KV cache for 59 out of 59 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving3-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,956] LMCache INFO:[0m Stored 59 out of total 59 tokens. size: 0.0054 gb, cost 0.3094 ms, throughput: 17.4582 GB/s; offload_time: 0.2981 ms, put_time: 0.0113 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,957] LMCache INFO:[0m Reqid: cmpl-benchmark-serving4-0, Total tokens 21, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,959] LMCache INFO:[0m Reqid: cmpl-benchmark-serving5-0, Total tokens 57, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:16,960] LMCache INFO:[0m Reqid: cmpl-benchmark-serving6-0, Total tokens 36, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,058] LMCache INFO:[0m Storing KV cache for 21 out of 21 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving4-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,058] LMCache INFO:[0m Stored 21 out of total 21 tokens. size: 0.0019 gb, cost 0.1706 ms, throughput: 11.2688 GB/s; offload_time: 0.1630 ms, put_time: 0.0076 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,058] LMCache INFO:[0m Storing KV cache for 57 out of 57 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving5-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,058] LMCache INFO:[0m Stored 57 out of total 57 tokens. size: 0.0052 gb, cost 0.2242 ms, throughput: 23.2781 GB/s; offload_time: 0.2188 ms, put_time: 0.0054 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,058] LMCache INFO:[0m Storing KV cache for 36 out of 36 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving6-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,059] LMCache INFO:[0m Stored 36 out of total 36 tokens. size: 0.0033 gb, cost 0.1580 ms, throughput: 20.8566 GB/s; offload_time: 0.1533 ms, put_time: 0.0047 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43780 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,389] LMCache INFO:[0m Reqid: cmpl-benchmark-serving7-0, Total tokens 78, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,480] LMCache INFO:[0m Storing KV cache for 78 out of 78 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving7-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,480] LMCache INFO:[0m Stored 78 out of total 78 tokens. size: 0.0071 gb, cost 0.3558 ms, throughput: 20.0707 GB/s; offload_time: 0.3458 ms, put_time: 0.0100 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43794 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,581] LMCache INFO:[0m Reqid: cmpl-benchmark-serving8-0, Total tokens 667, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,819] LMCache INFO:[0m Storing KV cache for 667 out of 667 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving8-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,821] LMCache INFO:[0m Stored 667 out of total 667 tokens. size: 0.0611 gb, cost 1.9845 ms, throughput: 30.7719 GB/s; offload_time: 1.9690 ms, put_time: 0.0155 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43798 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43810 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,872] LMCache INFO:[0m Reqid: cmpl-benchmark-serving9-0, Total tokens 414, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:17,872] LMCache INFO:[0m Reqid: cmpl-benchmark-serving10-0, Total tokens 227, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:18,102] LMCache INFO:[0m Storing KV cache for 414 out of 414 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving9-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:18,103] LMCache INFO:[0m Stored 414 out of total 414 tokens. size: 0.0379 gb, cost 1.2994 ms, throughput: 29.1701 GB/s; offload_time: 1.2864 ms, put_time: 0.0129 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:18,103] LMCache INFO:[0m Storing KV cache for 227 out of 227 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving10-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:18,104] LMCache INFO:[0m Stored 227 out of total 227 tokens. size: 0.0208 gb, cost 0.6798 ms, throughput: 30.5701 GB/s; offload_time: 0.6743 ms, put_time: 0.0056 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42422 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:18,617] LMCache INFO:[0m Reqid: cmpl-benchmark-serving11-0, Total tokens 7, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:18,689] LMCache INFO:[0m Storing KV cache for 7 out of 7 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving11-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:18,689] LMCache INFO:[0m Stored 7 out of total 7 tokens. size: 0.0006 gb, cost 0.1637 ms, throughput: 3.9151 GB/s; offload_time: 0.1539 ms, put_time: 0.0098 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42438 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42448 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:19,053] LMCache INFO:[0m Reqid: cmpl-benchmark-serving12-0, Total tokens 379, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:19,055] LMCache INFO:[0m Reqid: cmpl-benchmark-serving13-0, Total tokens 771, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42456 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42472 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42484 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42488 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:19,438] LMCache INFO:[0m Storing KV cache for 379 out of 379 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving12-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:19,439] LMCache INFO:[0m Stored 379 out of total 379 tokens. size: 0.0347 gb, cost 1.2198 ms, throughput: 28.4468 GB/s; offload_time: 1.2063 ms, put_time: 0.0135 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:19,439] LMCache INFO:[0m Storing KV cache for 771 out of 771 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving13-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:19,442] LMCache INFO:[0m Stored 771 out of total 771 tokens. size: 0.0706 gb, cost 2.1583 ms, throughput: 32.7054 GB/s; offload_time: 2.1473 ms, put_time: 0.0110 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:19,443] LMCache INFO:[0m Reqid: cmpl-benchmark-serving14-0, Total tokens 401, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:19,444] LMCache INFO:[0m Reqid: cmpl-benchmark-serving15-0, Total tokens 17, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:19,445] LMCache INFO:[0m Reqid: cmpl-benchmark-serving16-0, Total tokens 771, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:19,446] LMCache INFO:[0m Reqid: cmpl-benchmark-serving17-0, Total tokens 743, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42490 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42504 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42514 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42518 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42524 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:28:19 [loggers.py:236] Engine 000: Avg prompt throughput: 356.4 tokens/s, Avg generation throughput: 70.2 tokens/s, Running: 14 reqs, Waiting: 0 reqs, GPU KV cache usage: 21.1%, Prefix cache hit rate: 16.8%, External prefix cache hit rate: 0.2%
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42538 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,019] LMCache INFO:[0m Storing KV cache for 401 out of 401 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving14-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,021] LMCache INFO:[0m Stored 401 out of total 401 tokens. size: 0.0367 gb, cost 1.2709 ms, throughput: 28.8881 GB/s; offload_time: 1.2582 ms, put_time: 0.0127 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,021] LMCache INFO:[0m Storing KV cache for 17 out of 17 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving15-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,021] LMCache INFO:[0m Stored 17 out of total 17 tokens. size: 0.0016 gb, cost 0.1199 ms, throughput: 12.9756 GB/s; offload_time: 0.1146 ms, put_time: 0.0053 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,021] LMCache INFO:[0m Storing KV cache for 771 out of 771 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving16-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,023] LMCache INFO:[0m Stored 771 out of total 771 tokens. size: 0.0706 gb, cost 2.1589 ms, throughput: 32.6966 GB/s; offload_time: 2.1509 ms, put_time: 0.0080 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,023] LMCache INFO:[0m Storing KV cache for 743 out of 743 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving17-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,026] LMCache INFO:[0m Stored 743 out of total 743 tokens. size: 0.0680 gb, cost 2.0597 ms, throughput: 33.0263 GB/s; offload_time: 2.0533 ms, put_time: 0.0064 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,028] LMCache INFO:[0m Reqid: cmpl-benchmark-serving18-0, Total tokens 245, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,029] LMCache INFO:[0m Reqid: cmpl-benchmark-serving19-0, Total tokens 20, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,030] LMCache INFO:[0m Reqid: cmpl-benchmark-serving20-0, Total tokens 403, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,031] LMCache INFO:[0m Reqid: cmpl-benchmark-serving21-0, Total tokens 325, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,031] LMCache INFO:[0m Reqid: cmpl-benchmark-serving22-0, Total tokens 61, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,032] LMCache INFO:[0m Reqid: cmpl-benchmark-serving23-0, Total tokens 412, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42544 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42550 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,498] LMCache INFO:[0m Storing KV cache for 245 out of 245 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving18-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,499] LMCache INFO:[0m Stored 245 out of total 245 tokens. size: 0.0224 gb, cost 0.8349 ms, throughput: 26.8673 GB/s; offload_time: 0.8222 ms, put_time: 0.0126 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,499] LMCache INFO:[0m Storing KV cache for 20 out of 20 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving19-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,500] LMCache INFO:[0m Stored 20 out of total 20 tokens. size: 0.0018 gb, cost 0.1313 ms, throughput: 13.9478 GB/s; offload_time: 0.1259 ms, put_time: 0.0054 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,500] LMCache INFO:[0m Storing KV cache for 403 out of 403 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving20-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,501] LMCache INFO:[0m Stored 403 out of total 403 tokens. size: 0.0369 gb, cost 1.1582 ms, throughput: 31.8562 GB/s; offload_time: 1.1519 ms, put_time: 0.0063 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,501] LMCache INFO:[0m Storing KV cache for 325 out of 325 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving21-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,502] LMCache INFO:[0m Stored 325 out of total 325 tokens. size: 0.0298 gb, cost 0.9350 ms, throughput: 31.8230 GB/s; offload_time: 0.9290 ms, put_time: 0.0060 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,502] LMCache INFO:[0m Storing KV cache for 61 out of 61 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving22-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,502] LMCache INFO:[0m Stored 61 out of total 61 tokens. size: 0.0056 gb, cost 0.2185 ms, throughput: 25.5647 GB/s; offload_time: 0.2141 ms, put_time: 0.0044 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,502] LMCache INFO:[0m Storing KV cache for 412 out of 412 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving23-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,503] LMCache INFO:[0m Stored 412 out of total 412 tokens. size: 0.0377 gb, cost 1.1656 ms, throughput: 32.3596 GB/s; offload_time: 1.1598 ms, put_time: 0.0059 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,506] LMCache INFO:[0m Reqid: cmpl-benchmark-serving24-0, Total tokens 88, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,507] LMCache INFO:[0m Reqid: cmpl-benchmark-serving25-0, Total tokens 181, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42552 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,646] LMCache INFO:[0m Storing KV cache for 88 out of 88 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving24-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,646] LMCache INFO:[0m Stored 88 out of total 88 tokens. size: 0.0081 gb, cost 0.3626 ms, throughput: 22.2177 GB/s; offload_time: 0.3505 ms, put_time: 0.0122 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,646] LMCache INFO:[0m Storing KV cache for 181 out of 181 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving25-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,647] LMCache INFO:[0m Stored 181 out of total 181 tokens. size: 0.0166 gb, cost 0.5527 ms, throughput: 29.9831 GB/s; offload_time: 0.5475 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,649] LMCache INFO:[0m Reqid: cmpl-benchmark-serving26-0, Total tokens 26, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42560 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,744] LMCache INFO:[0m Storing KV cache for 26 out of 26 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving26-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,744] LMCache INFO:[0m Stored 26 out of total 26 tokens. size: 0.0024 gb, cost 0.2102 ms, throughput: 11.3234 GB/s; offload_time: 0.1998 ms, put_time: 0.0104 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,747] LMCache INFO:[0m Reqid: cmpl-benchmark-serving27-0, Total tokens 28, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,842] LMCache INFO:[0m Storing KV cache for 28 out of 28 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving27-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,842] LMCache INFO:[0m Stored 28 out of total 28 tokens. size: 0.0026 gb, cost 0.1867 ms, throughput: 13.7322 GB/s; offload_time: 0.1789 ms, put_time: 0.0077 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42576 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42582 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,922] LMCache INFO:[0m Reqid: cmpl-benchmark-serving28-0, Total tokens 188, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:20,923] LMCache INFO:[0m Reqid: cmpl-benchmark-serving29-0, Total tokens 507, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42598 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42600 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42616 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,193] LMCache INFO:[0m Storing KV cache for 188 out of 188 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving28-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,194] LMCache INFO:[0m Stored 188 out of total 188 tokens. size: 0.0172 gb, cost 0.6716 ms, throughput: 25.6265 GB/s; offload_time: 0.6599 ms, put_time: 0.0117 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,194] LMCache INFO:[0m Storing KV cache for 507 out of 507 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving29-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,195] LMCache INFO:[0m Stored 507 out of total 507 tokens. size: 0.0464 gb, cost 1.4439 ms, throughput: 32.1476 GB/s; offload_time: 1.4371 ms, put_time: 0.0067 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,198] LMCache INFO:[0m Reqid: cmpl-benchmark-serving30-0, Total tokens 249, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,199] LMCache INFO:[0m Reqid: cmpl-benchmark-serving31-0, Total tokens 384, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,200] LMCache INFO:[0m Reqid: cmpl-benchmark-serving32-0, Total tokens 835, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,675] LMCache INFO:[0m Storing KV cache for 249 out of 249 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving30-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,676] LMCache INFO:[0m Stored 249 out of total 249 tokens. size: 0.0228 gb, cost 0.8415 ms, throughput: 27.0916 GB/s; offload_time: 0.8304 ms, put_time: 0.0111 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,676] LMCache INFO:[0m Storing KV cache for 384 out of 384 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving31-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,677] LMCache INFO:[0m Stored 384 out of total 384 tokens. size: 0.0352 gb, cost 1.1204 ms, throughput: 31.3796 GB/s; offload_time: 1.1135 ms, put_time: 0.0069 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,677] LMCache INFO:[0m Storing KV cache for 835 out of 835 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving32-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,680] LMCache INFO:[0m Stored 835 out of total 835 tokens. size: 0.0764 gb, cost 2.3023 ms, throughput: 33.2047 GB/s; offload_time: 2.2944 ms, put_time: 0.0079 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42618 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:21,853] LMCache INFO:[0m Reqid: cmpl-benchmark-serving33-0, Total tokens 386, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:22,047] LMCache INFO:[0m Storing KV cache for 386 out of 386 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving33-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:22,048] LMCache INFO:[0m Stored 386 out of total 386 tokens. size: 0.0353 gb, cost 1.2091 ms, throughput: 29.2284 GB/s; offload_time: 1.1960 ms, put_time: 0.0131 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42632 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:22,578] LMCache INFO:[0m Reqid: cmpl-benchmark-serving34-0, Total tokens 5, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:22,671] LMCache INFO:[0m Storing KV cache for 5 out of 5 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving34-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:22,672] LMCache INFO:[0m Stored 5 out of total 5 tokens. size: 0.0005 gb, cost 0.1583 ms, throughput: 2.8909 GB/s; offload_time: 0.1480 ms, put_time: 0.0103 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42646 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:22,939] LMCache INFO:[0m Reqid: cmpl-benchmark-serving35-0, Total tokens 679, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42650 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42666 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,213] LMCache INFO:[0m Storing KV cache for 679 out of 679 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving35-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,215] LMCache INFO:[0m Stored 679 out of total 679 tokens. size: 0.0622 gb, cost 2.0164 ms, throughput: 30.8292 GB/s; offload_time: 2.0025 ms, put_time: 0.0139 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,218] LMCache INFO:[0m Reqid: cmpl-benchmark-serving36-0, Total tokens 25, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,219] LMCache INFO:[0m Reqid: cmpl-benchmark-serving37-0, Total tokens 742, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42670 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42674 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42676 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,493] LMCache INFO:[0m Storing KV cache for 25 out of 25 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving36-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,493] LMCache INFO:[0m Stored 25 out of total 25 tokens. size: 0.0023 gb, cost 0.2331 ms, throughput: 9.8180 GB/s; offload_time: 0.2217 ms, put_time: 0.0114 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,493] LMCache INFO:[0m Storing KV cache for 742 out of 742 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving37-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,495] LMCache INFO:[0m Stored 742 out of total 742 tokens. size: 0.0679 gb, cost 2.0804 ms, throughput: 32.6528 GB/s; offload_time: 2.0724 ms, put_time: 0.0081 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,497] LMCache INFO:[0m Reqid: cmpl-benchmark-serving38-0, Total tokens 385, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,498] LMCache INFO:[0m Reqid: cmpl-benchmark-serving39-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,499] LMCache INFO:[0m Reqid: cmpl-benchmark-serving40-0, Total tokens 332, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42684 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42690 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,775] LMCache INFO:[0m Storing KV cache for 385 out of 385 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving38-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,776] LMCache INFO:[0m Stored 385 out of total 385 tokens. size: 0.0352 gb, cost 1.2537 ms, throughput: 28.1141 GB/s; offload_time: 1.2397 ms, put_time: 0.0140 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,777] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving39-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,777] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.2250 ms, throughput: 23.5963 GB/s; offload_time: 0.2199 ms, put_time: 0.0051 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,777] LMCache INFO:[0m Storing KV cache for 332 out of 332 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving40-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,778] LMCache INFO:[0m Stored 332 out of total 332 tokens. size: 0.0304 gb, cost 0.9688 ms, throughput: 31.3728 GB/s; offload_time: 0.9624 ms, put_time: 0.0065 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,780] LMCache INFO:[0m Reqid: cmpl-benchmark-serving41-0, Total tokens 11, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,780] LMCache INFO:[0m Reqid: cmpl-benchmark-serving42-0, Total tokens 37, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,887] LMCache INFO:[0m Storing KV cache for 11 out of 11 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving41-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,887] LMCache INFO:[0m Stored 11 out of total 11 tokens. size: 0.0010 gb, cost 0.1662 ms, throughput: 6.0589 GB/s; offload_time: 0.1562 ms, put_time: 0.0100 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,887] LMCache INFO:[0m Storing KV cache for 37 out of 37 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving42-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:23,888] LMCache INFO:[0m Stored 37 out of total 37 tokens. size: 0.0034 gb, cost 0.1685 ms, throughput: 20.0977 GB/s; offload_time: 0.1635 ms, put_time: 0.0051 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42704 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42712 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,169] LMCache INFO:[0m Reqid: cmpl-benchmark-serving43-0, Total tokens 161, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,170] LMCache INFO:[0m Reqid: cmpl-benchmark-serving44-0, Total tokens 788, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42724 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42728 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,490] LMCache INFO:[0m Storing KV cache for 161 out of 161 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving43-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,490] LMCache INFO:[0m Stored 161 out of total 161 tokens. size: 0.0147 gb, cost 0.5989 ms, throughput: 24.6109 GB/s; offload_time: 0.5871 ms, put_time: 0.0118 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,490] LMCache INFO:[0m Storing KV cache for 788 out of 788 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving44-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,493] LMCache INFO:[0m Stored 788 out of total 788 tokens. size: 0.0721 gb, cost 2.2115 ms, throughput: 32.6213 GB/s; offload_time: 2.2031 ms, put_time: 0.0085 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,494] LMCache INFO:[0m Reqid: cmpl-benchmark-serving45-0, Total tokens 30, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,494] LMCache INFO:[0m Reqid: cmpl-benchmark-serving46-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,615] LMCache INFO:[0m Storing KV cache for 30 out of 30 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving45-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,615] LMCache INFO:[0m Stored 30 out of total 30 tokens. size: 0.0027 gb, cost 0.2160 ms, throughput: 12.7136 GB/s; offload_time: 0.2047 ms, put_time: 0.0113 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,615] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving46-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,616] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.2241 ms, throughput: 23.6920 GB/s; offload_time: 0.2189 ms, put_time: 0.0052 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42734 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,711] LMCache INFO:[0m Reqid: cmpl-benchmark-serving47-0, Total tokens 24, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42742 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,818] LMCache INFO:[0m Storing KV cache for 24 out of 24 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving47-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,819] LMCache INFO:[0m Stored 24 out of total 24 tokens. size: 0.0022 gb, cost 0.2174 ms, throughput: 10.1092 GB/s; offload_time: 0.2072 ms, put_time: 0.0101 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:24,820] LMCache INFO:[0m Reqid: cmpl-benchmark-serving48-0, Total tokens 774, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42750 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:25,102] LMCache INFO:[0m Storing KV cache for 774 out of 774 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving48-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:25,104] LMCache INFO:[0m Stored 774 out of total 774 tokens. size: 0.0709 gb, cost 2.2795 ms, throughput: 31.0863 GB/s; offload_time: 2.2646 ms, put_time: 0.0149 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:25,106] LMCache INFO:[0m Reqid: cmpl-benchmark-serving49-0, Total tokens 335, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:25,267] LMCache INFO:[0m Storing KV cache for 335 out of 335 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving49-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:25,268] LMCache INFO:[0m Stored 335 out of total 335 tokens. size: 0.0307 gb, cost 1.0599 ms, throughput: 28.9375 GB/s; offload_time: 1.0399 ms, put_time: 0.0200 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42766 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:25,647] LMCache INFO:[0m Reqid: cmpl-benchmark-serving50-0, Total tokens 58, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:25,760] LMCache INFO:[0m Storing KV cache for 58 out of 58 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving50-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:25,761] LMCache INFO:[0m Stored 58 out of total 58 tokens. size: 0.0053 gb, cost 0.3124 ms, throughput: 16.9973 GB/s; offload_time: 0.3012 ms, put_time: 0.0112 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42772 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:25,961] LMCache INFO:[0m Reqid: cmpl-benchmark-serving51-0, Total tokens 273, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:26,119] LMCache INFO:[0m Storing KV cache for 273 out of 273 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving51-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:26,120] LMCache INFO:[0m Stored 273 out of total 273 tokens. size: 0.0250 gb, cost 0.8947 ms, throughput: 27.9365 GB/s; offload_time: 0.8832 ms, put_time: 0.0115 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42776 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:26,628] LMCache INFO:[0m Reqid: cmpl-benchmark-serving52-0, Total tokens 5, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:26,736] LMCache INFO:[0m Storing KV cache for 5 out of 5 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving52-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:26,737] LMCache INFO:[0m Stored 5 out of total 5 tokens. size: 0.0005 gb, cost 0.1700 ms, throughput: 2.6922 GB/s; offload_time: 0.1587 ms, put_time: 0.0113 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42786 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:27,047] LMCache INFO:[0m Reqid: cmpl-benchmark-serving53-0, Total tokens 48, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:27,161] LMCache INFO:[0m Storing KV cache for 48 out of 48 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving53-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:27,162] LMCache INFO:[0m Stored 48 out of total 48 tokens. size: 0.0044 gb, cost 0.2780 ms, throughput: 15.8059 GB/s; offload_time: 0.2673 ms, put_time: 0.0107 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42788 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:27,269] LMCache INFO:[0m Reqid: cmpl-benchmark-serving54-0, Total tokens 158, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:27,399] LMCache INFO:[0m Storing KV cache for 158 out of 158 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving54-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:27,399] LMCache INFO:[0m Stored 158 out of total 158 tokens. size: 0.0145 gb, cost 0.5743 ms, throughput: 25.1895 GB/s; offload_time: 0.5636 ms, put_time: 0.0106 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42790 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42806 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:27,822] LMCache INFO:[0m Reqid: cmpl-benchmark-serving55-0, Total tokens 306, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:27,823] LMCache INFO:[0m Reqid: cmpl-benchmark-serving56-0, Total tokens 772, LMCache hit tokens: 0, need to load: 0 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42810 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42820 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42824 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:37670 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:37674 - "POST /v1/completions HTTP/1.1" 200 OK
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:28,184] LMCache INFO:[0m Storing KV cache for 306 out of 306 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving55-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:28,185] LMCache INFO:[0m Stored 306 out of total 306 tokens. size: 0.0280 gb, cost 1.0233 ms, throughput: 27.3769 GB/s; offload_time: 1.0100 ms, put_time: 0.0133 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:28,185] LMCache INFO:[0m Storing KV cache for 772 out of 772 tokens (skip_leading_tokens=0) for request cmpl-benchmark-serving56-0 [3m(vllm_v1_adapter.py:1293:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:28,188] LMCache INFO:[0m Stored 772 out of total 772 tokens. size: 0.0707 gb, cost 2.1526 ms, throughput: 32.8348 GB/s; offload_time: 2.1444 ms, put_time: 0.0081 ms [3m(cache_engine.py:352:lmcache.v1.cache_engine)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m [32;20m[2025-12-07 23:28:28,296] LMCache INFO:[0m Reqid: cmpl-benchmark-serving56-0, Total tokens 773, LMCache hit tokens: 768, need to load: 128 [3m(vllm_v1_adapter.py:1511:lmcache.integration.vllm.vllm_v1_adapter)[0m
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845] EngineCore encountered a fatal error.
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845] Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 836, in run_engine_core
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]     engine_core.run_busy_loop()
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 863, in run_busy_loop
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]     self._process_engine_step()
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 892, in _process_engine_step
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]     outputs, model_executed = self.step_fn()
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]                               ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 342, in step
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]     scheduler_output = self.scheduler.schedule()
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]                        ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/core/sched/scheduler.py", line 531, in schedule
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]     assert num_new_tokens > 0
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845]            ^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=398219)[0;0m ERROR 12-07 23:28:28 [core.py:845] AssertionError
[0;36m(EngineCore_DP0 pid=398219)[0;0m Process EngineCore_DP0:
[0;36m(EngineCore_DP0 pid=398219)[0;0m Traceback (most recent call last):
[0;36m(EngineCore_DP0 pid=398219)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[0;36m(EngineCore_DP0 pid=398219)[0;0m     self.run()
[0;36m(EngineCore_DP0 pid=398219)[0;0m   File "/usr/lib/python3.12/multiprocessing/process.py", line 108, in run
[0;36m(EngineCore_DP0 pid=398219)[0;0m     self._target(*self._args, **self._kwargs)
[0;36m(EngineCore_DP0 pid=398219)[0;0m   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 847, in run_engine_core
[0;36m(EngineCore_DP0 pid=398219)[0;0m     raise e
[0;36m(EngineCore_DP0 pid=398219)[0;0m   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 836, in run_engine_core
[0;36m(EngineCore_DP0 pid=398219)[0;0m     engine_core.run_busy_loop()
[0;36m(EngineCore_DP0 pid=398219)[0;0m   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 863, in run_busy_loop
[0;36m(EngineCore_DP0 pid=398219)[0;0m     self._process_engine_step()
[0;36m(EngineCore_DP0 pid=398219)[0;0m   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 892, in _process_engine_step
[0;36m(EngineCore_DP0 pid=398219)[0;0m     outputs, model_executed = self.step_fn()
[0;36m(EngineCore_DP0 pid=398219)[0;0m                               ^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=398219)[0;0m   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py", line 342, in step
[0;36m(EngineCore_DP0 pid=398219)[0;0m     scheduler_output = self.scheduler.schedule()
[0;36m(EngineCore_DP0 pid=398219)[0;0m                        ^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=398219)[0;0m   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/core/sched/scheduler.py", line 531, in schedule
[0;36m(EngineCore_DP0 pid=398219)[0;0m     assert num_new_tokens > 0
[0;36m(EngineCore_DP0 pid=398219)[0;0m            ^^^^^^^^^^^^^^^^^^
[0;36m(EngineCore_DP0 pid=398219)[0;0m AssertionError
[0;36m(APIServer pid=398063)[0;0m ERROR 12-07 23:28:28 [async_llm.py:546] AsyncLLM output_handler failed.
[0;36m(APIServer pid=398063)[0;0m ERROR 12-07 23:28:28 [async_llm.py:546] Traceback (most recent call last):
[0;36m(APIServer pid=398063)[0;0m ERROR 12-07 23:28:28 [async_llm.py:546]   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/async_llm.py", line 498, in output_handler
[0;36m(APIServer pid=398063)[0;0m ERROR 12-07 23:28:28 [async_llm.py:546]     outputs = await engine_core.get_output_async()
[0;36m(APIServer pid=398063)[0;0m ERROR 12-07 23:28:28 [async_llm.py:546]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[0;36m(APIServer pid=398063)[0;0m ERROR 12-07 23:28:28 [async_llm.py:546]   File "/home/yunwei37/workspace/gpu/LMCache/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py", line 885, in get_output_async
[0;36m(APIServer pid=398063)[0;0m ERROR 12-07 23:28:28 [async_llm.py:546]     raise self._format_exception(outputs) from None
[0;36m(APIServer pid=398063)[0;0m ERROR 12-07 23:28:28 [async_llm.py:546] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42750 - "POST /v1/completions HTTP/1.1" 500 Internal Server Error
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42560 - "POST /v1/completions HTTP/1.1" 500 Internal Server Error
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:43778 - "POST /v1/completions HTTP/1.1" 500 Internal Server Error
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42504 - "POST /v1/completions HTTP/1.1" 500 Internal Server Error
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42456 - "POST /v1/completions HTTP/1.1" 500 Internal Server Error
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:42514 - "POST /v1/completions HTTP/1.1" 500 Internal Server Error
[0;36m(APIServer pid=398063)[0;0m INFO:     127.0.0.1:37670 - "POST /v1/completions HTTP/1.1" 500 Internal Server Error
[0;36m(APIServer pid=398063)[0;0m INFO:     Shutting down
[0;36m(APIServer pid=398063)[0;0m INFO 12-07 23:28:29 [loggers.py:236] Engine 000: Avg prompt throughput: 1233.3 tokens/s, Avg generation throughput: 266.0 tokens/s, Running: 56 reqs, Waiting: 6 reqs, GPU KV cache usage: 96.7%, Prefix cache hit rate: 4.3%, External prefix cache hit rate: 0.0%
[0;36m(APIServer pid=398063)[0;0m INFO:     Waiting for application shutdown.
[0;36m(APIServer pid=398063)[0;0m INFO:     Application shutdown complete.
[0;36m(APIServer pid=398063)[0;0m INFO:     Finished server process [398063]
[rank0]:[W1207 23:28:30.025037821 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
